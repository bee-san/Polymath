<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Absolute Fundamentals of Machine Learning | Polymath.cloud</title>

<meta name="keywords" content="Artificial Intelligence" />
<meta name="description" content="Absolute Fundamentals of Machine Learning https://unsplash.com/photos/0E_vhMVqL9g Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to teach in this article.
I found that learning the theroetical side alongside the programming side makes it easier to learn both, so this article features both easy to understand mathematics and the algorithms implemented in Python. Also, technology becomes outdated — fast. The code used in this tutorial will likely be meaningless in 5 years time.">
<meta name="author" content="Bee">
<link rel="canonical" href="https://polymath.cloud/ml-fundamentals/" />
<link href="https://polymath.cloud/assets/css/stylesheet.min.94a69f3d0b70cac76c6d6f7dfecc9f91f2319ec73d54be960b0d3624fa5a25e2.css" integrity="sha256-lKafPQtwysdsbW99/syfkfIxnsc9VL6WCw02JPpaJeI=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://polymath.cloud/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://polymath.cloud/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://polymath.cloud/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://polymath.cloud/apple-touch-icon.png">
<link rel="mask-icon" href="https://polymath.cloud/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />




<style>
    td, th {
    border: thin solid #999 !important;
    padding: 12px 15px;
}

thead tr {
    background-color: #009879;
    color: #ffffff;
    text-align: left;
}

table {
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    font-family: sans-serif;
    min-width: 400px;
    overflow: auto;
    display: table;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
}

tbody tr {
    border-bottom: thin solid #dddddd;
}


tbody tr:last-of-type {
    border-bottom: 2px solid #009879;
}

tbody td.active-item {
    font-weight: bold;
    color: #009879;
}

tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
    }


body.dark tbody tr:nth-of-type(even) {
    background-color: #383838;
}


img {
    display: block;
    margin: auto;
    text-align: center;
}

</style>
<meta property="og:title" content="Absolute Fundamentals of Machine Learning" />
<meta property="og:description" content="Absolute Fundamentals of Machine Learning https://unsplash.com/photos/0E_vhMVqL9g Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to teach in this article.
I found that learning the theroetical side alongside the programming side makes it easier to learn both, so this article features both easy to understand mathematics and the algorithms implemented in Python. Also, technology becomes outdated — fast. The code used in this tutorial will likely be meaningless in 5 years time." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://polymath.cloud/ml-fundamentals/" />
<meta property="article:published_time" content="2019-03-11T14:48:10+00:00" />
<meta property="article:modified_time" content="2019-03-11T14:48:10+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Absolute Fundamentals of Machine Learning"/>
<meta name="twitter:description" content="Absolute Fundamentals of Machine Learning https://unsplash.com/photos/0E_vhMVqL9g Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to teach in this article.
I found that learning the theroetical side alongside the programming side makes it easier to learn both, so this article features both easy to understand mathematics and the algorithms implemented in Python. Also, technology becomes outdated — fast. The code used in this tutorial will likely be meaningless in 5 years time."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Absolute Fundamentals of Machine Learning",
  "name": "Absolute Fundamentals of Machine Learning",
  "description": "Absolute Fundamentals of Machine Learning https://unsplash.com/photos/0E_vhMVqL9g Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and …",
  "keywords": [
    "Artificial Intelligence"
  ],
  "articleBody": "Absolute Fundamentals of Machine Learning https://unsplash.com/photos/0E_vhMVqL9g Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to teach in this article.\nI found that learning the theroetical side alongside the programming side makes it easier to learn both, so this article features both easy to understand mathematics and the algorithms implemented in Python. Also, technology becomes outdated — fast. The code used in this tutorial will likely be meaningless in 5 years time. So for that reason, I’ve decided to also teach the mathematical side to Machine Learning that will not die out in a few years.\nWhat is machine learning? Let’s start with learning. Learning is a long and complicated process to describe but in a nutshell it is turning experience into knowledge.\nMachine learning is teaching machines how to learn, as insane as that sounds it’s actually plausable using probability.\nI highly reccomend you read this article on probability, as it is the essential foundation to machine learning and artifical intelligence. We will go over conditional probability and Bayes therom again in this article.\nTypes of learning There are many different ways for a machine to learn, in this I try to explain the different types.\nSupervised Learning In supervised learning we start with a dataset that has training examples, each example has an assiocated label which identifies it.\nAn example of this is presented below: Google ReCaptcha We want the machine to be able to identify drinks, so we present the machine with 9 images, some containing drinks. We then select the pictures that contains the drinks, teaching the computer what a drink looks like.\nIt does this by running labelled data through a **learning algorithm. **The goal of supervised learning is to be able to correctly identify new data given to it, having learnt how to identify the data using the previous data set and learning algorithm.\nUnsupervised Learning Unsupervised learning is quite different from supervised in the sense that it almost always does not have a definite output. The learning agent aims to find structures or patterns in the data.\nA good article on unsupervised learning can be found here.\nReinforcement Learning Reinforcement learning is where the learner receives rewards and punishments for its actions. The reward could simply be utility and the agent could be told to receive as much utility as possible in order to “win”. Utility here could just be a normal variable.\nA good example of reinforcement learning is this:\nMachine Learning Now we understand some of the terminiology of machine learning, we’re actually going to teach a machine something. In order for us to do that, we need to learn a bit of probability.\nMost of the probability here is directly copied off of another blogpost I wrote on probability, but I’ll only include the important parts required for machine learning here.\nConditional Probability Conditional probability is where an event can only happen if another event has happened. Let’s start with an easy problem:\nJohn’s favourite programming languages are Haskell and x86 Assembley. Let A represent the event that he forces a class to learn Haskell and B represent the event that he forces a class to learn x86 Assembley.On a randomly selected day, John is taken over by Satan himself, so the probability of P(A) is 0.6 and the probability of P(B) is 0.4 and the conditional probability that he teaches Haskell, given that he has taught x86 Assembley that day is P(A|B) = 0.7.Based on the information, what is P(B|A), the conditional probability that John teaches x86 Assembley given that he taught Haskell, rounded to the nearest hundredth?\nTherefore the probability of P(A and B) = P(A|B) * P(B); read “|” as given, as in, “A|B” is read as “A given B”. It can also be written as P(B|A) * P(A).\nThe reason it is P(A|B) * P(B) is because given the probability of “Given the probability that B happens, A happens” and the probability of B asP(B). (A|B) is a different probability to P(B) and P(A and B) can only happen if P(B) happens which then allows P(B|A) to happen.\nSo we can transform this into a mathematical formula:\nP(A and B) = P(A|B) * P(B) = 0.7 * 0.5 = 0.35Solving it P(B|A) * P(A) P(A) = 0.5 So 0.6 * P(B|A) Now we don’t know what P(B|A) is, but we want to find out. We know that P(B|A) must be a part of P(A and B) because P(A and B) is the probability that both of these events happen so…P(A and B) = 0.350.35 = P(B|A) * 0.5 With simple algebraic manipulation 0.35/0.5 = P(B|A) P(B|A) = 0.7\nBayes Therom Bayes Therom allows us to work out the probability of events given prior knowledge about the events. It is more of an observation than a therom, as it correctly works all the time. Bayes therom is created by Thomas Bayes, who noted this observation in a notebook. He never published it, so he wasn’t recgonised for his famous therom during his life time. From https://betterexplained.com/articles/colorized-math-equations/ Confusing, right? Let us look at an example.\nSuppose a new drug is found on the streets and the police want to identify whether someone is a user or not.The drug is 99% sensitive, that is that the proportion of people who are correctly identified as taking the drug.The drug is 99% specific, that is that the proportion of people who are correctly identified as not taking the drug.Note: there is a 1% false positive rate for both users and non users.Suppose that 0.5% of people at John Moores (A rival university) takes the drug. What is the probability that a randomly selected John Moores student with a positive test is a user?\nLet’s colourfy this and put this into the equation Note: we don’t directly know the chance of a positive test actually being positive, it isn’t given to us so we have to calculate it, that is why the formula expands on the denominator in the second part.\nLearning with Bayes Therom Now, bayes therom isn’t designed to be used only once. It is designed to work with continus data. Baye originally created this using a thought experiment. He wondered that if his assistant threw a ball on the table, can he predict where it will land?\nBaye asked his assistant to do just that. Then again, and to tell him whether it was left or right of the original spot. He noted down every single spot it hit, and over time, with each throw, he got better at predicting where it’ll land. This is where bayes therom comes into play, by repeatedly using bayes therom we can overtime calculate where the ball might land more accurcately.\nLet’s do a quick example just to get some numbers: Medium doesn’t support colours, so I have to copy and paste images. Sorry! So what would happen if you went to another hospital, got a second opinion and had the test run again by a different lab? This test also comes back as positive, so what is the probability you have the disease?\nWe change the postieor probability of having the disease before the test to 9%, so it looks like this: Now the new number is 0.9073319 or 91% chance of having the disease.\nWhich makes sense, 2 positive results from 2 different labs are unlikely to be conincidental. And if you get more lab results done and they all come back as positive, this number will gradually increase towards 100%.\nNaive Bayes Classifier Naive Bayes Classifier is a type of classifier that is based on Bayes’ Therom with an assumption of independence among predictors. A Naive Bayes Classifier assumes that the presence of a particular feature is unrelated to the presence of any other feature. For example, a frut may be considered an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existance of all other features of an apple, all of these contribute to the probability that the fruit is an apple and that is why this classifier is known “ Naive”.\nWikipedia put it nicely:\n In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 4” in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple.\n Problem\nSuppose we have 2 datasets, “play” and “weather”. We want to see the possibillities of playing given the weather. Image from https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n The first step is to turn the data into a frequency table The second step is to create a likelihood table by finding the probabilities like Overcast = 0.29 and the probability of playing is 4 / 14 (where 14 is the amount of times they play overall) and 4 is the number of times they played in that weather previously. Now use the Naieve Bayesian equation to calculate the conditional probability for each class.  Here’s an example problem question solved using the above data and the equation.\nIs the statement “players will play if weather is sunny” correct?\nP(Yes | Sunny) = P(Sunny|Yes) * P(Yes) /P(Sunny)\nRemember Bayes Therom? This is the same but with variable names inputted to make it easier to read.\nNow we have P(Sunny|Yes) = 3/9 = 0.33. P(Sunny) = 5/14 = 0.36. P(yes) = 9/14 = 0.64Now using Bayes Therom we can work out P(Yes|Sunny) = 0.33 * 0.674 / 0.36 = 0.60 which has high probability.Thus, given that it is sunny, there is a 60% chance they will play.\nMachine Learning using Python Wow, what a boring read that was. “Theoretical Computer Science is boring” I hear you say. Well, you’ll be excited to know this next part is about the application of machine learning using Python.\nLet’s calculate whether an email is spam or ham (that is, a normal email) using machine learning.\nSo, given a large corpus of ham and spam emails, we need to train the model. We’ll start by calculating for each word in each email the number of times that word appears in spam and ham emails.\nWe’ll then calculate a “spaminess” level of each word using a formula. The formula we will be using is quite simple. So, the numerator (the number on top) shows how many times the word appears in a spam email, and the denominator (the bottom) shows how many times it appears in any email.\nSo, lets say there are 9 spam emails and 10 ham emails, making 19 total. If a word appears in 9 spam emails, but 1 ham email than the equation is 9/10 which gives us a 90% chance that this word means it is a spam email.\nBut we cannot simply just count one word, we need to do this for all the words in the email.\nWe essentially have 3 steps:\n Find the spamminess of each word in a message. Find the total spamminess of a message, say by multiplying together the spaminess levels of each word. Call this S[M]. Then find the haminess of each word, by multiplying together the (1 — spaminess) level of each word. Call this H[M].  Then if S[M]  H[M] then the message is spam, else it is ham.\nOkay, so let’s implement this. We’ll be using the bodies of emails only, and there won’t be many emails just to make the explanation easy. Also, the spam emails are **fairly **obvious. Below is the bad emails text file I’m using\nViagra for sale, get sexy body soon! U want sum viagra bby? u sexy s00n VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA SEXY SOON VIAGRA PLEASE YASSSSS VIAGRA BUY PLEASE I really like today\nAnd the good emails\nHello, how are you? I hope you have the work ready for January the 8th. Do you like cats or dogs? Viagra viagra\nSo the first step is to somehow take this file and split it up.\nThe code reads the file, and splits it per new line, spitting out 2 lists. The lists look like this. Below is the bad emails\n[‘Viagra for sale, get sexy body soon!', ‘U want sum viagra bby? u sexy s00n’, ‘VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA’, ‘SEXY SOON VIAGRA PLEASE YASSSSS ‘, ‘VIAGRA BUY PLEASE’, ‘I really like today’]\nAnd the good emails\n[‘Hello, how are you?', ‘I hope you have the work ready for January the 8th.', ‘Do you like cats or dogs?', ‘Viagra’, ‘viagra’]\nOkay, now we need to calculate the spaminess of each word. We do this by finding the word, and seeing how many times it appears in the bad emails and how many times it appears in the good emails, basically the formula seen here: So to find out how many words appear in bad and good emails, we could use nested for loops; but I’ve opted to go a bit functional here.\nSo, the first line is a lambda (anonymous) function which just turns a list of lists into one list. Anonymous functions are functions that are only intended to be used once or twice. The second and third line do the same. First, they map the function “x.split(“ “)” onto each email in the list, generating a new list of list of words in the email, then they apply the flatten function to make the list of lists into one list.\nNow we have 2 lists, one containing all the words in a bad email and one containing all the good words in a good email.\nNow we create a function that calculates the spamminess level of a word. First, it counts how many times a word appears in the bad emails. Then it counts how many times it appears in both emails. Then it returns how many times it appears in bad emails divided by how many times it appears in total emails.\nNow we want a dictionary of every word and its spaminess level.\nword_Dict shall now return\n{‘body’: 1.0, ‘': 1.0, ‘want’: 1.0, ‘get’: 1.0, ‘I’: 0.5, ‘January’: 0.0, ‘s00n’: 1.0, ‘SOON’: 1.0, ‘you?': 0.0, ‘dogs?': 0.0, ‘sale,': 1.0, ‘U’: 1.0, ‘are’: 0.0, ‘Viagra’: 0.5, ‘sexy’: 1.0, ‘ready’: 0.0, ‘SEXY’: 1.0, ‘the’: 0.0, ‘8th.': 0.0, ‘really’: 1.0, ‘Do’: 0.0, ‘Hello,': 0.0, ‘BUY’: 1.0, ‘bby?': 1.0, ‘like’: 0.5, ‘for’: 0.5, ‘sum’: 1.0, ‘work’: 0.0, ‘PLEASE’: 1.0, ‘VIAGRA’: 1.0, ‘YASSSSS’: 1.0, ‘how’: 0.0, ‘viagra’: 0.5, ‘cats’: 0.0, ‘u’: 1.0, ‘hope’: 0.0, ‘have’: 0.0, ‘you’: 0.0, ‘or’: 0.0, ‘today’: 1.0, ‘soon!': 1.0}\nGiven a message, this code will compute how spammy the message is by multiplying the spaminess level of each word. The reduce function is foldl in haskell, it takes a list and a function and returns a single value by applying the function to every item of the list. In this case, given a list of [x, y, x, y, x, y] it will multiply x*y and then add that together.\nNow we need to find the haminess of each message, not so hard now that we can find the spaminess.\nThe haminess is 1 — the spaminess\nNow we just need to create a function that determines whether it is spam or ham.\nIf it is true, it is spam, if not; it’s ham. Obviously this won’t work so well with such a limited dataset, but if we had thousands of emails this should work well, despite the initial startup time being slow.\nWe just used a naieve bayes classifer — a supervised machine learning based approach to spam detection. Isn’t that really cool sounding?\nK-Nearest Neighbour Like Naieve Bayes above, let’s create an intution about K-nearest neighbour.\nLet’s start off with some facts to build up this intuition.\nA point is a location. It has no size, but much like any location it shows you where something is. It can be represented in different ways, depending on how many dimensions it lives in.\nA line is a 1 dimensional object. We can represent a location on a line like so So say the line starts at the number 3 and ends at the number 9, a location is any number between 3 and 9. In this diagram, it is represented as X.\nIn a 2-dimensional object such as a square, we can represent the location using X and Y coordinates. Any location in this square can be reached using X and Y coordinatess.\nA cube is a 3-dimensional square, we can represent its location using 3 coordinates Taken from here A point in geometry is a location. It has no size i.e. no width, no length and no depth. A point is shown by a dot.\nSo a point in an n-dimensional space needs n coordinates to represent it. We find it hard to visualise anything more than 3 dimensional space (as our world is 3 dimensional).\nAn n-dimensional space is represented by an n-dimensional hypercube.\nA hypercube is just a fancy word meaning that any location can be represented as a tuple of n numbers.\nNow, back to the example of stopping spam. Any email message can be represented as a point in a hypercube.\nLet’s say for example, we have a list of every single possible word that could ever appear in an email.\nSo there are:\nW1, W2, …, Wn\nWords in the list. Let’s simplfy this, lets say every single word that could appear in an email is\n(‘Hello’, ‘Goodbye’, ‘Email’, ‘Cheese’, ‘United Kingdom’)\nfor simplicities sake, we’ll pretend only these words are possible in an email.\nEvery single email will contain a subset of the words above like so:\n“Hello, Cheese.” or “United Kingdom, Goodbye.”\nEach of these messages can be represented as a list of 1’s and 0’s, just like bit vectors.\nSo for the first message:\n(‘Hello’, ‘Cheese’) (1, 0, 0, 1, 0)\nSo we simply turn the words that are in both the email and the universe of possible words into a 1, in this instance the first element, “Hello”, and the fouth element, “cheese” are turned into 1s.\nSo now we’ve taken a message and turned it into a point in an n-dimensional hypercube.\nNow we want to do this for the email we want to classify as well as every email we hold information about. So we have training data, and a message to classify.\nNow we have a bunch of points in an n-dimensional hypercube, we want to calculate the distance between each point. There are many ways to calculate the distance. We’ll be using the Euclidean Distance Formula here. Taken from here. More on this later. When we insert a new email to be classified, we check its nearest neighbours. If most of the emails near it are spam, then the email is likely to be spam. But if most of the emails near it are ham, then the email is likely to be ham. The feature space for spam and ham emails So we have our training data classified and this is what it looks like on a graph. When we place our email to be classified into it, the graph will look like: Feature space We know where to place the email to be classified because we turned it into a set of coordinates earlier. Feature space We draw a circle around the email to be classified and since most of the spam emails are inside the circle, we mark this email as spam. This is slightly simpler than reality.\nWhen we categorised each email as a bit vector containing 0 and 1s, normally we would hash subsets of the email and use that instead and the distance will likely not be a straight euclidean formula but a more sophisticated formula that works well in training.\nDistance Measurements I said that Euclidean Distance was one of the ways you could calculate the distance, in this section I’m going to talk a little more about that.\nThe Euclidean Distance is used for **quantitative **data, data that is only numbers.\nWhat is “Euclidean Distance”? Well, first let’s go back to Pythagoras’ Therom. We all remember it as Taken from here but Pythagoras’ therom can be applied to many instances. In short, it is findign the distance between 2 points at a right angle. Let’s say you walk 3 meters east and 4 meters north, how far away are you from the original source? Well, 5 meters as the crow flies. If you want to learn more about how the Pythagoras’ Formula can be used to find any distance, read this article:\nHow To Measure Any Distance With The Pythagorean Theorem *We’ve underestimated the Pythagorean theorem all along. It’s not about triangles; it can apply to any shape. It’s not…*betterexplained.com\nThe actual formual for euclidean distance is: Notice how it’s very similar to Pythagoras’ therom.\nLet’s say we have this table: We want to find the distance between these 2 cars, so we insert the values into the Euclidean Distance formula as such: Thus the distance is around 220.\nWhat About Qualitative Data? Qualitative data is data that cannot be represented as a number. Let’s try an example: Table inspired by slides from Frank Wolter, AI Lectuer @ UoL Lets say you have data of the weather over 4 days and whether or not children in a school played during breaktime. There’s no numbers here, so we cannot use the Euclidean Distance. You cannot square a concept like “sunny” or squareroot the resultant concept.\nSo we have to find a different way of measuring how close a day is to another. Let’s take day 1 and day 3. How simialr are they? There is an equally natural way of doing this, we just count the number of features on which the days are the same.\nSo day 1 and 2 have all the same features apart from wind, so we give it a distance of 1. We count the amount of features that differ.\nSo day 1 and 3 have a distance of 2.\nYou can easily change this corresponding to what you believe is more important, this method isn’t a one size fits all method but rather a method that you, as a human being (and not AI; hopefully) need to decide on.\nWhat defines an ‘neighbour’ You may have spotted something weird earlier that I didn’t expand on here: Feature space How was the size of the circle decided? Well, good question. This is a very typical problem in learning. Do you want the nearest 3 neighbours or the nearest 20 neighbours? Well, that all depends on your training data. This is entirely down to you.\nImplementing K Nearest Neighbour in Python So now we understand K nearest neighbour, lets implement it in Python. Here is the data we’ll be using in Comma Seperated Value (CSV) format.\nNote: This section uses code from here,here, and here.\nDownload the file here either by running the command\nwget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\nIn a Terminal or by selecting all the lines on the website, copying them and pasting them into a text file called iris.data.\nNext up we want to read in the data like so:\nThe csv module allows us to work with comma seperated value (CSV) files. If you were to print it by adding the following to the code\nYou would get something like this as an output, but much longer.\n6.5, 3.2, 5.1, 2.0, Iris-virginica 6.4, 2.7, 5.3, 1.9, Iris-virginica 6.8, 3.0, 5.5, 2.1, Iris-virginica 5.7, 2.5, 5.0, 2.0, Iris-virginica 5.8, 2.8, 5.1, 2.4, Iris-virginica 6.4, 3.2, 5.3, 2.3, Iris-virginica\nNow we need to make 2 datasets, one is a training dataset and the other is a test dataset. The training dataset is used to allow K-nearest neighbour (KNN) to make predictions (note: KNN does not *generalise *the data)\nThe file we just loaded was loaded in as strings, so we need to turm them into numbers we can work with then we need to split the data set randomly into training and testing datasets. A ratio of 2/3 for training and 1/3 for testing is normally the standard ratio used.\nNote to the reader: I had a hard time trying to import the iris data on my own, so this code is exactly copied from here.\nNow we want to calculate the Euclidean Distance between 2 points, since the flower dataset is mostly numbers and a small string at the end, we can just use the euclidean distance and ignore the final feature.\nWoah! What is that? This is functional programming. I’ve just completed a 12-week functional programming class and as my lectuer said:\n By the end of 12 weeks, you will be the person who always says “this could be done in 1 line in functional” to every bit of code you read\n Let’s divert quickly from AI and have a run down over this functional code. With functional, it’s best to read inwards to outwards so we’ll start with the first functional bit of code\nNote: the remove_flowers function will be talked about later, but without this here it causes an error in the code later on. remove_flowers just turns an item of data like\n5.0,3.4,1.5,0.2,Iris-setosa\ninto\n5.0,3.4,1.5,0.2\nMap is a function which applies a function to every item in a list. The function it is applying is a lambda (anonymous) function. An lambda function is a function which is only used for one purpose, so it doesn’t need to have its own define statement. The syntax for a lambda looks like this:\nLambda var1, var2: calculation\nIn this instance we have 2 variables, x and y and we are putting them into Pythagoras’ Therom. The map syntax is:\nmap ( function, list)\nBut our map takes 2 lists, X and Y. When it does the maths, it takes the first element of list1 and makes it into a variable called x, then it takes the second element of list2 and makes it into a variable called y. it does this until all items in both lists are finished.\nThe last part, list, turns our map into a list. So we get a list containing the Pythagoras’ Therom output of every item in both list1 and list2, which is just\n[4, 4, 4]\nOur next piece of code is\nNow we know what list(map(… does we can just ignore it and treat it as a function which returns [4, 4, 4].\nSo that leaves us with this funky function:\nNow, reduce is just foldl from Haskell but implemented in Python. It takes a list and turns it into one single value. It does this by applying a function to a list. The function it is applying is again a lambda function, where it adds the x and y together. What are x and y? Say we have a list like so\n[1, 2, 3]\nthen when we add x + y we get\n[1, 2, 3] x = 1, y = 2 1 + 2 = 3 x = 3, y = 3 x + y = 6\nNow, we know what the lambda function does inside reduce, what list is it applying to? Well, reduce is taking the list produced by our earlier map function and turning it into one value.\nSo list is taking [4, 4, 4] and just adding them together. 4 + 4 + 4 = 12. So the output is 12.\nNow we just need to find the square root of it all, which is simple.\nAlthough this may not be readable, it’s simple to understand once you know some functional programming.\nThere’s a slight error, the flowers all end with a string representing what flower it is. We can’t perform math on this, so we have to use this function\nto any list we pass to the Euclidean Distance function.\nNow we need to find the distance from every item in the training set to the item in the test set.\nThis code simply maps a lambda (anonymous) function onto every element in the training and test sets which finds the distance between the two elements and returns those distances in a list. The second part then zips together the element with the corresponding distance. So the first part returns\n[3.4641016151377544, 0.0]\nWhen given these parameters\ntrainSet = [[2, 2, 2, ‘a’], [4, 4, 4, ‘b’]]testInstance = [4, 4, 4]k = 1\nand the second part returns this\n[([2, 2, 2, ‘a’], 3.4641016151377544), ([4, 4, 4, ‘b’], 0.0)]\nThe last part to the code orders the code and returns K many neighbours, in this instance K = 1 so it just returns\n[[4, 4, 4, ‘b’]]\nNext we need to somehow vote on what K could be, now that we know its neighbours. We’ll create a voting system. Each neighbor will vote on its attribute (IE, what it is) and the majority vote will be taken as the prediction. So if the majority of its neighbours are of class “a” then the prediction will be of class “a”.\nSo this just goes through all the nearest neighbours and keeps a dictionary. The dictionary is just the flowername + how many times it appears. Each flower votes for their type into the dictionary, the dictionary is sorted so the most amount of vote appears in the first item in the dictionary and then the predicted flower is returned.\nAnd that’s it. We’ve put together a K-Nearest Neighbours classifier. If you want to read more about implementing this algorithm I suggest reading this or this.\nK-Nearest Neighbour vs Naieve Bayes It’s entirely down to you when to implement KNN or Naieve Bayes. There are some advantages and disadvantages to both, so I’ll list them here:\nK-Nearest Neighbour Advantages\n Simple but effective Only a single parameter, K, easily leanred by cross validation  K-Nearest Neighbour Disadvantages\n What does nearest mean? Need to define a distance measure Computational cost — Must store and search through the entire trainign set at test time.  Note: The Netflix progress prize winner was essentially K-Nearest Neighbour.\nNaieve Bayes Advantages\n Very simple to implement Needs less training data Can generalise the data  Naieve Bayes Disadvantages\n Accuracy increases with the more data it has, so to get a 95% or higher accuracy you’ll need alot of data Strong Independence Assumptions  You should now have a firm foundation in machine learning. Instead of teaching you an algorithm that will very likely be replaced in 3 years, I decided to teach the fundamental mathematicals and theroy that define these algorithms and then show 2 of the most widely used algorithms that will likely be around for a while.\n/*(  Produced by Brandon Skerritt https://skerritt.tech Instagram: @brandon.codes Email: brandon@skerritt.tech\nRemove stop words Create frequency table of words - how many times each word appears in the text Assign TF score to each sentence depending on the words it contains and the frequency table Assign IDF Score to each sentence, same as above Build summary by adding every sentence above a certain score threshold Only chooses top 3 highest scoring sentences\nRequirements: JQuery */\nfunction prettify(document){ // Turns an array of words into lowercase and removes stopwords const stopwords = [“a”, “\", “share”, “linkthese”, “about”, “above”, “after”, “again”, “against”, “all”, “am”, “an”, “and”, “any”,“are”,“aren’t”,“as”,“at”,“be”,“because”,“been”,“before”,“being”,“below”,“between”,“both”,“but”,“by”,“can’t”,“cannot”,“could”,“couldn’t”,“did”,“didn’t”,“do”,“does”,“doesn’t”,“doing”,“don’t”,“down”,“during”,“each”,“few”,“for”,“from”,“further”,“had”,“hadn’t”,“has”,“hasn’t”,“have”,“haven’t”,“having”,“he”,“he’d”,“he’ll”,“he’s”,“her”,“here”,“here’s”,“hers”,“herself”,“him”,“himself”,“his”,“how”,“how’s”,“i”,“i’d”,“i’ll”,“i’m”,“i’ve”,“if”,“in”,“into”,“is”,“isn’t”,“it”,“it’s”,“its”,“itself”,“let’s”,“me”,“more”,“most”,“mustn’t”,“my”,“myself”,“no”,“nor”,“not”,“of”,“off”,“on”,“once”,“only”,“or”,“other”,“ought”,“our”,“ours”,“ourselves”,“out”,“over”,“own”,“same”,“shan’t”,“she”,“she’d”,“she’ll”,“she’s”,“should”,“shouldn’t”,“so”,“some”,“such”,“than”,“that”,“that’s”,“the”,“their”,“theirs”,“them”,“themselves”,“then”,“there”,“there’s”,“these”,“they”,“they’d”,“they’ll”,“they’re”,“they’ve”,“this”,“those”,“through”,“to”,“too”,“under”,“until”,“up”,“very”,“was”,“wasn’t”,“we”,“we’d”,“we’ll”,“we’re”,“we’ve”,“were”,“weren’t”,“what”,“what’s”,“when”,“when’s”,“where”,“where’s”,“which”,“while”,“who”,“who’s”,“whom”,“why”,“why’s”,“with”,“won’t”,“would”,“wouldn’t”,“you”,“you’d”,“you’ll”,“you’re”,“you’ve”,“your”,“yours”,“yourself”,“yourselves”, “this”]; // turn document into lowercase words, remove all stopwords var document = document.replace(/[.,]/g, ‘'); let document_in_lowercase = document.split(” “).map(function(x){ return x.toLowerCase() }); return document_in_lowercase.filter( x = !stopwords.includes(x) ); }\nfunction countWords(words){ // returns a dictionary of {WORD: COUNT} where count is // how many times that word appears in “words”. const unique_words = uniqueWords(words); let dict = {}; // for every single unique word for (let i = 0; i function uniqueWords(words){ const unique_words_set = new Set(words); return unique_words = Array.from(unique_words_set); }\nfunction termFrequency(document){ // calculates term frequency of each sentence words_without_stopwords = prettify(document);\n// gets rid of trailing spaces const sentences = document.split(\".\").map(item = item.trim()); sentences[0] = sentences[0].substring(146); const TFVals = countWords(words_without_stopwords) const unique_words = uniqueWords(words_without_stopwords); // actually makes it TF values according to formula for (const [key, value] of Object.entries(TFVals)){ TFVals[key] = TFVals[key] / words_without_stopwords.length; } // splits it up into sentences now var TFSentences = {}; // for every sentence for (let i = 0; i  }\n// each document is a sentence function inverseDocumentFrequency(document){ // calculates the inverse document frequency of every sentence const words_without_stopwords = prettify(document); const unique_words_set = uniqueWords(words_without_stopwords);\nconst sentences = document.split(\".\").map(item = item.trim()); sentences[0] = sentences[0].substring(146); const lengthOfDocuments = sentences.length; // prettifys each sentence so it doesn't have stopwords const wordCountAll = countWords(words_without_stopwords); // counts words of each sentence // as each sentence is a document wordCountSentences = []; for (let i = 0; i  }\nfunction longerSentenceWeighting(sentence){ // longer sentences are weighted better // has to be how many words // number was randomly chosen sentence = sentence.toString() return (length(sentence.split(” “)) * 1.5); }\nfunction isNumber(n) { return !isNaN(parseFloat(n)) \u0026\u0026 isFinite(n); }\nfunction numberWeighting(sentence){ // negative weighting on setnences with just numbers in them sentence = sentence.toString() temp = sentence.split(” “); weighting = 1.0; for (var item in temp){ if (isNumber(item)){ // more numbers in a sentence, the harsher the weighting // but still gets the option of having one or two numbers // number was randomly chosen weighting = weighting * 0.8 } } return(temp * weighting) }\nfunction TFIDF(documents){ // calculates TF*IDF const TFVals = termFrequency(documents); const IDFVals = inverseDocumentFrequency(documents);\nlet TFidfDict = {}; for (const [key, value] of Object.entries(TFVals)){ if (key in IDFVals){ TFidfDict[key] = TFVals[key] * IDFVals[key] * longerSentenceWeighting(value) * numberWeighting(value); } } let max = 0.0; let max2 = 0.0; let max3 = 0.0; let max_sentence = \"\"; let max2Sent = \"\"; let max3Sent = \"\"; // finds the top 3 sentences in TFidfDict for (const [key, value] of Object.entries(TFidfDict)){ if (TFidfDict[key]  max){ max = TFidfDict[key]; max_sentence = key; } else if (TFidfDict[key]  max2 \u0026\u0026 TFidfDict[key] max3 \u0026\u0026 TFidfDict[key]  }\n// get all text from .story-body within p tags on a BBC news web article\n// console.log(termFrequency(“Hello, my name is Brandon. Brandon Brandon. The elephant jumps over the moon”));\n// get all text from .story-body within p tags on a BBC news web article let $article = $('.post-full-content’).find(‘p’).text(); console.log($article) // insert text into body of document let $insert = TFIDF($article); // let insert = $('.story-body’).prepend(TFIDF($article));\nFeel free to connect with me: LinkedIn | GitHub\n",
  "wordCount" : "6217",
  "inLanguage": "en",
  "datePublished": "2019-03-11T14:48:10Z",
  "dateModified": "2019-03-11T14:48:10Z",
  "author":{
    "@type": "Person",
    "name": "Bee"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://polymath.cloud/ml-fundamentals/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Polymath.cloud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://polymath.cloud/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://polymath.cloud" accesskey="h">Polymath.cloud</a>
            <span class="logo-switches">
                <span class="theme-toggle">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://polymath.cloud/archives" title="Archive">
                    <span>
                        Archive
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/search/" title="Search">
                    <span>
                        Search
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/tags/" title="Tags">
                    <span>
                        Tags
                    </span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Absolute Fundamentals of Machine Learning
    </h1>
    <div class="post-meta">

March 11, 2019&nbsp;·&nbsp;30 min&nbsp;·&nbsp;Bee

    </div>
  </header> 

  <div class="post-content">
<h3 id="absolute-fundamentals-of-machine-learning">Absolute Fundamentals of Machine Learning<a hidden class="anchor" aria-hidden="true" href="#absolute-fundamentals-of-machine-learning">#</a></h3>
<p><img src="https://cdn-images-1.medium.com/max/800/1*X3TNZbdu06LWNCpHixIFxQ.jpeg" alt=""><a href="https://unsplash.com/photos/0E_vhMVqL9g">https://unsplash.com/photos/0E_vhMVqL9g</a>
Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to teach in this article.</p>
<p>I found that learning the theroetical side alongside the programming side makes it easier to learn both, so this article features both easy to understand mathematics and the algorithms implemented in Python. Also, technology becomes outdated — fast. The code used in this tutorial will likely be meaningless in 5 years time. So for that reason, I’ve decided to also teach the mathematical side to Machine Learning that will not die out in a few years.</p>
<h3 id="what-is-machine-learning">What is machine learning?<a hidden class="anchor" aria-hidden="true" href="#what-is-machine-learning">#</a></h3>
<p>Let’s start with learning. Learning is a long and complicated process to describe but in a nutshell it is turning experience into knowledge.</p>
<p>Machine learning is teaching machines how to learn, as insane as that sounds it’s actually plausable using probability.</p>
<p>I highly reccomend you read this <a href="https://medium.com/brandons-computer-science-notes/an-introdcution-to-probability-45a64aee7606">article on probability</a>, as it is the essential foundation to machine learning and artifical intelligence. We will go over conditional probability and Bayes therom again in this article.</p>
<h3 id="types-of-learning">Types of learning<a hidden class="anchor" aria-hidden="true" href="#types-of-learning">#</a></h3>
<p>There are many different ways for a machine to learn, in this I try to explain the different types.</p>
<h4 id="supervised-learning">Supervised Learning<a hidden class="anchor" aria-hidden="true" href="#supervised-learning">#</a></h4>
<p>In supervised learning we start with a dataset that has training examples, each example has an assiocated label which identifies it.</p>
<p>An example of this is presented below:
<img src="https://cdn-images-1.medium.com/max/800/1*RzTRpIv0IPvTuwRLdtAoWw.jpeg" alt="">Google ReCaptcha
We want the machine to be able to identify drinks, so we present the machine with 9 images, some containing drinks. We then select the pictures that contains the drinks, teaching the computer what a drink looks like.</p>
<p>It does this by running labelled data through a **learning algorithm. **The goal of supervised learning is to be able to correctly identify new data given to it, having learnt how to identify the data using the previous data set and learning algorithm.</p>
<h4 id="unsupervised-learning">Unsupervised Learning<a hidden class="anchor" aria-hidden="true" href="#unsupervised-learning">#</a></h4>
<p>Unsupervised learning is quite different from supervised in the sense that it almost always does not have a definite output. The learning agent aims to find structures or patterns in the data.</p>
<p>A good article on unsupervised learning can be found <a href="https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294">here</a>.</p>
<h4 id="reinforcement-learning">Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning">#</a></h4>
<p>Reinforcement learning is where the learner receives rewards and punishments for its actions. The reward could simply be utility and the agent could be told to receive as much utility as possible in order to “win”. Utility here could just be a normal variable.</p>
<p>A good example of reinforcement learning is this:</p>
<h3 id="machine-learning">Machine Learning<a hidden class="anchor" aria-hidden="true" href="#machine-learning">#</a></h3>
<p>Now we understand some of the terminiology of machine learning, we’re actually going to teach a machine something. In order for us to do that, we need to learn a bit of probability.</p>
<p>Most of the probability here is directly copied off of another <a href="https://medium.com/brandons-computer-science-notes/an-introdcution-to-probability-45a64aee7606">blogpost I wrote</a> on probability, but I’ll only include the important parts required for machine learning here.</p>
<h3 id="conditional-probability">Conditional Probability<a hidden class="anchor" aria-hidden="true" href="#conditional-probability">#</a></h3>
<p>Conditional probability is where an event can only happen if another event has happened. Let’s start with an easy problem:</p>
<p>John&rsquo;s favourite programming languages are Haskell and x86 Assembley. Let A represent the event that he forces a class to learn Haskell and B represent the event that he forces a class to learn x86 Assembley.On a randomly selected day, John is taken over by Satan himself, so the probability of P(A) is 0.6 and the probability of P(B) is 0.4 and the conditional probability that he teaches Haskell, given that he has taught x86 Assembley that day is P(A|B) = 0.7.Based on the information, what is P(B|A), the conditional probability that John teaches x86 Assembley given that he taught Haskell, rounded to the nearest hundredth?</p>
<p>Therefore the probability of P(A and B) = P(A|B) * P(B); read “|” as given, as in, “A|B” is read as “A given B”. It can also be written as P(B|A) * P(A).</p>
<p>The reason it is P(A|B) * P(B) is because given the probability of “Given the probability that B happens, A happens” and the probability of B asP(B). (A|B) is a different probability to P(B) and P(A and B) can only happen if P(B) happens which then allows P(B|A) to happen.</p>
<p>So we can transform this into a mathematical formula:</p>
<p>P(A and B) = P(A|B) * P(B) = 0.7 * 0.5 = 0.35Solving it
P(B|A) * P(A)
P(A) = 0.5
So
0.6 * P(B|A)
Now we don&rsquo;t know what P(B|A) is, but we want to find out. We know that P(B|A) must be a part of P(A and B) because P(A and B) is the probability that both of these events happen so&hellip;P(A and B) = 0.350.35 = P(B|A) * 0.5
With simple algebraic manipulation
0.35/0.5 = P(B|A)
P(B|A) = 0.7</p>
<h3 id="bayes-therom">Bayes Therom<a hidden class="anchor" aria-hidden="true" href="#bayes-therom">#</a></h3>
<p>Bayes Therom allows us to work out the probability of events given prior knowledge about the events. It is more of an observation than a therom, as it correctly works all the time. Bayes therom is created by Thomas Bayes, who noted this observation in a notebook. He never published it, so he wasn’t recgonised for his famous therom during his life time.
<img src="https://cdn-images-1.medium.com/max/800/1*4NP-Lj4PxOP98zmv6IfilA.png" alt="">From <a href="https://betterexplained.com/articles/colorized-math-equations/">https://betterexplained.com/articles/colorized-math-equations/</a>
Confusing, right? Let us look at an example.</p>
<p>Suppose a new drug is found on the streets and the police want to identify whether someone is a user or not.The drug is 99% sensitive, that is that the proportion of people who are correctly identified as taking the drug.The drug is 99% specific, that is that the proportion of people who are correctly identified as not taking the drug.Note: there is a 1% false positive rate for both users and non users.Suppose that 0.5% of people at John Moores (A rival university) takes the drug. What is the probability that a randomly selected John Moores student with a positive test is a user?</p>
<p>Let’s colourfy this and put this into the equation
<img src="https://cdn-images-1.medium.com/max/800/1*p-RPfG6rszMYVuzVHrJy8A.png" alt="">
Note: we don’t directly know the chance of a positive test actually being positive, it isn’t given to us so we have to calculate it, that is why the formula expands on the denominator in the second part.</p>
<h4 id="learning-with-bayes-therom">Learning with Bayes Therom<a hidden class="anchor" aria-hidden="true" href="#learning-with-bayes-therom">#</a></h4>
<p>Now, bayes therom isn’t designed to be used only once. It is designed to work with continus data. Baye originally created this using a thought experiment. He wondered that if his assistant threw a ball on the table, can he predict where it will land?</p>
<p>Baye asked his assistant to do just that. Then again, and to tell him whether it was left or right of the original spot. He noted down every single spot it hit, and over time, with each throw, he got better at predicting where it’ll land. This is where bayes therom comes into play, by repeatedly using bayes therom we can overtime calculate where the ball might land more accurcately.</p>
<p>Let’s do a quick example just to get some numbers:
<img src="https://cdn-images-1.medium.com/max/800/1*R9Bnx_rVyXqNJS64zSHTCA.png" alt="">Medium doesn’t support colours, so I have to copy and paste images. Sorry!
So what would happen if you went to another hospital, got a second opinion and had the test run again by a different lab? This test also comes back as positive, so what is the probability you have the disease?</p>
<p>We change the postieor probability of having the disease before the test to 9%, so it looks like this:
<img src="https://cdn-images-1.medium.com/max/800/1*LyJu_eojIeAF9-MHQYO5WQ.png" alt="">
Now the new number is 0.9073319 or 91% chance of having the disease.</p>
<p>Which makes sense, 2 positive results from 2 different labs are unlikely to be conincidental. And if you get more lab results done and they all come back as positive, this number will gradually increase towards 100%.</p>
<h3 id="naive-bayes-classifier">Naive Bayes Classifier<a hidden class="anchor" aria-hidden="true" href="#naive-bayes-classifier">#</a></h3>
<p>Naive Bayes Classifier is a type of classifier that is based on Bayes’ Therom with an assumption of independence among predictors. A Naive Bayes Classifier assumes that the presence of a particular feature is unrelated to the presence of any other feature. For example, a frut may be considered an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existance of all other features of an apple, all of these contribute to the probability that the fruit is an apple and that is why this classifier is known “ Naive”.</p>
<p><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Wikipedia </a>put it nicely:</p>
<blockquote>
<p>In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 4&rdquo; in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple.</p>
</blockquote>
<p><strong>Problem</strong></p>
<p>Suppose we have 2 datasets, “play” and “weather”. We want to see the possibillities of playing given the weather.
<img src="https://cdn-images-1.medium.com/max/800/1*yjbGBM9AUU8JVuT7q-2C0w.png" alt="">Image from <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/</a></p>
<ol>
<li>The first step is to turn the data into a frequency table</li>
<li>The second step is to create a likelihood table by finding the probabilities like Overcast = 0.29 and the probability of playing is 4 / 14 (where 14 is the amount of times they play overall) and 4 is the number of times they played in that weather previously.</li>
<li>Now use the Naieve Bayesian equation to calculate the conditional probability for each class.</li>
</ol>
<p>Here’s an example problem question solved using the above data and the equation.</p>
<p><strong>Is the statement “players will play if weather is sunny” correct?</strong></p>
<p>P(Yes | Sunny) = P(Sunny|Yes) * P(Yes) /P(Sunny)</p>
<p>Remember Bayes Therom? This is the same but with variable names inputted to make it easier to read.</p>
<p>Now we have P(Sunny|Yes) = 3/9 = 0.33.
P(Sunny) = 5/14 = 0.36.
P(yes) = 9/14 = 0.64Now using Bayes Therom we can work out
P(Yes|Sunny) = 0.33 * 0.674 / 0.36 = 0.60 which has high probability.Thus, given that it is sunny, there is a 60% chance they will play.</p>
<h3 id="machine-learning-using-python">Machine Learning using Python<a hidden class="anchor" aria-hidden="true" href="#machine-learning-using-python">#</a></h3>
<p>Wow, what a boring read that was. “Theoretical Computer Science is boring” I hear you say. Well, you’ll be excited to know this next part is about the application of machine learning using Python.</p>
<p>Let’s calculate whether an email is spam or ham (that is, a normal email) using machine learning.</p>
<p>So, given a large corpus of ham and spam emails, we need to train the model. We’ll start by calculating for each word in each email the number of times that word appears in spam and ham emails.</p>
<p>We’ll then calculate a “spaminess” level of each word using a formula. The formula we will be using is quite simple.
<img src="https://cdn-images-1.medium.com/max/800/1*8s6GK2l4s0gttxJ_2EMbMA.png" alt="">
So, the numerator (the number on top) shows how many times the word appears in a spam email, and the denominator (the bottom) shows how many times it appears in any email.</p>
<p>So, lets say there are 9 spam emails and 10 ham emails, making 19 total. If a word appears in 9 spam emails, but 1 ham email than the equation is 9/10 which gives us a 90% chance that this word means it is a spam email.</p>
<p>But we cannot simply just count one word, we need to do this for all the words in the email.</p>
<p>We essentially have 3 steps:</p>
<ol>
<li>Find the spamminess of each word in a message.</li>
<li>Find the total spamminess of a message, say by multiplying together the spaminess levels of each word. Call this S[M].</li>
<li>Then find the haminess of each word, by multiplying together the (1 — spaminess) level of each word. Call this H[M].</li>
</ol>
<p>Then if S[M] &gt; H[M] then the message is spam, else it is ham.</p>
<p>Okay, so let’s implement this. We’ll be using the bodies of emails only, and there won’t be many emails just to make the explanation easy. Also, the spam emails are **fairly **obvious. Below is the bad emails text file I’m using</p>
<p>Viagra for sale, get sexy body soon!
U want sum viagra bby? u sexy s00n
VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA
SEXY SOON VIAGRA PLEASE YASSSSS
VIAGRA BUY PLEASE
I really like today</p>
<p>And the good emails</p>
<p>Hello, how are you?
I hope you have the work ready for January the 8th.
Do you like cats or dogs?
Viagra
viagra</p>
<p>So the first step is to somehow take this file and split it up.</p>
<p>The code reads the file, and splits it per new line, spitting out 2 lists. The lists look like this. Below is the bad emails</p>
<p>[&lsquo;Viagra for sale, get sexy body soon!', &lsquo;U want sum viagra bby? u sexy s00n&rsquo;, &lsquo;VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA&rsquo;, &lsquo;SEXY SOON VIAGRA PLEASE YASSSSS &lsquo;, &lsquo;VIAGRA BUY PLEASE&rsquo;, &lsquo;I really like today&rsquo;]</p>
<p>And the good emails</p>
<p>[&lsquo;Hello, how are you?', &lsquo;I hope you have the work ready for January the 8th.', &lsquo;Do you like cats or dogs?', &lsquo;Viagra&rsquo;, &lsquo;viagra&rsquo;]</p>
<p>Okay, now we need to calculate the spaminess of each word. We do this by finding the word, and seeing how many times it appears in the bad emails and how many times it appears in the good emails, basically the formula seen here:
<img src="https://cdn-images-1.medium.com/max/800/1*8s6GK2l4s0gttxJ_2EMbMA.png" alt="">
So to find out how many words appear in bad and good emails, we could use nested for loops; but I’ve opted to go a bit functional here.</p>
<p>So, the first line is a lambda (anonymous) function which just turns a list of lists into one list. Anonymous functions are functions that are only intended to be used once or twice. The second and third line do the same. First, they map the function “x.split(“ “)” onto each email in the list, generating a new list of list of words in the email, then they apply the flatten function to make the list of lists into one list.</p>
<p>Now we have 2 lists, one containing all the words in a bad email and one containing all the good words in a good email.</p>
<p>Now we create a function that calculates the spamminess level of a word. First, it counts how many times a word appears in the bad emails. Then it counts how many times it appears in both emails. Then it returns how many times it appears in bad emails divided by how many times it appears in total emails.</p>
<p>Now we want a dictionary of every word and its spaminess level.</p>
<p>word_Dict shall now return</p>
<p>{&lsquo;body&rsquo;: 1.0, &lsquo;': 1.0, &lsquo;want&rsquo;: 1.0, &lsquo;get&rsquo;: 1.0, &lsquo;I&rsquo;: 0.5, &lsquo;January&rsquo;: 0.0, &lsquo;s00n&rsquo;: 1.0, &lsquo;SOON&rsquo;: 1.0, &lsquo;you?': 0.0, &lsquo;dogs?': 0.0, &lsquo;sale,': 1.0, &lsquo;U&rsquo;: 1.0, &lsquo;are&rsquo;: 0.0, &lsquo;Viagra&rsquo;: 0.5, &lsquo;sexy&rsquo;: 1.0, &lsquo;ready&rsquo;: 0.0, &lsquo;SEXY&rsquo;: 1.0, &lsquo;the&rsquo;: 0.0, &lsquo;8th.': 0.0, &lsquo;really&rsquo;: 1.0, &lsquo;Do&rsquo;: 0.0, &lsquo;Hello,': 0.0, &lsquo;BUY&rsquo;: 1.0, &lsquo;bby?': 1.0, &lsquo;like&rsquo;: 0.5, &lsquo;for&rsquo;: 0.5, &lsquo;sum&rsquo;: 1.0, &lsquo;work&rsquo;: 0.0, &lsquo;PLEASE&rsquo;: 1.0, &lsquo;VIAGRA&rsquo;: 1.0, &lsquo;YASSSSS&rsquo;: 1.0, &lsquo;how&rsquo;: 0.0, &lsquo;viagra&rsquo;: 0.5, &lsquo;cats&rsquo;: 0.0, &lsquo;u&rsquo;: 1.0, &lsquo;hope&rsquo;: 0.0, &lsquo;have&rsquo;: 0.0, &lsquo;you&rsquo;: 0.0, &lsquo;or&rsquo;: 0.0, &lsquo;today&rsquo;: 1.0, &lsquo;soon!': 1.0}</p>
<p>Given a message, this code will compute how spammy the message is by multiplying the spaminess level of each word. The reduce function is foldl in haskell, it takes a list and a function and returns a single value by applying the function to every item of the list. In this case, given a list of [x, y, x, y, x, y] it will multiply x*y and then add that together.</p>
<p>Now we need to find the haminess of each message, not so hard now that we can find the spaminess.</p>
<p>The haminess is 1 — the spaminess</p>
<p>Now we just need to create a function that determines whether it is spam or ham.</p>
<p>If it is true, it is spam, if not; it’s ham. Obviously this won’t work so well with such a limited dataset, but if we had thousands of emails this should work well, despite the initial startup time being slow.</p>
<p>We just used a naieve bayes classifer — a supervised machine learning based approach to spam detection. Isn’t that really cool sounding?</p>
<h3 id="k-nearest-neighbour">K-Nearest Neighbour<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbour">#</a></h3>
<p>Like Naieve Bayes above, let’s create an intution about K-nearest neighbour.</p>
<p>Let’s start off with some facts to build up this intuition.</p>
<p>A point is a location. It has no size, but much like any location it shows you where something is. It can be represented in different ways, depending on how many <a href="https://science.howstuffworks.com/science-vs-myth/everyday-myths/dimension.htm">dimensions </a>it lives in.</p>
<p>A line is a 1 dimensional object. We can represent a location on a line like so
<img src="https://cdn-images-1.medium.com/max/800/1*RhumPny-HLtWeRUVUK0cBQ.png" alt="">
So say the line starts at the number 3 and ends at the number 9, a location is any number between 3 and 9. In this diagram, it is represented as X.</p>
<p>In a 2-dimensional object such as a square, we can represent the location using X and Y coordinates.
<img src="https://cdn-images-1.medium.com/max/800/1*JZ8XKa2eKYoy8glSbem8fA.png" alt="">
Any location in this square can be reached using X and Y coordinatess.</p>
<p>A cube is a 3-dimensional square, we can represent its location using 3 coordinates
<img src="https://cdn-images-1.medium.com/max/800/1*OhQEZlQn6l2PQvWjc20QYQ.png" alt="">Taken from <a href="http://www.petercollingridge.co.uk/book/export/html/460">here</a>
A point in geometry is a location. It has no size i.e. no width, no length and no depth. A point is shown by a dot.</p>
<p>So a point in an n-dimensional space needs n coordinates to represent it. We find it hard to visualise anything more than 3 dimensional space (as our world is 3 dimensional).</p>
<p>An n-dimensional space is represented by an n-dimensional <strong>hypercube</strong>.</p>
<p>A hypercube is just a fancy word meaning that any location can be represented as a tuple of n numbers.</p>
<p>Now, back to the example of stopping spam. Any email message can be represented as a point in a hypercube.</p>
<p>Let’s say for example, we have a list of every single possible word that could ever appear in an email.</p>
<p>So there are:</p>
<p>W1, W2, &hellip;, Wn</p>
<p>Words in the list. Let’s simplfy this, lets say every single word that could appear in an email is</p>
<p>(&lsquo;Hello&rsquo;, &lsquo;Goodbye&rsquo;, &lsquo;Email&rsquo;, &lsquo;Cheese&rsquo;, &lsquo;United Kingdom&rsquo;)</p>
<p>for simplicities sake, we’ll pretend only these words are possible in an email.</p>
<p>Every single email will contain a subset of the words above like so:</p>
<p>“Hello, Cheese.” or “United Kingdom, Goodbye.”</p>
<p>Each of these messages can be represented as a list of 1’s and 0’s, just like bit vectors.</p>
<p>So for the first message:</p>
<p>(&lsquo;Hello&rsquo;, &lsquo;Cheese&rsquo;)
(1, 0, 0, 1, 0)</p>
<p>So we simply turn the words that are in both the email and the universe of possible words into a 1, in this instance the first element, “Hello”, and the fouth element, “cheese” are turned into 1s.</p>
<p>So now we’ve taken a message and turned it into a point in an n-dimensional hypercube.</p>
<p>Now we want to do this for the email we want to classify as well as every email we hold information about. So we have training data, and a message to classify.</p>
<p>Now we have a bunch of points in an n-dimensional hypercube, we want to calculate the distance between each point. There are many ways to calculate the distance. We’ll be using the <a href="https://en.wikipedia.org/wiki/Euclidean_distance"><strong>Euclidean Distance Formula</strong></a> here.
<img src="https://cdn-images-1.medium.com/max/800/1*phAWDJmnLx9TmPD7kBC3_A.gif" alt="">Taken from <a href="https://hlab.stanford.edu/brian/making7.gif">here</a>. More on this later.
When we insert a new email to be classified, we check its nearest neighbours. If most of the emails near it are spam, then the email is likely to be spam. But if most of the emails near it are ham, then the email is likely to be ham.
<img src="https://cdn-images-1.medium.com/max/800/1*Wh8FzeWpN_VUw8xYahGzKQ.png" alt="">The feature space for spam and ham emails
So we have our training data classified and this is what it looks like on a graph. When we place our email to be classified into it, the graph will look like:
<img src="https://cdn-images-1.medium.com/max/800/1*G0fYQgx3UsVf8aMPwaulIA.png" alt="">Feature space
We know where to place the email to be classified because we turned it into a set of coordinates earlier.
<img src="https://cdn-images-1.medium.com/max/800/1*GBa1jEh0eHqZPYhlM9REwQ.png" alt="">Feature space
We draw a circle around the email to be classified and since most of the spam emails are inside the circle, we mark this email as spam. This is slightly simpler than reality.</p>
<p>When we categorised each email as a bit vector containing 0 and 1s, normally we would hash subsets of the email and use that instead and the distance will likely not be a straight euclidean formula but a more sophisticated formula that works well in training.</p>
<h4 id="distance-measurements">Distance Measurements<a hidden class="anchor" aria-hidden="true" href="#distance-measurements">#</a></h4>
<p>I said that Euclidean Distance was one of the ways you could calculate the distance, in this section I’m going to talk a little more about that.</p>
<p>The Euclidean Distance is used for **quantitative **data, data that is only numbers.</p>
<h4 id="what-is-euclidean-distance">What is “Euclidean Distance”?<a hidden class="anchor" aria-hidden="true" href="#what-is-euclidean-distance">#</a></h4>
<p>Well, first let’s go back to Pythagoras’ Therom. We all remember it as
<img src="https://cdn-images-1.medium.com/max/800/1*4JlqG-uqhSgxDz8LhcRpgQ.png" alt="">Taken from <a href="http://cdn.pythagorasandthat.co.uk/wp-content/uploads/2014/07/pythagoras-theorem-1024x675.png">here</a>
but Pythagoras’ therom can be applied to many instances. In short, it is findign the distance between 2 points at a right angle. Let’s say you walk 3 meters east and 4 meters north, how far away are you from the original source? Well, 5 meters as the crow flies. If you want to learn more about how the Pythagoras’ Formula can be used to find any distance, read this article:</p>
<p><a href="https://betterexplained.com/articles/measure-any-distance-with-the-pythagorean-theorem/"><strong>How To Measure Any Distance With The Pythagorean Theorem</strong></a>
<a href="https://betterexplained.com/articles/measure-any-distance-with-the-pythagorean-theorem/">*We&rsquo;ve underestimated the Pythagorean theorem all along. It&rsquo;s not about triangles; it can apply to any shape. It&rsquo;s not…*betterexplained.com</a></p>
<p>The actual formual for euclidean distance is:
<img src="https://cdn-images-1.medium.com/max/800/1*ucxWe05bMojzL6aUGIWH8Q.png" alt="">
Notice how it’s very similar to Pythagoras’ therom.</p>
<p>Let’s say we have this table:
<img src="https://cdn-images-1.medium.com/max/800/1*bfpwr8GxcKFRTOnMvL279g.png" alt="">
We want to find the distance between these 2 cars, so we insert the values into the Euclidean Distance formula as such:
<img src="https://cdn-images-1.medium.com/max/800/1*M-pUsMAlllq4yLav5bp-yQ.png" alt="">
Thus the distance is around 220.</p>
<h4 id="what-about-qualitative-data">What About Qualitative Data?<a hidden class="anchor" aria-hidden="true" href="#what-about-qualitative-data">#</a></h4>
<p>Qualitative data is data that cannot be represented as a number. Let’s try an example:
<img src="https://cdn-images-1.medium.com/max/800/1*kKKdXM2S79tzqO4K-Gx0jA.png" alt="">Table inspired by slides from Frank Wolter, AI Lectuer @ UoL
Lets say you have data of the weather over 4 days and whether or not children in a school played during breaktime. There’s no numbers here, so we cannot use the Euclidean Distance. You cannot square a concept like “sunny” or squareroot the resultant concept.</p>
<p>So we have to find a different way of measuring how close a day is to another. Let’s take day 1 and day 3. How simialr are they? There is an equally natural way of doing this, we just count the number of features on which the days are the same.</p>
<p>So day 1 and 2 have all the same features apart from wind, so we give it a distance of 1. We count the amount of features that differ.</p>
<p>So day 1 and 3 have a distance of 2.</p>
<p>You can easily change this corresponding to what you believe is more important, this method isn’t a one size fits all method but rather a method that you, as a human being (and not AI; hopefully) need to decide on.</p>
<h4 id="what-defines-an-neighbour">What defines an ‘neighbour’<a hidden class="anchor" aria-hidden="true" href="#what-defines-an-neighbour">#</a></h4>
<p>You may have spotted something weird earlier that I didn’t expand on here:
<img src="https://cdn-images-1.medium.com/max/800/1*GBa1jEh0eHqZPYhlM9REwQ.png" alt="">Feature space
How was the size of the circle decided? Well, good question. This is a very typical problem in learning. Do you want the nearest 3 neighbours or the nearest 20 neighbours? Well, that all depends on your training data. This is entirely down to you.</p>
<h4 id="implementing-k-nearest-neighbour-in-python">Implementing K Nearest Neighbour in Python<a hidden class="anchor" aria-hidden="true" href="#implementing-k-nearest-neighbour-in-python">#</a></h4>
<p>So now we understand K nearest neighbour, lets implement it in Python. Here is the data we’ll be using in Comma Seperated Value (CSV) format.</p>
<p>Note: This section uses code from <a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/">here,</a><a href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/">here,</a> and <a href="https://www.dataquest.io/blog/k-nearest-neighbors-in-python/">here</a>.</p>
<p>Download the file <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">here</a> either by running the command</p>
<p>wget <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a></p>
<p>In a Terminal or by selecting all the lines on the website, copying them and pasting them into a text file called <strong>iris.data</strong>.</p>
<p>Next up we want to read in the data like so:</p>
<p>The csv module allows us to work with comma seperated value (CSV) files. If you were to print it by adding the following to the code</p>
<p>You would get something like this as an output, but much longer.</p>
<p>6.5, 3.2, 5.1, 2.0, Iris-virginica
6.4, 2.7, 5.3, 1.9, Iris-virginica
6.8, 3.0, 5.5, 2.1, Iris-virginica
5.7, 2.5, 5.0, 2.0, Iris-virginica
5.8, 2.8, 5.1, 2.4, Iris-virginica
6.4, 3.2, 5.3, 2.3, Iris-virginica</p>
<p>Now we need to make 2 datasets, one is a training dataset and the other is a test dataset. The training dataset is used to allow K-nearest neighbour (KNN) to make predictions (note: KNN does not *generalise *the data)</p>
<p>The file we just loaded was loaded in as strings, so we need to turm them into numbers we can work with then we need to split the data set randomly into training and testing datasets. A ratio of 2/3 for training and 1/3 for testing is normally the standard ratio used.</p>
<p>Note to the reader: I had a hard time trying to import the iris data on my own, so this code is exactly copied from <a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/">here</a>.</p>
<p>Now we want to calculate the Euclidean Distance between 2 points, since the flower dataset is mostly numbers and a small string at the end, we can just use the euclidean distance and ignore the final feature.</p>
<p>Woah! What is that? This is functional programming. I’ve just completed a 12-week functional programming class and as my lectuer said:</p>
<blockquote>
<p>By the end of 12 weeks, you will be the person who always says “this could be done in 1 line in functional” to every bit of code you read</p>
</blockquote>
<p>Let’s divert quickly from AI and have a run down over this functional code. With functional, it’s best to read inwards to outwards so we’ll start with the first functional bit of code</p>
<p>Note: the remove_flowers function will be talked about later, but without this here it causes an error in the code later on. remove_flowers just turns an item of data like</p>
<p>5.0,3.4,1.5,0.2,Iris-setosa</p>
<p>into</p>
<p>5.0,3.4,1.5,0.2</p>
<p>Map is a function which applies a function to every item in a list. The function it is applying is a lambda (anonymous) function. An lambda function is a function which is only used for one purpose, so it doesn’t need to have its own define statement. The syntax for a lambda looks like this:</p>
<p>Lambda var1, var2: calculation</p>
<p>In this instance we have 2 variables, x and y and we are putting them into Pythagoras’ Therom. The map syntax is:</p>
<p>map ( function, list)</p>
<p>But our map takes 2 lists, X and Y. When it does the maths, it takes the first element of list1 and makes it into a variable called x, then it takes the second element of list2 and makes it into a variable called y. it does this until all items in both lists are finished.</p>
<p>The last part, list, turns our map into a list. So we get a list containing the Pythagoras’ Therom output of every item in both list1 and list2, which is just</p>
<p>[4, 4, 4]</p>
<p>Our next piece of code is</p>
<p>Now we know what list(map(… does we can just ignore it and treat it as a function which returns [4, 4, 4].</p>
<p>So that leaves us with this funky function:</p>
<p>Now, reduce is just foldl from Haskell but implemented in Python. It takes a list and turns it into one single value. It does this by applying a function to a list. The function it is applying is again a lambda function, where it adds the x and y together. What are x and y? Say we have a list like so</p>
<p>[1, 2, 3]</p>
<p>then when we add x + y we get</p>
<p>[1, 2, 3]
x = 1, y = 2
1 + 2 = 3
x = 3, y = 3
x + y = 6</p>
<p>Now, we know what the lambda function does inside reduce, what list is it applying to? Well, reduce is taking the list produced by our earlier map function and turning it into one value.</p>
<p>So list is taking [4, 4, 4] and just adding them together. 4 + 4 + 4 = 12. So the output is 12.</p>
<p>Now we just need to find the square root of it all, which is simple.</p>
<p>Although this may not be readable, it’s simple to understand once you know some functional programming.</p>
<p>There’s a slight error, the flowers all end with a string representing what flower it is. We can’t perform math on this, so we have to use this function</p>
<p>to any list we pass to the Euclidean Distance function.</p>
<p>Now we need to find the distance from every item in the training set to the item in the test set.</p>
<p>This code simply maps a lambda (anonymous) function onto every element in the training and test sets which finds the distance between the two elements and returns those distances in a list. The second part then zips together the element with the corresponding distance. So the first part returns</p>
<p>[3.4641016151377544, 0.0]</p>
<p>When given these parameters</p>
<p>trainSet = [[2, 2, 2, &lsquo;a&rsquo;], [4, 4, 4, &lsquo;b&rsquo;]]testInstance = [4, 4, 4]k = 1</p>
<p>and the second part returns this</p>
<p>[([2, 2, 2, ‘a’], 3.4641016151377544), ([4, 4, 4, ‘b’], 0.0)]</p>
<p>The last part to the code orders the code and returns K many neighbours, in this instance K = 1 so it just returns</p>
<p>[[4, 4, 4, &lsquo;b&rsquo;]]</p>
<p>Next we need to somehow vote on what K could be, now that we know its neighbours. We’ll create a voting system. Each neighbor will vote on its attribute (IE, what it is) and the majority vote will be taken as the prediction. So if the majority of its neighbours are of class “a” then the prediction will be of class “a”.</p>
<p>So this just goes through all the nearest neighbours and keeps a dictionary. The dictionary is just the flowername + how many times it appears. Each flower votes for their type into the dictionary, the dictionary is sorted so the most amount of vote appears in the first item in the dictionary and then the predicted flower is returned.</p>
<p>And that’s it. We’ve put together a K-Nearest Neighbours classifier. If you want to read more about implementing this algorithm I suggest reading <a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/">this</a> or <a href="https://www.dataquest.io/blog/k-nearest-neighbors-in-python/">this</a>.</p>
<h3 id="k-nearest-neighbour-vs-naieve-bayes">K-Nearest Neighbour vs Naieve Bayes<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbour-vs-naieve-bayes">#</a></h3>
<p>It’s entirely down to you when to implement KNN or Naieve Bayes. There are some advantages and disadvantages to both, so I’ll list them here:</p>
<p><strong>K-Nearest Neighbour Advantages</strong></p>
<ul>
<li>Simple but effective</li>
<li>Only a single parameter, K, easily leanred by cross validation</li>
</ul>
<p><strong>K-Nearest Neighbour Disadvantages</strong></p>
<ul>
<li>What does nearest mean? Need to define a distance measure</li>
<li>Computational cost — Must store and search through the entire trainign set at test time.</li>
</ul>
<p>Note: The <a href="http://cs229.stanford.edu/proj2006/HongTsamis-KNNForNetflix.pdf">Netflix progress prize winner</a> was essentially K-Nearest Neighbour.</p>
<p><strong>Naieve Bayes Advantages</strong></p>
<ul>
<li>Very simple to implement</li>
<li>Needs less training data</li>
<li>Can generalise the data</li>
</ul>
<p><strong>Naieve Bayes Disadvantages</strong></p>
<ul>
<li>Accuracy increases with the more data it has, so to get a 95% or higher accuracy you’ll need alot of data</li>
<li><a href="https://stats.stackexchange.com/questions/183056/what-does-it-mean-disadvantage-of-naive-bayes-classifier-strong-feature-indepe">Strong Independence Assumptions</a></li>
</ul>
<p>You should now have a firm foundation in machine learning. Instead of teaching you an algorithm that will very likely be replaced in 3 years, I decided to teach the fundamental mathematicals and theroy that define these algorithms and then show 2 of the most widely used algorithms that will likely be around for a while.</p>
<pre><code>/*(
</code></pre>
<p>Produced by Brandon Skerritt
<a href="https://skerritt.tech">https://skerritt.tech</a>
Instagram: @brandon.codes
Email: <a href="mailto:brandon@skerritt.tech">brandon@skerritt.tech</a></p>
<p>Remove stop words
Create frequency table of words - how many times each word appears in the text
Assign TF score to each sentence depending on the words it contains and the frequency table
Assign IDF Score to each sentence, same as above
Build summary by adding every sentence above a certain score threshold
Only chooses top 3 highest scoring sentences</p>
<p>Requirements:
JQuery
*/</p>
<p>function prettify(document){
// Turns an array of words into lowercase and removes stopwords
const stopwords = [&ldquo;a&rdquo;, &ldquo;&quot;, &ldquo;share&rdquo;, &ldquo;linkthese&rdquo;, &ldquo;about&rdquo;, &ldquo;above&rdquo;, &ldquo;after&rdquo;, &ldquo;again&rdquo;, &ldquo;against&rdquo;, &ldquo;all&rdquo;, &ldquo;am&rdquo;, &ldquo;an&rdquo;, &ldquo;and&rdquo;, &ldquo;any&rdquo;,&ldquo;are&rdquo;,&ldquo;aren&rsquo;t&rdquo;,&ldquo;as&rdquo;,&ldquo;at&rdquo;,&ldquo;be&rdquo;,&ldquo;because&rdquo;,&ldquo;been&rdquo;,&ldquo;before&rdquo;,&ldquo;being&rdquo;,&ldquo;below&rdquo;,&ldquo;between&rdquo;,&ldquo;both&rdquo;,&ldquo;but&rdquo;,&ldquo;by&rdquo;,&ldquo;can&rsquo;t&rdquo;,&ldquo;cannot&rdquo;,&ldquo;could&rdquo;,&ldquo;couldn&rsquo;t&rdquo;,&ldquo;did&rdquo;,&ldquo;didn&rsquo;t&rdquo;,&ldquo;do&rdquo;,&ldquo;does&rdquo;,&ldquo;doesn&rsquo;t&rdquo;,&ldquo;doing&rdquo;,&ldquo;don&rsquo;t&rdquo;,&ldquo;down&rdquo;,&ldquo;during&rdquo;,&ldquo;each&rdquo;,&ldquo;few&rdquo;,&ldquo;for&rdquo;,&ldquo;from&rdquo;,&ldquo;further&rdquo;,&ldquo;had&rdquo;,&ldquo;hadn&rsquo;t&rdquo;,&ldquo;has&rdquo;,&ldquo;hasn&rsquo;t&rdquo;,&ldquo;have&rdquo;,&ldquo;haven&rsquo;t&rdquo;,&ldquo;having&rdquo;,&ldquo;he&rdquo;,&ldquo;he&rsquo;d&rdquo;,&ldquo;he&rsquo;ll&rdquo;,&ldquo;he&rsquo;s&rdquo;,&ldquo;her&rdquo;,&ldquo;here&rdquo;,&ldquo;here&rsquo;s&rdquo;,&ldquo;hers&rdquo;,&ldquo;herself&rdquo;,&ldquo;him&rdquo;,&ldquo;himself&rdquo;,&ldquo;his&rdquo;,&ldquo;how&rdquo;,&ldquo;how&rsquo;s&rdquo;,&ldquo;i&rdquo;,&ldquo;i&rsquo;d&rdquo;,&ldquo;i&rsquo;ll&rdquo;,&ldquo;i&rsquo;m&rdquo;,&ldquo;i&rsquo;ve&rdquo;,&ldquo;if&rdquo;,&ldquo;in&rdquo;,&ldquo;into&rdquo;,&ldquo;is&rdquo;,&ldquo;isn&rsquo;t&rdquo;,&ldquo;it&rdquo;,&ldquo;it&rsquo;s&rdquo;,&ldquo;its&rdquo;,&ldquo;itself&rdquo;,&ldquo;let&rsquo;s&rdquo;,&ldquo;me&rdquo;,&ldquo;more&rdquo;,&ldquo;most&rdquo;,&ldquo;mustn&rsquo;t&rdquo;,&ldquo;my&rdquo;,&ldquo;myself&rdquo;,&ldquo;no&rdquo;,&ldquo;nor&rdquo;,&ldquo;not&rdquo;,&ldquo;of&rdquo;,&ldquo;off&rdquo;,&ldquo;on&rdquo;,&ldquo;once&rdquo;,&ldquo;only&rdquo;,&ldquo;or&rdquo;,&ldquo;other&rdquo;,&ldquo;ought&rdquo;,&ldquo;our&rdquo;,&ldquo;ours&rdquo;,&ldquo;ourselves&rdquo;,&ldquo;out&rdquo;,&ldquo;over&rdquo;,&ldquo;own&rdquo;,&ldquo;same&rdquo;,&ldquo;shan&rsquo;t&rdquo;,&ldquo;she&rdquo;,&ldquo;she&rsquo;d&rdquo;,&ldquo;she&rsquo;ll&rdquo;,&ldquo;she&rsquo;s&rdquo;,&ldquo;should&rdquo;,&ldquo;shouldn&rsquo;t&rdquo;,&ldquo;so&rdquo;,&ldquo;some&rdquo;,&ldquo;such&rdquo;,&ldquo;than&rdquo;,&ldquo;that&rdquo;,&ldquo;that&rsquo;s&rdquo;,&ldquo;the&rdquo;,&ldquo;their&rdquo;,&ldquo;theirs&rdquo;,&ldquo;them&rdquo;,&ldquo;themselves&rdquo;,&ldquo;then&rdquo;,&ldquo;there&rdquo;,&ldquo;there&rsquo;s&rdquo;,&ldquo;these&rdquo;,&ldquo;they&rdquo;,&ldquo;they&rsquo;d&rdquo;,&ldquo;they&rsquo;ll&rdquo;,&ldquo;they&rsquo;re&rdquo;,&ldquo;they&rsquo;ve&rdquo;,&ldquo;this&rdquo;,&ldquo;those&rdquo;,&ldquo;through&rdquo;,&ldquo;to&rdquo;,&ldquo;too&rdquo;,&ldquo;under&rdquo;,&ldquo;until&rdquo;,&ldquo;up&rdquo;,&ldquo;very&rdquo;,&ldquo;was&rdquo;,&ldquo;wasn&rsquo;t&rdquo;,&ldquo;we&rdquo;,&ldquo;we&rsquo;d&rdquo;,&ldquo;we&rsquo;ll&rdquo;,&ldquo;we&rsquo;re&rdquo;,&ldquo;we&rsquo;ve&rdquo;,&ldquo;were&rdquo;,&ldquo;weren&rsquo;t&rdquo;,&ldquo;what&rdquo;,&ldquo;what&rsquo;s&rdquo;,&ldquo;when&rdquo;,&ldquo;when&rsquo;s&rdquo;,&ldquo;where&rdquo;,&ldquo;where&rsquo;s&rdquo;,&ldquo;which&rdquo;,&ldquo;while&rdquo;,&ldquo;who&rdquo;,&ldquo;who&rsquo;s&rdquo;,&ldquo;whom&rdquo;,&ldquo;why&rdquo;,&ldquo;why&rsquo;s&rdquo;,&ldquo;with&rdquo;,&ldquo;won&rsquo;t&rdquo;,&ldquo;would&rdquo;,&ldquo;wouldn&rsquo;t&rdquo;,&ldquo;you&rdquo;,&ldquo;you&rsquo;d&rdquo;,&ldquo;you&rsquo;ll&rdquo;,&ldquo;you&rsquo;re&rdquo;,&ldquo;you&rsquo;ve&rdquo;,&ldquo;your&rdquo;,&ldquo;yours&rdquo;,&ldquo;yourself&rdquo;,&ldquo;yourselves&rdquo;, &ldquo;this&rdquo;];
// turn document into lowercase words, remove all stopwords
var document = document.replace(/[.,]/g, &lsquo;');
let document_in_lowercase = document.split(&rdquo; &ldquo;).map(function(x){ return x.toLowerCase() });
return document_in_lowercase.filter( x =&gt; !stopwords.includes(x) );
}</p>
<p>function countWords(words){
// returns a dictionary of {WORD: COUNT} where count is
// how many times that word appears in &ldquo;words&rdquo;.
const unique_words = uniqueWords(words);
let dict = {};
// for every single unique word
for (let i = 0; i &lt;= unique_words.length - 1; i++){
dict[unique_words[i]] = 0
// see how many times this unique word appears in all words
for (let x = 0; x &lt;= words_without_stopwords.length -1; x++){
if (unique_words[i] == words[x]){
dict[unique_words[i]] = dict[unique_words[i]] + 1;
}
}
}
return dict;
}</p>
<p>function uniqueWords(words){
const unique_words_set = new Set(words);
return unique_words = Array.from(unique_words_set);
}</p>
<p>function termFrequency(document){
// calculates term frequency of each sentence
words_without_stopwords = prettify(document);</p>
<pre><code>// gets rid of trailing spaces
const sentences = document.split(&quot;.&quot;).map(item =&gt; item.trim());
sentences[0] = sentences[0].substring(146);

const TFVals = countWords(words_without_stopwords)
const unique_words = uniqueWords(words_without_stopwords);

// actually makes it TF values according to formula
for (const [key, value] of Object.entries(TFVals)){
    TFVals[key] = TFVals[key] / words_without_stopwords.length;
}

// splits it up into sentences now
var TFSentences = {};
// for every sentence
for (let i = 0; i &lt;= sentences.length - 1; i ++){
    // for every word in that sentence
    let sentence_split_words = sentences[i].split(&quot; &quot;);
    // get the assiocated TF values of each word
    // temp.add is the &quot;TF&quot; value of a sentence, we need to divide it at the end
    let temp_add = 0.0;
    let words_no_stop_words_length = prettify(sentences[i]).length;
    for (let x = 0; x &lt;= sentence_split_words.length - 1; x++){
        // get the assiocated TF value and add it to temp_add
        if (sentence_split_words[x].toLowerCase() in TFVals){
            // adds all the TF values up
            temp_add = temp_add + TFVals[sentence_split_words[x].toLowerCase()];
        }
        else{
            // nothing, since it's a stop word.
        }
    }
    // TF sentences divide by X number of items on top
    TFSentences[sentences[i]] = temp_add / words_no_stop_words_length;
}

return TFSentences;
</code></pre>
<p>}</p>
<p>// each document is a sentence
function inverseDocumentFrequency(document){
// calculates the inverse document frequency of every sentence
const words_without_stopwords = prettify(document);
const unique_words_set = uniqueWords(words_without_stopwords);</p>
<pre><code>const sentences = document.split(&quot;.&quot;).map(item =&gt; item.trim());
sentences[0] = sentences[0].substring(146);

const lengthOfDocuments = sentences.length;
// prettifys each sentence so it doesn't have stopwords

const wordCountAll = countWords(words_without_stopwords);

// counts words of each sentence
// as each sentence is a document
wordCountSentences = [];
for (let i = 0; i &lt;= lengthOfDocuments - 1; i ++){
    wordCountSentences.push(countWords(prettify(sentences[i])));
}

// calculate TF values of all documents
let IDFVals = {};

// how many times that word appears in all sentences (documents)
wordCountSentencesLength = wordCountSentences.length;
// for every unique word
for (let i = 0; i &lt;= unique_words_set.length - 1; i++){
    let temp_add = 0;
    // count how many times unique word appears in all sentences
    for (let x = 0; x &lt;= wordCountSentencesLength - 1; x++){
        if (unique_words_set[i] in wordCountSentences[x]){
            temp_add =+ 1;
        }
    }
    IDFVals[unique_words_set[i]] = Math.log10(wordCountAll[unique_words_set[i]] / temp_add);
}

let IDFSentences = {};
// for every sentence
for (let i = 0; i &lt;= lengthOfDocuments - 1; i ++){
    // for every word in that sentence
    let sentence_split_words = sentences[i].split(&quot; &quot;);
    // get the assiocated IDF values of each word
    // temp.add is the &quot;IDF&quot; value of a sentence, we need to divide it at the end
    let temp_add = 0.0;
    let words_no_stop_words_length = prettify(sentences[i]).length;
    for (let x = 0; x &lt;= sentence_split_words.length - 1; x++){
        // if the word is not a stopword, get the assiocated IDF value and add it to temp_add
        if (sentence_split_words[x].toLowerCase() in IDFVals){
            // adds all the IDF values up
            temp_add = temp_add + IDFVals[sentence_split_words[x].toLowerCase()];
        }
        else{
            // nothing, since it's a stop word.
        }
    }
    // term frequency is always between 0 and 1
    IDFSentences[sentences[i]] = temp_add / words_no_stop_words_length;
}
return IDFSentences;
</code></pre>
<p>}</p>
<p>function longerSentenceWeighting(sentence){
// longer sentences are weighted better
// has to be how many words
// number was randomly chosen
sentence = sentence.toString()
return (length(sentence.split(&rdquo; &ldquo;)) * 1.5);
}</p>
<p>function isNumber(n) {
return !isNaN(parseFloat(n)) &amp;&amp; isFinite(n);
}</p>
<p>function numberWeighting(sentence){
// negative weighting on setnences with just numbers in them
sentence = sentence.toString()
temp = sentence.split(&rdquo; &ldquo;);
weighting = 1.0;
for (var item in temp){
if (isNumber(item)){
// more numbers in a sentence, the harsher the weighting
// but still gets the option of having one or two numbers
// number was randomly chosen
weighting = weighting * 0.8
}
}
return(temp * weighting)
}</p>
<p>function TFIDF(documents){
// calculates TF*IDF
const TFVals = termFrequency(documents);
const IDFVals = inverseDocumentFrequency(documents);</p>
<pre><code>let TFidfDict = {};

for (const [key, value] of Object.entries(TFVals)){
    if (key in IDFVals){
        TFidfDict[key] = TFVals[key] * IDFVals[key] * longerSentenceWeighting(value) * numberWeighting(value);
    }
}

let max = 0.0;
let max2 = 0.0;
let max3 = 0.0;

let max_sentence = &quot;&quot;;
let max2Sent = &quot;&quot;;
let max3Sent = &quot;&quot;;

// finds the top 3 sentences in TFidfDict
for (const [key, value] of Object.entries(TFidfDict)){
    if (TFidfDict[key] &gt; max){
        max = TFidfDict[key];
        max_sentence = key;
    }
    else if (TFidfDict[key] &gt; max2 &amp;&amp; TFidfDict[key] &lt; max){
        max2 = TFidfDict[key];
        max2Sent = key;
    }
    // do i need the third &amp;&amp; here?
    else if (TFidfDict[key] &gt; max3 &amp;&amp; TFidfDict[key] &lt; max2 &amp;&amp; TFidfDict[key] &lt; max){
        max3 = TFidfDict[key];
        max3Sent = key;
    }
}
return (&quot;&lt;br&gt;&quot; + &quot;•&quot; + max_sentence + &quot;&lt;br&gt;&lt;br&gt;&quot; + &quot;•&quot; + max2Sent + &quot;&lt;br&gt;&lt;br&gt;&quot; + &quot;•&quot; + max3Sent);
</code></pre>
<p>}</p>
<p>// get all text from .story-body within p tags on a BBC news web article</p>
<p>// console.log(termFrequency(&ldquo;Hello, my name is Brandon. Brandon Brandon. The elephant jumps over the moon&rdquo;));</p>
<p>// get all text from .story-body within p tags on a BBC news web article
let $article = $('.post-full-content&rsquo;).find(&lsquo;p&rsquo;).text();
console.log($article)
// insert text into body of document
let $insert = TFIDF($article);
// let insert = $('.story-body&rsquo;).prepend(TFIDF($article));</p>
<h3 id="feel-free-to-connect-with-me">Feel free to connect with me:<a hidden class="anchor" aria-hidden="true" href="#feel-free-to-connect-with-me">#</a></h3>
<p><a href="https://www.linkedin.com/in/brandonls/">LinkedIn</a> | <a href="https://github.com/brandonskerritt/">GitHub</a></p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://polymath.cloud/tags/artificial-intelligence/">Artificial Intelligence</a></li>
    </ul>






<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=Absolute%20Fundamentals%20of%20Machine%20Learning&amp;url=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f&amp;hashtags=ArtificialIntelligence">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f&amp;title=Absolute%20Fundamentals%20of%20Machine%20Learning&amp;summary=Absolute%20Fundamentals%20of%20Machine%20Learning&amp;source=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f&title=Absolute%20Fundamentals%20of%20Machine%20Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on whatsapp"
        href="https://api.whatsapp.com/send?text=Absolute%20Fundamentals%20of%20Machine%20Learning%20-%20https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Absolute Fundamentals of Machine Learning on telegram"
        href="https://telegram.me/share/url?text=Absolute%20Fundamentals%20of%20Machine%20Learning&amp;url=https%3a%2f%2fpolymath.cloud%2fml-fundamentals%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2020 <a href="https://polymath.cloud">Polymath.cloud</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>



<script defer src="https://polymath.cloud/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
