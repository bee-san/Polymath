<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots | Polymath.cloud</title>

<meta name="keywords" content="Artificial Intelligence" />
<meta name="description" content="Reinforcement Learning">
<meta name="author" content="Bee">
<link rel="canonical" href="https://polymath.cloud/dissertation/" />
<link href="https://polymath.cloud/assets/css/stylesheet.min.94a69f3d0b70cac76c6d6f7dfecc9f91f2319ec73d54be960b0d3624fa5a25e2.css" integrity="sha256-lKafPQtwysdsbW99/syfkfIxnsc9VL6WCw02JPpaJeI=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://polymath.cloud/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://polymath.cloud/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://polymath.cloud/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://polymath.cloud/apple-touch-icon.png">
<link rel="mask-icon" href="https://polymath.cloud/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />




<style>
    td, th {
    border: thin solid #999 !important;
    padding: 12px 15px;
}

thead tr {
    background-color: #009879;
    color: #ffffff;
    text-align: left;
}

table {
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    font-family: sans-serif;
    min-width: 400px;
    overflow: auto;
    display: table;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
}

tbody tr {
    border-bottom: thin solid #dddddd;
}


tbody tr:last-of-type {
    border-bottom: 2px solid #009879;
}

tbody td.active-item {
    font-weight: bold;
    color: #009879;
}

tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
    }


body.dark tbody tr:nth-of-type(even) {
    background-color: #383838;
}


img {
    display: block;
    margin: auto;
    text-align: center;
    transition: transform .5s;

	border-left: 1px solid #ccc;
	border-top: 1px solid #ccc;
	border-right: 1px solid #888;
	border-bottom: 1px solid #888;
	background-color: #fcfcfc;
	padding: 4px;
}


img:hover
{
    transition-delay:0.22s;
    -ms-transform: scale(2, 2);
    -webkit-transform: scale(2, 2);
    transform: scale(2, 2);
}

figure {
    text-align: center;
    line-height: 0.1em;
    padding: 1em;
    margin: 5px;  
    overflow-wrap: break-word;
}

figure img {
  vertical-align: top;
}

figure figcaption{
    text-align: center;
    overflow-wrap: break-word;
    word-wrap: break-word;
}



</style>

<meta property="og:title" content="Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots" />
<meta property="og:description" content="Reinforcement Learning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://polymath.cloud/dissertation/" />
<meta property="article:published_time" content="2019-11-29T23:46:37+00:00" />
<meta property="article:modified_time" content="2019-11-29T23:46:37+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots"/>
<meta name="twitter:description" content="Reinforcement Learning"/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots",
  "name": "Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots",
  "description": "Abstract Reinforcement learning has traditionally been a dull topic to learn, especially for students who may not understand programming. With this dissertation, it is shown that …",
  "keywords": [
    "Artificial Intelligence"
  ],
  "articleBody": "Abstract Reinforcement learning has traditionally been a dull topic to learn, especially for students who may not understand programming. With this dissertation, it is shown that reinforcement learning can be joyous to learn, and easier than some might have suspected.\nThis dissertation shows a framework for teaching reinforcement learning to key stage 3 school students. Using Python to implement reinforcement learning, and using lessons based around the code.\nThe aim was that by teaching them a \"hard\" and \"cool\" computer science topics, the students would become engaged in the world of computer science.\nThis project implements a Q-learning reinforcement agent in a grid-world 2-dimensional maze to teach the students reinforcement learning. It also has lessons and exercises for the students, that tie in with the Q-learning agent to reinforce what the students can learn.\nIntroduction If students or teachers find a subject difficult, the chance that that subject is picked (for GCSE or A Levels) goes down substantially [@Ofqual]. Difficulty can come in many forms, whether it is:\n  Bad teaching.\n  Under specialisation of teachers.\n  However, the question will remain. How do we make reinforcement learning interesting? According to Zahorik [@doi:10.1086/461844] some methods include:\n  Novelty.\n  Emotional Appeal.\n  Group Work.\n  Computers.\n  \"Lived experience\" (practical knowledge).\n  Aims and Objectives This project created an engaging set of exercises involving (simulated) robots and reinforcement learning for key stage 3 students to learn from.\nThe objectives are:\n  Create an ASCII based grid-world reinforcement learning playground.\n  Implement a reinforcement learning framework in an existing Python based robot simulator.\n  Design an API for reinforcement learning.\n  Devise a set of challenge problems to be used with the simulator.\n  Design the learning exercises.\n  Gather feedback about the learning exercises and improve upon it.\n  Create an autonomously driving robot vehicle agent.\n  Key Literature and Background Reading Reinforcement Learning: An Introduction This book by Sutton \u0026 Barto [@sutton] was one of the most popular introductions to reinforcement learning. The book starts simple. Introduction, why reinforcement learning is needed, and early history of it.\nThe book then introduced us to reinforcement learning using the K-armed bandit. This is not a challenge in optimising the algorithm, it’s simply a problem to introduce us to the terminology of reinforcement learning and how we can conclude that this problem can be solved with reinforcement learning.\nThe book then took us through some paradigms we may know that can be considered reinforcement learning paradigms. Such as:\n  Dynamic Programming.\n  Finite Markov Decision Processes.\n  Monte Carlo Methods.\n  Before taking us into tabular (Q-Learning) based reinforcement learning.\nThe book ends with the psychology of reinforcement learning. We know the theory, and we know how to implement it, but how does this work with humans? According to Sutton \u0026 Barto [@Sutton]\n \"Reinforcement learning is the closest to the kind of learning that humans\".\n If reinforcement learning was closet to that of humans, Simon \u0026 Barto shows us how humans learn using psychology and neuroscience.\nReinforcement learning is a type of learning that involves repeatedly (and often, failingly) doing things until the agent learns what moves return the highest (or lowest) rewards.\n{#fig:agentReward}\nThe agent takes an action. This action has some effect on the environment. If the agent can only see 30cm in front of it and it moves 5cm forward, the environment doesn’t change but the agent’s perception of the environment changes.\nThe agent might destroy, create, or move things in the environment.\nThis action returns an observation and a reward. The observation is what has happened to the agent and the environment. For example, if the agent moved 5cm forward, the agent can now see 5cm more than it previously could.\nThe reward is either a positive reward or a negative reward. As an example, the agent’s goal might be to search for rocks. If the agent sees a rock within 30cm of it now, the agent is rewarded.\nThe agent would then store this information into a Q-table. The Q-table answers the question: \"When the environment is exactly this, and there are these moves available, this is the reward you will get\".\nHowever, there are 2 types of rewards. Seen above is the positive reward. The agent is positively reinforced by something. But a negative reward is also possible.\nAs an example of a negative reward, the agent could be in an environment with one straight line of light. The agent’s purpose is to avoid the light. If the agent moves into the light, the agent gets a negative reward.\nIt is unusual for a reward to be both positive and negative. It is either -1 and 0 or 0 and 1.\nThe agent uses this Q-learning table to make further decisions. However, we stumble upon the exploration vs. exploitation problem.\nExploitation here means that the agent exploits the Q-learning table to maximise its reward.\nLet’s say our agent learns to avoid the light by staying on the edge between light and darkness. Our agent realises if it goes into the light, it gains a negative reward. But if it stays between the two, it doesn’t gain a negative reward - but it also doesn’t gain anything positive.\nThe agent, since it exploits the Q-learning table, never learns that it can stay out of the light completely and get a much higher score than where it is.\nExploration, regardless of how much reward an action has, is important in this regard as it informs the learning of the agent - it maximises the agents learning.\nAlthough too much exploration and exploitation will no longer offset the value of exploration.\nThis can be solved by having the reward as a Boolean data type.\nDeep Learning Deep Learning by Goodfellow et all [@Goodfellow] is an introductory book to deep learning. While Reinforcement Learning by Sutton \u0026 Barto is an introduction to reinforcement learning, it assumes prerequisite knowledge of deep learning. More specifically, the mathematics involved in deep learning.\nGoodfellow et all start the book off with an introduction to applied mathematics.\n  Linear Algebra.\n  Probability and Information Theory.\n  Numerical Computation.\n  Machine Learning Basics.\n  Once the reader is familiar with the mathematics behind deep learning, Goodfellow et all introduce neural networks. Starting slowly with feed-forward networks and then eventually ends with convolutional networks and recurrent neural networks.\nThe final chapter is on the research, the more academically rigorous side of this book.\nThe book proved to be useful in providing the creator knowledge of the mathematics behind artificial intelligence.\nMIT Deep Learning While this project centred around reinforcement learning, most fundamentally, it was about creating a framework to teach reinforcement learning.\nFor the creator to understand the most effective methods to teach reinforcement learning, they have chosen to study MIT’s deep learning course [@MIT].\nThe reason why MIT was chosen was that they publish every lecture online for free. They are also an Ivy Leauge college, which means they pride themselves on their world-changing research.\nSome of the more important things the author learnt whilst watching the lectures were:\nTalk Slowly All of the professors in the lecture videos talked slowly, but not too slowly. They talked clearly, loudly, so everyone can hear.\nDifferent Forms of Media The professors used video, whiteboards, quizzes, live-programming and more. Anything that isn’t text on a plain background is appeared to be fair game for them to use.\nOne of the things the author wanted to involve in their teachings is talking over videos. The professor put on a video of a car learning to drive [@lex], and explains how it is learning. We were engaged in the video, and we learnt how it works.\nUsing Colour to Learn All of the mathematical formulae used by MIT were colour coded.\n{#fig:mitformula width=”\\textwidth” height=”\\textheight”}\nIn this lecture by Professor Amini [@Amini] the colours have meaning. The purple underline for the second half of the formula corresponds to the purple final grades matrix on the right.\nThe quotation under each part of the formula stating what that part does (\"Actual\", \"Predicted\"). The use of the blue box at the bottom of the screen showing what the code is for that formula.\nCOMP335 The creator took the module COMP335 - \"Communicating Computer Science\". [@COMP335]. This module aimed to enable the students to better teach computer science to key stage 3 students.\nThe creator specifically learnt these key points from the module:\nStudents Are Slow The idea of learning rates, which may seem simple to a university student, is not simple for younger students.\nEven the idea of a machine that is capable of learning like a human can be confusing to younger students.\nDr. Thomason has shown that even formulae, as simple as formulae may be, can still be confusing to a younger audience whose only exploration into formulae is Pythagoras’s Theorem.\nDo Not Forget Things We Take for Granted As a university student, the creator took for granted the idea that all schools will have Python installed. That they will have the resources to download packages, or install Python if is it not installed.\nThis is wasn’t the case. Some schools have restrictive downloading, some teachers may not even know how to install packages.\nDiscrete Mathematics with Applications This book by Epp [@epp] is typically a first-year university students introduction to discrete mathematics. The book served as a good pointer to not only explaining mathematics well but explaining how to teach well.\nLike MIT [@MIT], the book made heavy use of any form that isn’t plain text. Images, quotes, history tidbits, miniature tests, tables, and many, many more.\nThe book uses colour and syntax-highlighted code to get across how each formula works.\nDevelopment Process and Method The Agile Programming Metholdogy was used, more specifically SCRUM [@schwaber2002agile]. SCRUM allowed the creator to design the end program with the user in mind, using the user stories feature. Since it is agile, the creator had a working prototype early on in the development process which can acted as an ultimate fall back. See the risks \u0026 contingencies chapter for more information on this.\nThe project has to be easy to use for students. It would be unwise to build software using waterfall, because at the end the stduents may not understand how the software works and the project will be a failure. Agile methodology allows us to work towards good code that is easily understandable.\nThis is because if the project had failed entirely, the Waterfall method would not have provided any working prototypes to use to teach.\nThe tools the creator used were:\nGitHub GitHub was used as a version control system, making backups of the code and making it possible to revert to a previous version.\nThe GitHub Project board was used as a Kanban board to monitor the progress of the project. With 4 sections. ToDo, Testing, Implemented, and finally Issues.\nThe creator also made heavy use of GitHub actions, the continuous integration part of GitHub to automatically run tests and verify working code.\nPython The code s written entirely in Python 3.8, making use of small libraries to allow for ease of use for a school.\nNumPy The project required NumPy, which is used to perform calculations that are required in basic reinforcement learning such as the Q learning algorithm and maintaining the Q learning table.\nLocal backups Local backups of the code were regularly made on the creator’s machine and network-attached storage device.\nIDLE \u0026 Code Editor Some schools were not able to use a code editor and instead used IDLE - Python’s integrated development environment. The code was regularly tested against both an editor and IDLE.\nThe creator made a note that the biggest issue was that what worked on the development PC wouldn’t always work on the school PCs. They may not have the required packages, or they may not even have Python installed at all.\nThe software was developed on a separate virtual machine made specifically for the development of this project. On this machine, libraries such as NumPy were automatically installed. Each new install was vetted with the creator.\nIf the worst came to the worst during the demonstration of this project, the creator would be able to export the virtual machine and install it on the computers in the classrooms. Dr. Thomason had said that this was done before with Kali Linux but is not the preferred way to do things. However, it was still possible.\nData Sources No data sources were used for this project. However, potential data sources might be used by the students.\nThe maze program set out for the students can be altered, to such a degree that students may search online for mazes that will work with the project. They may download these alternative data sources and attempt to incorporate those into the program.\nHowever, no data sources were used by the creator of the project.\nEthical Considerations Here is a list of the ethical issues discussed between the creator and their supervisor.\nCombining 2 Modules Into 1 The creator combined 2 modules into 1. COMP335 Communicating Computer Science with COMP39x, this project. There were some ethical issues raised due to using the same work for 2 modules.\nAfter the creator discussed this with their supervisor, due to COMP335’s deadline of \"finish a lesson plan by week 12\" this would mean that the project would have to have a working prototype by week 12.\nBecause of this limitation by combining 2 modules the creator did more work than if I was to do each module separately, which made it ethically \u0026 morally okay.\nHow to Evaluate the Effectiveness of This Project The simplest way to evaluate the effectiveness of this project was by asking the students (who are children) what they think of it. However, this brings up a plethora of ethical issues with collecting data from children.\nThe easiest way to evaluate this project wasn’t through the children or the teachers. During the teaching sessions for COMP335 Dr. Thomason will evaluate the teaching. By using this dissertation project in the teaching, Dr. Thomason will partly mark the project (the activity) as well as the teaching.\nThroughout the year, the widening participation team at the department hosted events. Dr. Dennis, my supervisor, suggested it might be possible for the creator to bring the project to one of these.\nHowever, due to COVID-19 it wasn’t possible to teach, or attend any events. The evaluation chapter will go into more details.\nDesign The Grid World The Grid World was the environment in which most of the players experienced the exercises.\nThe grid world was a $\\alpha ;x; \\beta$ world. The grid world had walls, made of the number 0. The idea behind this was that the number 0 is physically wide, much wider than the number 1. Therefore, wideness equated to a wall and the number 1 equated to a corridor.\npython 0000000000000000 0111111111111110 0111111111111110 0000000000000000\nThe environment object created a new grid world (or use a previously generated one) for every run. An example of a simple grid world is:\npython 0000000000000000 X111111111111110 01111111111111Y0 0000000000000000\nWhere X is where the agent starts and Y is where the agent ends.\nThe creator then represented this world in a 2d array.\nIf the objective was to follow a light, the grid world might have looked like this:\npython 0000000000000000 X 0 —————- 0000000000000000\nWhere the line is the light. The agent will then \"explore\" the world. The agent might decide to move to the right (forwards).\npython 0000000000000000 X 0 —————- 0000000000000000\nThis did not return a reward, so the agent tries again. This time, it moves into the light.\npython 0000000000000000 0 X————- 0000000000000000\nThis returns a reward, so the agent knows it is doing the right thing.\nOf course, this is a very simple problem with only 2 lanes to choose from. A real-life problem might simulate the disparity of light. The light might not be a perfect line, it might have signs of light all over the place. Such as this:\npython 0000000000000000 ————0 X============= 0000000000000000\nHere we used a double line (equals symbols) to represent the strength of the light. The light in the second lane is weaker, so it only used 1 line.\nWith a grid world like this, and teaching this to key stage 3 pupils, it is important to wisely choose the symbolic meaning of each item in the grid world so the students can effectively learn and understand what is happening on their screen. Abstract symbols will only make for confused students.\nThe reasoning behind 0 is that it is physically wide, like a wall. Whereas 1 is physically thin, representing a corridor.\nComponents of the System There were 2 separate modules to the system. Agent.py, and Maze.py.\n{#fig:api}\nAgent.py controlled the agent and what the agent does, as well as the Q-table. Maze.py controlled the maze, and decided on what was a legal move for the agent to take.\nEach task focussed on one part of the system, so the students effecitvely learnt how they integrate togeter. If the task called for a change to the rewards system, the student would explore Agent.py. If the task called for a change to the legal moveset or the maze itself, the students entered into Maze.py.\nBy seperating 2 distinct parts of the system this way, students saw how the 2 parts work with eachother.\nData Structures and Algorithms Arrays The most important datastructure used was multi-dimensional arrays, used for the Q-table.\nTo explain this datatructure, look at the below example.\npython 0: [0.5, 421, -1, False)], 1: [(1.0, 228, -1, False)], 2: [(1.0, 348, -1, False)], 3: [(1.0, 328, -1, False)], 4: [(1.0, 328, -10, False)], 5: [(1.0, 328, -10, False)]\nWhere the dictionary has the structure:\npython action: [(probability, nextstate, reward, done)]\nThis is an example of a rewards table in a multi-dimensional array.\nQ-Learning Q-learning is a reinforcement learning algorithm. The goal is to learn something which tells the agent what action to take under what circumstances. It does this using the reward table, as seen in the previous section.\nIt is the simplest of all reinforcement learning algorithms, as understood by [@Sutton] in their book. This is useful because a simpler algorithm will be exponentially simpler to teach to key stage 3 students.\nUser Interface There is no direct user interface for this project. It is a code library. However, the design of the code library will matter.\nAll the important variables were represented near the top of the file, clearly marked for the students to understand. Each function had full documentation, as well as the thought process behind why that function exists.\nHowever. the lessons themselves were designed intentionally well. Therefore this document details the design of the lessons, as they are what the user interfaced with when they used the project.\nExercise 1 The first exercise was designed to be the easiest. Given this code:\npython learningrate = 0.5\nThe user would change the learning rate to observe what will happen to how the agent acts on the maze.\nMore specifically, the learning goal of this lesson was to learn what the learning rate was, and see for themselves how the learning rate affected how the agent behaved in the environment.\nExercises Each exercise does not require knowledge of the previous lessons. This meant that the students could skip ahead without worrying about missing some vital information.\nEach exercise contained a learning goal, to maximise the students learning. The exercises contained succinct knowledge, with clearly marked Python code and line number references to enable the students to easily find the exact variables and functions they needed to change or observe.\nThe agent runs with an epoch of 1, meaning it is easy for a student to observe the changes in the terminal as they come in.\nFor more information on exercises, look at the implementation chapter where every exercise created is detailed.\nUML Python had no public or private classes which contradicted the Unified Modelling Language.\n{#fig:UML}\nThe environment and agent interact with each other and using a simple AI API (which is the files themselves, as they are objects and can be interacted with like an API) the tasks can be completed. The preferred method was to directly edit the classes themselves, so the students could get a direct feel of how the system works. But the API still exists.\nSimplification The code contains simple variables which can be changed before run-time to enable users to learn through real-life reinforcement learning. These variables can be changed:\n  The maze itself. Which is defined as self.maze. By changing self.maze, it will change the maze the agent learns on.\n  The agent’s starting location.\n  The allowed legal moves of the agent. In the default example, the agent can move up, down, left or right.\n  The goal of the maze. What does the agent touch to know it has reached the end of the maze?\n  The rewards table itself. It is possible to manually input into the rewards table.\n  Gamma. The value of the future reward. How much the agent values future rewards vs how much the agent values current rewards.\n  Learning rate. The speed at which the agent learns.\n  Maxepochs. How many epochs will the agent perform?\n  Explore. In Reinforcement Learning, it is a balance between exploratory rewards and exploitionary rewards. Setting this to 50, for example, would mean half the time the agent explores and the other half the agent exploits, taking the highest possible reward move.\n  AgentReward - how much reward the agent starts with. Traditionally 0.\n  PenaltyMoving - how much of a penalty does the agent receive for moving? This prevents the agent from going around in circles.\n  In the code, these variables were very clearly explained and were at the top of the code. This makes it easier for younger students to find the variables to play around with. Here is a copy of that code.\npython # Where does the agent start self.start = (0, 0)\n# Where is the goal of the agent? # self.maze.getSizeOfMaze() sets # the agent goal to the very bottom right hand corner # Can use coordinates such as (15, 15) self.goal = self.maze.getSizeOfMaze()\n# The reward table the agent starts with. # You can manually insert rewards like: # self.rewardsTable = (1, 1): \"State\": (1, 1), # \"Action\": (0, 1), \"Reward\": 0.75, # (0, 0): \"State\": (0, 0), \"Action\": (0, 1), \"Reward\": 0.75 self.rewardsTable =\n# What actions can the agent take? self.possibleActions = \"Up\": (1, 0), \"Down\": (-1, 0), \"Left\": (0, -1), \"Right\": (0, 1)\n# How much the agent expects to gain from future value. self.gamma = 0.5\n# How fast can the agent learn? self.learningRate = 0.5\n# How many times will the agent complete # the maze before it stops? self.maxepochs = 1000\n# The probability that the agent will # explore on that round, # instead of exploiting the rewards table self.explore = 0.15\n# The agent’s current reward when it starts self.Agentreward = 0.00\n# The agents penalty for moving. Prevents # the agent from running around # in circles self.penaltyMoving = -0.05\nThe code was encapsulated in a class, so students could call the class in Python’s IDLE. As an example:\npython import Qagent agent = Qagent.agent()\nFrom here, it was possible to change the variables as simple as calling them in IDLE.\npython  agent.learningRate = 0.97\nAnd then calling the agent’s run() method.\nSince there are many epochs, and each epoch can have 10,000+ moves, it was unreasonable to assume the students can monitor the maze in real-time. It was also unreasonable for the students’ computers to be able to display that many frames a second of a maze changing.\nTherefore, it was proposed that students either perform 1 epoch to monitor how the maze changes over time and thus how the agent will go in the wrong direction before the rewards table is fully fleshed out, or to look at the Q table and the successful route of each epoch.\nIf given as a presentation, it was recommended that the teacher uses the former method. The students being able to see that the agent is, in fact, dumb, and repeatedly goes the wrong way, but only finds the correct route after brute-forcing all possible combinations is an invaluable lesson.\nThe latter, watching the successful route of the maze, didn’t look nice on a low-resolution monitor. It was understandable that most schools do not have a high-resolution monitor, and especially those with dyslexia may not be have been able to tell the difference between edges of the maze or the route used. Therefore, it was expected that the latter is used more often than the former.\nDesign of the documentation For more advance students, merely playing with the variables may not be enough to learn. The more advance students may have wished to play around with the code itself, to understand how to build their own reinforcement learning agent. For that reason, almost all lines of the codebase were commented. Extensive documentation was given, not only in the form of comments but in the form of docstrings for each function.\nThe thought process behind that function and how it behaves as it does was also included. So not only do the students learn how a function behaves as it does, but also how they can develop the thought process to create their own functions for their own reinforcement learning agents.\nAs an example, take the documentation for the reward() function at figure 8.\nThe end goal of having such extensive documentation was to enable students of all levels the ability to learn from the code. The students starting at a lower level would have been able to read the documentation and understand how the function works and what its rule was in reinforcement learning. The students starting from a higher level will be able to see how the creator’s thought process solved the problems solved by the functions, and how they can integrate this own thought process into their own code to better solve problems.\nImplementation The implementation of this project was difficult. Not only did the creator have to wrestle with reinforcement learning, but they had to create lessons for key stage 3 students to learn from.\nAnd the code written by the creator would have to be simple enough for a key stage 3 student to manipulate and understand, for them to learn.\nUltimately, this proved many problems for the creator. This section on the implementation will detail the multiple problems the creator had with the implementation, as well as how this project was implemented.\nPython 2 vs Python 3 When the creator first started out writing the code for this project, they had to decide on what version of Python to use. Python 2 would more likely be universally used in schools, as it was much older. But Python 3 was the newest version of Python, and the creator believed that Python 3 would be better to teach.\nHowever, it is not down to the creator of this project to decide on what Python versions schools use. The creator cannot simply show up and demand Python 3 when the school has been teaching with Python 2 for so long. It was unreasonable to expect the school to install a newer version of Python too. Since on Windows, this can create compatibility problems if the Python versions aren’t properly sandboxed from each other.\nHowever, in late 2019 the Python Software Foundation, the overseeing body of Python, announced that they will officially sunset Python 2 on the 1st of December 2020 [@pycon].\nThe creator took this into account and realised that most teachers will likely choose to upgrade to the latest version of Python 3, as that will be the only supported version from 2020 onwards.\nHowever, the creator knows that what a teacher should do and what they are doing are 2 different things. The teacher may not have the resources or the time to manage the switch over and to upgrade all of their own teaching material in time.\nFor this reason, the creator worked hard to ensure Python2 compatibility as well as Python3 compatibility.\nHowever, it also depended on what version of Python 2 the schools were using. The latest version Python 2.7 worked fine, but any earlier versions that are no longer supported may not have worked.\nFor this reason, there were 3 options the teacher could have taken to ensure the lesson could continue.\nFirstly, the teacher could have installed Python 3 on their own work laptop and used a projector to project the program, and give a lecture on each task of the exercises. But this required the teacher to have a work laptop that they can install software on, and even have a projector. Not all schools will have this privilege.\nThe 2nd method was to manually teach the principles of reinforcement learning without the use of code. Simply, a lecture was given on learning rates, gamma, and so on.\nFinally, the 3rd option was to open an online Python 3 sandbox program. These programs allow any user to create, run, and test Python 3 applications on the web. What’s more is that these online sandboxes can install third-party software, such as NumPy which was required for this project.\nThe final option was a lot harder than it may seem to implement. To use these online code sandboxes the web browser would have to have been the latest update, allowing use of WebGL and more.\nBlack Black [@Anaya] was a Python formatter. Run Black on Python code and it will automatically format it according to PEP8 [@KennethReitz2016], the Python Style Guide. It was created and owned by the Python Software Foundation, the organisation behind the language.\nBy running the code through Black, it was guaranteed to look similar to other Blackend code, or any code that strictly follows PEP8. Because of this enhancement, the code is easier to be read to someone that has seen Blackened code before or even to someone that hasn’t heard of PEP8.\nExternal Packages There were many ways to install the external package NumPy, all of them are easy to do and required little resources. The NumPy installation was 20mb, and there are 4 methods mentioned in the documentation.\n  pip install NumPy\n  python setup.py install\n  pip install -r NumPy\n  run install.py\n  Pip was in the standard library for Python, so every Python installation would have Pip installed. And using Pip does not require administrator rights or a superuser account.\nSetup.py will automatically install NumPy. Setup.py is recommended by the Python Software Foundation to be packaged with all public releases to allow anyone to run the code.\npython from setuptools import setup\nsetup( name='Reinforcement Learning Agents’, version=”, packages=[\"], url='https://github.com/Author/diss’, license=”, author='Author, authoremail=”, description=” installrequires=[‘NumPy’] )\nInstall.py is the final option, a Python file that calls the command \"pip install NumPy\". Below is the source code for install.py:\npython import sys sys.os(\"pip install NumPy\")\nOn every version of Python, Pip was automatically installed. It does not require admin or superuser rights to run Pip. The NumPy module is only 20mb large, which is tiny in comparison to other modules. The hardest part about installing NumPy will be running the pip install script, hence 4 options were given to the user.\nRewards One of the most daunting tasks of this project was the implementation of a rewards system. It was simple to imagine that the Q algorithm would take care of all the learning, and while it did provide a large relief for the programmer, it was not completely up to scratch with how the rewards system should work.\n$$\\underbrace{\\text{New}Q(s,a)}-{\\scriptstyle\\text{New Q-Value}}=Q(s,a)+\\mkern-34mu\\underset{\\text{New Q-Value}}{\\underset{\\Bigl|}{\\alpha}}\\mkern-30mu[\\underbrace{R(s,a)}-{\\scriptstyle\\text{Reward}}+\\mkern-30mu\\underset{\\text{Discount rate}}{\\underset{\\Biggl|}{\\gamma}}\\mkern-75mu\\overbrace{\\max Q’(s’,a’)}^{\\scriptstyle\\substack{\\text{Maximum predicted reward, given} \\ \\text{new state and all possible actions}}}\\mkern-45mu-Q(s,a)]$$\nMore specifically, the largest problem the creator encountered was deciding on what the reward should be.\nPositive or Negative Rewards The agent has 2 options for how their reward progresses. Take the maze example. The agent gains a positive reward for reaching the goal, or the agent incurs a negative reward every action until it reaches the goal.\nIn the former, the agent is encouraged to find the goal, but it does not take into account the amount of time it takes. The agent gets there in the end. In the latter, the agent is encouraged to move faster to reach the end goal.\nHowever, we must take into account how students, especially younger students, understood a rewards system. In the traditional world, students get rewarded for the \"good\" things they do, and punished for the \"bad\" things they do.\nThe creator of the program decided that attempting to teach a new rewards system to students may be too confusing. Especially trying to explain how incurring a punishment every second was still a rewards system, as the connotation of reward meant good rewards.\nFor this purpose, the agent’s rewards are positive (more than 0), but the agent can incur penalties depending on if the agent has done something the creator does not wish it to do.\nThe agent reaches a positive reward for reaching the end goal, but the agent incurs a penalty every time it moves. This penalty encourages 2 things of the agent:\n  No senseless wandering.\n  To hasten the time it takes to find the goal.\n  It was possible for the agent to become stuck in an infinite loop if it never received a punishment for senseless wandering. And the punishment encourages the agent to move faster to the goal.\nHowever, on some occasions the agent decided to take the punishment as is and do nothing about it. It was possible for the agent to become stuck in an infinite loop, continually receiving negative rewards. It was not enough to merely give the agent a punishment, the agent had to learn to avoid the punishment.\nFor this reason, the \"game over\" variable was invented. This variable stated that when the agent should declare game over, and stop learning. The variable is there so the agent cannot get stuck in an infinite loop. If it does, we start the games afresh and retry until the agent learns. The game over variable is based on the agents own personal reward, not the reward from each square in the grid world maze.\nThe exact numbers chosen for these variables were randomly selected by the designer as being appropriate. It is expected that students will fiddle with these numbers in the tasks created.\nFinally, the agent incurs a further penalty if the agent re-visits a square it has already been to. This penalty is the largest penalty of all, in the hopes that the agent learns that visiting a place it has already been to is severely bad because it should find the route in one go, rather than having to double back on itself.\nThe Maze The maze class controlled the operation of the maze. It controlled how the agent moved around the maze, and what the maze itself was.\nThe maze itself was defined in a 2-dimensional array. In this array, the number 0 represented a wall (because 0 is physically wide, and looks closet to a wall) and the number 1 represents openness (because 1 looks like a corridor, a thin strip of openness).\nThe analogies were to help the students understand what each bit of the maze represented.\npython [ 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 0, 0, 1, 0, 0, 1, 0, 1, 1, 1], [ 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [ 1, 1, 0, 1, 0, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [ 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\nThe start location was defined by agent.startLocation, and the goal was defined by agent.goal.\nMovement of the Agent The agent chose exploratory or exploitatory moves. There was a 15% chance it would have picked an exploratory (random) move, but this number was expected to be changed by the students.\nIf the agent chose an exploratory move, it would pick a random legal move.\nIf the agent chose an exploiatory move, it would calculate the legal moves and work out which one has the highest reward to choose from.\nThe function getLegalMoves() calculated the legal moves of the agent. The maze got the current location of the agent, performed a move on those coordinates (for example, to go up is to add (1, 0) to the vector of the agent’s current coordinates) and then it would calculate whether that move was legal or not.\nA legal move needs to meet 3 conditions:\n  The new coordinates were not negative, as there is no negative plane on the 2-dimensional grid world.\n  The new coordinates are not physically outside of the 2-dimensional grid world.\n  The move was possible to do.\n  The creator made it impossible for the agent to leave the 2-dimensional grid world, by visiting coordinates that did not exist in the planes. For example, starting at (0, 0) and travelling up by (1, 0) would result in coordinates that do not exist in the grid world.\nThis function is called before the agent tries to move to determine what the legal moves are. This function meant that the agent cannot make an illegal move.\npython legalMoves = for move in self.moves: newCoords = self.vectorAddition(self.agentLocation, moves[move]) # checks to see if there is an illegal move negativeCoords = True if any(y outsideMap = True if any(y  len(self.maze[0]) - 1 for y in newCoords) else False\nillegalMove = True if negativeCoords + outsideMap == True else False\nif not illegalMove and newCoords != 0: legalMoves[move] = moves[move]\nVector Addition One of the problems the designer had with agent movement was vector addition. The coordinate system is based on vectors (x, y) where x is the latitude and y is the longitude. To move around on this vector-based coordinate system the agent had to add or take away other vectors depending on the appropriate action. For example, to move physically up by 1 the agent had to add (1, 0) to its current vector represented coordinates.\nBut, in Python adding 2 vectors together resulted in a vector representation that was unsatisfactory for this project. $$(0, 0) + (1, 1) = (0, 0, 1, 1)$$ Whereas the creator wanted the vector addition to being element-wise, as seen below: $$(0, 0) + (1, 1) = (1, 1)$$ To accomplish this, the designer created the function in figure 14. This function called operator.app on the 2 tuples (vectors), and then mapped this function over both vectors, eventually turning it back into a tuple (from a list).\npython def vectorAddition(self, tup1, tup2): return tuple(map(operator.add, tup1, tup2))\nPrinting of Data When the program runs, it prints data to the terminal. For example running the program for the first time and letting it run for a loops results in this:\n \"The steps taken so far are [(0, 0), (0, 0), (1, 0), (2, 0), (1, 0), (0, 0)]\"\n Here we can see the agent is repeatedly looping back on itself. As the agent learns, the steps it takes changes.\nThe agent also prints out the rewards table.\n \"(0, 0): ‘Action’: 0, ‘Reward’: -0.075, ‘State’: (0, 0), (1, 0): ‘Action’: 0, ‘Reward’: -0.075, ‘State’: (1, 0), (2, 0): ‘Action’: 0, ‘Reward’: -0.075, ‘State’: (2, 0)\"\n As the agent runs, the rewards table and taken steps change. Students have been able to follow the agent as it learns.\nIn the exercises and by default, the epochs of the agent are set very low so the students could observe the agent changing in finer detail. The program also, by default, sleeps for 1 second between every action. This enabled the students to read the print statements and figure out where the agent is going.\nGlobe Maze vs Flat Maze In some other programs replicating a maze solving algorithm [@5967320], the maze was built like a globe. That is, if the agent travelled north they would eventually end up on the southern side of the maze.\nWhereas in others, a flat maze was used [@Amini]. The agent can’t end up on the other side of the map by travelling outside of the border.\nIt was chosen to use a flat maze to represent the maze. This is because a 3-dimensional global maze represented in 2-dimensions might be too confusing for a student to worry about. The project was designed to help students understand reinforcement learning, not to help them understand how to transpose a 3-dimensional globe into a 2-dimensional flat space.\nIn some other mazes, the agent receives a harsh negative reward for trying to visit outside of the square. Logically, the author does not believe that a student should have to worry about the agent attempting to travel outside of the maze. To humans, the idea of walking backwards out of a maze and finding the end by walking around the maze, instead of through it would be cheating.\nWhile other creators decided to negatively punish the agent, the author deemed it to too much for the agent to attempt to even travel outside the maze. After all, the students in the classrooms themselves wouldn’t travel outside of a maze to try and beat it. Therefore, in getLegalMoves() it is impossible for the agent to even consider travelling outside of the maze.\nThe Legal Move Set Another problem the creator faced was \"what moves should the agent be allowed to perform?\".\nOne option would be to allow the agent to move Left, Right, Up, and Down. Another would be to allow diagonals such as North-West, South-East, South-West, North-East.\nAnother option would be to disallow any upward movements, as the goal tended to be downwards. However, at the time the creator decided this didn’t make sense. What if the goal was moved? Similarly, what if we banned left movements but the goal was on the left-hand side but there was a wall right below the agent? Therefore, the creator stuck to the traditional up, down, left, and right movements.\nWhen deciding on diagonal movements or not, the creator had to take into account the simplicity of the program. The creator wanted to make the code as simple as possible to allow the students to focus on the reinforcement learning aspect of the project.\nIf the creator added in diagonal movements, the vector addition would have to change. One cannot simply add one vector to another to get a diagonal movement, the program would have to add at least 3 vectors to get to the right spot. Overcomplicating the code meaning it is harder for students to learn from.\nIn the end, the creator chose to use the normal move set of up, down, left, and right.\nCreation of the learning materials The project was not intended to be 100% code-based. More so it was intended to create learning material to teach reinforcement learning to students that use code as a means to teach.\nFor this reason, the majority of the project sits in designing an easy to understand code interface for students, as well as lessons that teach students reinforcement learning.\nThe Learning Materials Here is each part of the lesson that was created. It features a small explanation, along with a task. It was expected that the teacher would give a short presentation on the task, as well as setting everyone up to use the code. It was estimated to take 15 - 25 minutes to get everyone to use the code, and 10 - 20 minutes to explain what’s happening and why it’s happening. Hence why the tasks are short. Also, the creator of the project was tasked with creating a program with some example tasks to teach reinforcement learning, not lesson plans themselves.\nThe Lesson What is Reinforcement Learning? Reinforcement learning is similar to how we humans learn. When we want to learn how to open a jar, we repeatedly try to open it in many different ways. First clockwise, then anti-clockwise, maybe we’ll use a tea towel or our shirts to get a better grip. Eventually, after trial and error, we know how to open a jar.\nReinforcement learning is the same. The machine repeatedly tries one thing after another until it gets it right.\nFor example, if we have a maze where the machine has to go from one side to the other, the machine will bump and get lost millions of times before it eventually learns how to complete the maze and even the fastest way through the maze.\nWhat is an Agent? The agent is the little person that explores the maze! We traditionally call them agents, because we tell them to accomplish something and they do it. And also, universities prefer agents over \"tiny computerised humans\".\nWhat is a Rewards Table? The way agents learn is that they store all the information they gather in a table. Specifically, the information gained from an agent is:\n  Where did the agent start?\n  What is the new location of the agent?\n  What is the reward of moving this way?\n  Take, for instance, the agent is at the start at (0, 0). If the agent moves right, that might give a reward of 0.2. If the agent moves down, it might give a reward of 0.5. The agent now knows the best move is to move down.\nThe agent has to write these things down. As humans, we also use memory to remember things. \"This door opens inwards\", \"to unlock the front door you have to push the lockdown and the door handle at the same time\" and so on. The agent has a memory, just like us.\nTask 1 We’re going to change how fast the agent learns, and we’re going to explore what this will result in.\nThe learning rate is the speed at which the agent learns. Set it to 1.0, and the agent learns extremely fast. Set it to 0, and the agent doesn’t learn at all.\nThe learning rate can only be between 0 and 1, think of increments like 0.25, 0.7831 and so on.\nHowever, simply increasing it to 1 does not mean the agent finds the best route through the maze. Because the agent learns so fast, it might be rushing through the maze missing all the import steps to finding the fastest way through it.\nBut setting it too low such as 0.01 means the agent is very slow, and it will take a longer time for the program to complete.\nOpen the file ‘Qagent.py’, and find this part in the file (it’s on line 13):\n \"# How fast can the agent learn? self.learningRate = 0.25\"\n 1. Set the learning rate to 0.5.\nNow, run the file ‘maze.py’ and observe the output. **Note: every time we want to run the agent, we have to run maze.py**\nLook at the path the agent took, and the rewards table. How big is the path? How small is the rewards table?\nPlay around with the learning rate. Set it to any number between 0 and 1, and see for yourself how changing the pace of learning the agent’s outputs are different.\nTask 2 We’re going to play around with the ‘self.explore’ variable now.\nThe explore variable is a probability between 0 and 1 (where 0.15 is 15\nIn reinforcement learning, we have a trade-off of exploiting what we know or trying something different.\nTake, for instance, buttering bread.\nIf we butter bread like a circle, going clockwise around the bread it’s not very efficient. But if we never explore, we would never find the most efficient way to butter the bread.\nIf the agent only does what it knows, it will never find the most efficient route through the maze.\nThe exploratory variable means that every move the agent takes, it has a 15% chance to randomly select a move, regardless of whether or not it understands how rewarding the move is.\nFind the exploratory variable at line 37 of Qagent.\n \"The probability that the agent will explore on that round, instead of exploiting the rewards table self.explore = 0.15\"\n 1. Set the exploratory percentage to 100 (‘self.explore = 1’) See how the agent has lost its memory? It has a memory, but it doesn’t use its memories! Every move it makes is random.\nWhat if we set explore to 0? Will the agent find the best route through the maze or not?\nTask 3 Now we’re going to understand the randomness aspect of exploratory. Because the agent explores, that means every time the agent attempts the maze will be different from the last time. Run your programs and have a look at the person next to you. See how they’re different routes? Different rewards? One of your agents is smarter than the other!\nBut, you might see a problem. There is only 1 route which is the fastest way through the maze. But if there are 1 route and every agent is slightly different because of the random explorations, how does it find the best route?\nThe answer is using epochs.\nEpoch is a fancy word for \"time\". Specifically, we might tell the agent:  \"Do this maze 10 billion times\" If we make 2 agents do the maze 2 or 3 times, their answers will be completely different. But if we make the agents do the maze 10 billion times, they will converge on the correct answer - the fastest route through the maze!\nSet the epochs variable to a high number, such as ‘10000’. However! Your screen might go crazy, so try not to read every single line that comes out of the program.\nYou can find epochs on line 34 of Qagent.\n \"How many times will the agent complete the maze before it stops? self.maxepochs = 1\"\n Note: You will need to set maxepochs back to 1 for the rest of this tutorial.\nTask 4 Let’s talk about gamma!\nGamma is the value of the future reward. If Gamma is set to 1, the agent sees reaching the goal in 10 moves and reaching the goal in 1 move as the same value.\nIf gamma is set to 0, the agent values direct moves (what it’s doing on the next turn) more than it does in the future.\nWhen we humans try to solve a maze, we know where the exit is and we try to tend towards the exit. If the agent’s gamma was 0, the agent wouldn’t tend towards the exit. The agent would simply try to collect as much reward as possible until it accidentally stumbles upon the exit.\nFind gamma on line 31 of Qagent:\n \"How much the agent expects to gain from future value. self.gamma = 0.5\"\n 1. Set the gamma rate to 1. 2. Set the gamma rate to 0.\nSee how they differ?\nTask 5 Now, we’re going to work on punishing the agent. To prevent the agent from wandering around in circles, the agent takes a punishment everytime it moves.\nIf the punishment is too much, the agent gives up and we get a new agent to try (but with the same rewards table as the last agent).\nIncreasing the punishment means the agent gives up faster, but also means the agent is more encouraged to find the goal faster.\nToo low of a punishment and the agent will think \"why bother?\" and will reach the goal in a much slower method.\nFind this on line 43 of Qagent:\n \"The agents penalty for moving. Prevents the agent from running around in circles self.penaltyMoving = -0.05\"\n 1. Set the penalty to -1. 2. Set the penalty to 0.\nObserve the outputs.\nTask 6 This is a more advanced task, so be prepared!\nThe maze the agent uses is located in ‘maze.py’ (line 12), specifically this is it:\n \"self.maze = np.array([ [ 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 0, 0, 1, 0, 0, 1, 0, 1, 1, 1], [ 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [ 1, 1, 0, 1, 0, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [ 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 0, 1, 1] ])\"\n The 1 represents openness, a straight corridor for the agent. The 0’s represent a wall, something the agent can not pass.\nWhat if we make the goal impossible to reach? The goal is in the bottom right-hand corner.\n1. Change some of the 1’s to 0’s (walls) to prevent the agent from reaching the goal. What will happen then?\nPlay around with changing the layout of the maze and see how the agent reacts.\nTask 7 Now we’re going to change what moves the agent knows.\nOn line 28 of ‘Qagent.py’ the agent defines the moveeset as:\n \"self.possibleActions = \"Up\": (1, 0), \"Down\": (-1, 0), \"Left\": (0, -1), \"Right\": (0, 1)\"\n The agent can move up, down, left, or right.\nIt does this using vector addition. On a 2 dimensional map, adding (1, 0) to our current coordinates (15, 15) means we would go up 1 square to (16, 15).\n1. Try to remove some moves. You can do this by deleting the entire dictionary entry. For example, to reduce Up do this:\n \"self.possibleActions = \"Down\": (-1, 0), \"Left\": (0, -1), \"Right\": (0, 1)\"\n Don’t forget to remove the comma!\nTask 8 Now we’re going to change the agents start location. Currently, it starts at the top left (0, 0). But it can start anywhere!\nThe way the coordinates system works is (row, column). \"Along the corridor, up the stairs\".\nFind the code on line 26 of ‘maze.py’:\n \"self.agentLocation = (0, 0)\"\n 1. Try setting the agent location to (5, 5) and seeing what the agent does! Does it walk to the top left again? Or go straight to the goal.\nTask 9 Change the goal.\nThe goal variable is located at line 21 of ‘Qagent.py’.\n \"self.goal = self.maze.getSizeOfMaze()\"\n 1. Change the goal to a location somewhere on the maze. For example, (15, 15) like so:\n \"self.goal = (15, 15)\"\n 2. If you set the goal and start location to the same place, what happens?\nTask 10 This is an advanced task. 1. Change how the agent learns\nSome things you may want to do: * Remove the penalty completely (by setting it to 0.0) * Remove the agent giving up when it is punished too much * Play with the learning rate * Play with the gamma rate\nTry to find the most optimal settings which result in the most optimal agent. Often, we would program a computer to automatically find the most optimal settings. But it is possible to deduct and use logic to work out roughly what the most optimal settings are.\nGoogle things you don’t understand such as \"learning rate\" or \"gamma q learning\" to get the answers you’re looking for.\nTesting and Evaluation Throughout the project, the creator used many means of testing to ensure the program was of high quality. Furthermore, the creator regularly asked for feedback to enhance the project.\nContinuous Integration Every time the code is committed to the master branch on GitHub, a GitHub action automatically tests the code. Firstly, for syntax compliance. Secondly, for unit testing. The syntax testing used was Black, and the testing module used was Pytest.\npython name: Python application\non: push: branches: [ master ] pullrequest: branches: [ master ]\njobs: build:\nruns-on: ubuntu-latest\nsteps: - uses: actions/checkout@v2 - name: Set up Python 3.8 uses: actions/setup-python@v1 with: python-version: 3.8 - name: Install dependencies run: | python -m pip install –upgrade pip pip install black pytest if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - name: Lint with black run: | # stop the build if there are Python # syntax errors or undefined names black . –count –select=E9,F63,F7,F82 –show-source –statistics # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide black . –count –exit-zero –max-complexity=10 –max-line-length=127 –statistics - name: Test with pytest run: | pytest\nMore specifically, the GitHub action in figure 15 was executed.\nUnit Tests Because the project was based on reinforcement learning, the exact maze, outputs, rewards, and other variables will differ for every time the program was run. Thus, there are little unittests involved other than to make such the Q learning algorithm operates correctly and the program doesn’t break under pressure.\nThe testing software used was PyTest, rather than the standard library UnitTest. This was because:\n  PyTest required less code to create tests.\n  PyTest provided more information on the errors.\n  Although, the biggest benefit to PyTest was that it was the default testing suite in GitHub Actions.\nThe tests were written in the style of boundary testing. Where a test is written for one extreme, and then the other. For instance, the Q Learning Algorithm tests were written using negative numbers, extremely large numbers, and the number 0. See figure 16 for some of the tests used.\npython def test-q-algorithm-normal(): \"\"\" Testing to see if the q algorithm operates correctly \"\"\" q = Qagent.agent(m) result = q.qAlgorithm(0.54, 1, 0.67) assert result == 0.835\ndef test-q-algorithm-negative(): \"\"\" Testing to see if it breaks on negative \"\"\" q = Qagent.agent(m) result = q.qAlgorithm(-50000, 60, 0.9999999) assert result  1\ndef test-q-algorithm-big(): \"\"\" Testing to see if it breaks on large numbers \"\"\" q = Qagent.agent(m) result = q.qAlgorithm(5000000000000000000 00000000000000000000000000000000, 5000000000000000 00000000000000000000000000000000000, 50000000000 0000000000000000000000000000000000000000) assert result  1\ndef test-q-algorithm-zero(): \"\"\" Testing to see if it breaks on 0 input \"\"\" q = Qagent.agent(m) result = q.qAlgorithm(0, 0, 0) assert result == 0.0\nAt the time this test was run, PyTest outputted this:\npython .… 4 passed in 0.09s\nWhere the four full stops represent each test as seen above passing.\nMore tests were written for the maze program. The maze class was responsible for creating the maze and deciding on what moves are legal. See figure 18 for the maze program’s tests.\npython def test-maze-movement(): \"\"\" Testing the movement of the agent in the maze\"\"\" m = maze.maze() # move to the right starting at 0,0 result = m.vectorAddition((0, 1), m.getAgentLocation())\nassert result == (0, 1)\ndef test-maze-movement-up(): \"\"\" Testing the movement of the agent in the maze\"\"\" m = maze.maze() # move to the right starting at 0,0 result = m.vectorAddition((1, 0), m.getAgentLocation())\nassert result == (1, 0)\ndef test-maze-movement-legal-moves(): \"\"\" Does legal moves work?\"\"\" m = maze.maze() # move to the right starting at 0,0 result = m.getLegalMoves()\nassert True if \"Right\" in result else False\npython test-maze.py ... test-program.py .… 7 passed in 0.10s\nAll the tests ran successfully.\nContinuous Manual Testing After a new function was written, that function would be copied to Python’s IDLE, where it was evaluated. Take the function of element-wise vector addition. After the function was written, it was copied and pasted into a terminal and manually tested.\nThe function did not rely on any external libraries or code, it merely added 2 vectors element-wise and returned it. As such, the chance of this function containing future errors after testing was none, unless Python’s default operators changed drastically, which was unlikely in the near future.\nOther functions were also the same, where it was unfeasible to imagine that in the future they would break. Thus, no unit tests were written for these functions.\nAfter each function was added and integrated into the core code, the code was run. The programmer evaluated the code, making sure that the integration of the new function did not break any important parts of the code.\nIf the code did break, no major harm came to the already working code. The programmers utilised branches on GitHub, creating a new branch for each change to the code. This meant that the master branch was always fully tested and worked, whereas the other branches could be broken.\nOnce the programmer committed code to the master GitHub branch, the code was evaluated using the GitHub action. If GitHub failed that testing, it would email the creator to let them know.\nEvaluation Earlier in this projects life cycle, it had been planned that the project would be demonstrated to teachers in classrooms or even used in classes, and the creator would use this feedback to better the product.\nHowever, due to COVID-19, the creator couldn’t enter a classroom.\nTherefore, the creator opted to ask other people who work with children (who are around the same age as year 7’s to year 11’s).\nThe creator sent the program and the learning materials to these people. However, most of them weren’t in ICT and didn’t understand Python, or artificial intelligence.\nOne clear comment was that if the teacher had decided to google \"gamma reinforcement learning\", the explanation given is that which would require a university degree to decipher. Because of this feedback, the creator tried to create a simpler explanation of some of the core concepts. So not only students can understand, but teaching staff may understand as well.\nThroughout this project, the creator spoke regularly with Dr. Dennis and Dr. Thomason, who were in the outreach program at the University of Liverpool for Computer Science.\nDr. Thomason’s most critical comment was that the project was too ambitious for a classroom. They stated that in a class, the creator can expect up to 20 minutes of solely installing the program, and without clear guidance, the students were likely not to learn. Thus it was needed for learning material to be created alongside, which specifies exactly what the students should do.\nAnother comment made by Dr. Dennis was that the students are all at different levels. It may be a GCSE Computer Science class, but some of the students will speed through the resources while others might spend the entire lesson on the first task. It proposed an age-old question, \"what do we do about the difference in students’ abilities?\".\nThe answer was to create many tasks, progressively getting harder with each task. The idea is that the first few tasks are easy, and can be used to encourage the slower students to get through them. While the last tasks are very difficult, which given the limited amount of time in a classroom should prove worthwhile to the faster students.\nAnother comment made that impacted the work of the creator was that reinforcement learning was simply too hard. The original idea was to teach students how to design an autonomous driving vehicle. Dr. Thomason told the creator that what may seem easy to a university student isn’t easy to a secondary school student. For this reason, the simplest task of them all is changing a variable and seeing how it affects the agent’s run.\nOverall, the creator would have liked to get the software in front of a classroom, to ask teachers directly and see how the students learn from it. But due to COVID-19, it simply wasn’t possible.\nBCS Project Criteria Practical and Analytical Skills This relates to BCS criteria:\n \"An ability to apply practical and analytical skills gained during the degree programme.\"\n This project is an AI system designed to be taught in schools. The creators’ modules over their degree which relate to this project are:\n  Artificial Intelligence.\n  Advanced Artificial Intelligence.\n  Communicating Computer Science.\n  Software Engineering.\n  Software Engineering 2.\n  The skills gained from artificial intelligence were used to create a reinforcement learning agent. Without those modules, the creators understanding of artificial intelligence would have acted as an inhibitor for finishing this project.\nThe skills gained from communicating computer science were used to better design effective exercises for the students, as mentioned in the key literature and background reading.\nThe skills gained from software engineering helped the creator write better code, better in the sense that it runs faster but also better in the sense that users can read it, that it is tested, and it is generally good code.\nInnovation and creativity As Isaacson [@Isaacson] says in his book, The Innovators:\n \"innovation comes from being able to stand at the intersection of art and science.\"\n The project is scientific in the sense of creating a reinforcement learning agent. However, the intersection with art comes from creating a project to use it to teach key stage 3 students.\nOn the technical side, the innovation came from creating a Python implementation of a reinforcement learning agent using basic code, without relying too heavily on external libraries. In most examples, the code was written to only be understood by other programmers. Whereas the code for this project was written to be understood by anyone.\nSynthesis of Information This relates to BCS criteria:\n \"Synthesis of information, ideas and practices to provide a quality solution together with an evaluation of that solution.\"\n This project was created by Dr. Dennis to solve a real-world problem. When the creator took up this project, they read books such as [@Sutton] as discussed in Key Literature and Background Reading.\nThe creator took in this information and decided on what were the best parts to take from each resource. The maze problem from Sutton, the teaching skills from MIT and COMP335. The creator took all of this information and combined the best of it into one quality solution.\nThen, the creator critically evaluated this project in the Evaluation section. As well as continually evaluating it throughout the projects life cycle. The creator regularly sought guidance, asked for help, and watched as other people attempted to use the project to learn. Using this information, the creator continually improved the project until the end of its life cycle.\nReal World Need This relates to BCS criteria:\n \"That your project meets a real need in a wider context\"\n Dr. Dennis, the creator’s supervisor, created this project because of a gap in the market. More specifically, computer science taught in classrooms wasn’t fun, or approximately close to what computer scientists were working on at the time. Sorting algorithms such as bubble sort were taught for multiple lessons, as an example.\nDr. Dennis decided that reinforcement learning would be fun, as it is a current subject in artificial intelligence still being worked on, and can be explained easily to a younger student using analogies such as how that student learns.\nSelf-Manage a Significant Piece of Work This relates to the BCS criteria:\n \"An ability to self-manage a significant piece of work.\"\n Throughout this project, the creator regularly stuck to the Gantt chart to monitor how well they were doing and the time schedule of code. As well as the Gantt chart the creator planned for emergencies, any issues that might arise and took care of them accordingly.\nThis dissertation is one example of many that the creator can self-manage a significant piece of work.\nCritical Self-Evaluation of the Process At the start of the project, the creator went to work quickly. Having finished a prototype before the first month finished. The creator asked Dr. Thomason for support in creating the educational texts and even took up online classes to better learn how to teach this subject to the students.\nThe creator followed the Gantt chart originally made, and all was set to be well.\nUnfortunately, the creator suffered an illness in December that went on until late February or early March. During this time, the creator carried on coding but could not communicate effectively with their supervisor or the university.\nShortly after the creator became well again, COVID-19 stormed the world. Causing panic in the creator, and worry about what’s going to happen.\nUnfortunately, the creator did not succeed in all the original tasks of the dissertation. One of the original tasks was to implement this reinforcement learning agent into a small robot. This task failed, mainly due to the illness but partly due to COVID-19 not allowing the creator to physically pick up the small robot to try and implement it.\nIn the future, the creator will take extra care and will try to assume the worst - including a potential global pandemic. If the creator had been in a more communicative state during their December - February illness, they might have just avoided COVID-19 and may have been able to implement the agent into the small robot.\nThe creator, unfortunately, felt like they didn’t need to tell anyone what was wrong with them, which led to their supervisor and university becoming worried and asking if they were okay. The creator did not want to admit their own defeats, so they refused to communicate with the university from December to February.\nThe dissertation portion of the final year project was also delayed. The creator felt like they could create more tasks, more agents that could help aid the students. The creator felt bad that they couldn’t complete the small autonomous robot task, as it was impossible to pick up during COVID-19. Therefore, the creator, instead of spending time working on the dissertation, spent most of their time trying to create more tasks and improving the agents.\nWhat would have been a better choice would be to discuss with their supervisor what the best use of their time could be.\nOverall, the creator has learnt that to self-manage such a large project means that the creator should regularly communicate with those supporting them. Without that support in December - February, the creator was lost and this led to the failure of the mini robot car task.\nThe creator should have put their own health first, and then the university studies second. This would have no doubt caused the illness the creator to suffer from to have been reduced, and it would have let the creator continue to come back to work much sooner. The creator was scared that their supervisor wouldn’t allow them to look after themselves and they would have been forced to work full speed on their project. When in actual fact, looking at what has happened it seems more likely that the creator would have done more work and gotten back on track much quicker if they had been honest and open with those actively supporting the creator.\nIt would have been nice for the creator to show the agent moving through the maze on the map, clearly. However, this caused issues at higher epochs and it was hard to see the agent moving through the maze amongst the 0’s and 1’s, again the creator focused too much on the design and creation of the project rather than spending it on working what matters most to the project.\nConclusion Although this project failed in one of the more major tasks, it was a success in that the creator still managed to fulfil the original goal of creating a reinforcement learning system that can teach key stage 3 students about artificial intelligence and encourage them.\nThere would always have been something extra to have done or something new and exciting to add to the project. Merely finishing the project on time was a blessing for the creator, and it is exciting to know that the project will potentially be used to excite the next generation of computer scientists.\nLooking back at the introduction, these were the tasks assigned to the creator:\n  Create an ASCII based grid-world reinforcement learning playground.\n  Implement a reinforcement learning framework in an existing Python-based robot simulator.\n  Design an API for reinforcement learning.\n  Devise a set of challenge problems to be used with the simulator.\n  Design the learning exercises.\n  Gather feedback about the learning exercises and improve upon it.\n  Create an autonomously driving robot vehicle agent.\n  The creator did make a grid-world reinforcement learning playground, in maze.py. The creator did design an API to be used in IDLE, they did devise a set of problems to be used with the simulator as seen in the exercises section. The creator gathered valuable feedback from the exercises and used that feedback to improve upon the exercises.\nOverall, the creator succeeds in 6 out of the 7 tasks, which is positive as most software projects tend to fail or over-run.\n",
  "wordCount" : "12089",
  "inLanguage": "en",
  "datePublished": "2019-11-29T23:46:37.121Z",
  "dateModified": "2019-11-29T23:46:37.121Z",
  "author":{
    "@type": "Person",
    "name": "Bee"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://polymath.cloud/dissertation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Polymath.cloud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://polymath.cloud/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://polymath.cloud" accesskey="h">Polymath.cloud</a>
            <span class="logo-switches">
                <span class="theme-toggle">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://polymath.cloud/archives" title="Archive">
                    <span>
                        Archive
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/search/" title="Search">
                    <span>
                        Search
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/tags/" title="Tags">
                    <span>
                        Tags
                    </span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots
    </h1>
    <div class="post-meta">

November 29, 2019&nbsp;·&nbsp;57 min&nbsp;·&nbsp;Bee

    </div>
  </header> 

  <div class="post-content">
<h1 id="abstract" class="unnumbered">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<p>Reinforcement learning has traditionally been a dull topic to learn,
especially for students who may not understand programming. With this
dissertation, it is shown that reinforcement learning can be joyous to
learn, and easier than some might have suspected.</p>
<p>This dissertation shows a framework for teaching reinforcement learning
to key stage 3 school students. Using Python to implement reinforcement
learning, and using lessons based around the code.</p>
<p>The aim was that by teaching them a &quot;hard&quot; and &quot;cool&quot; computer
science topics, the students would become engaged in the world of
computer science.</p>
<p>This project implements a Q-learning reinforcement agent in a grid-world
2-dimensional maze to teach the students reinforcement learning. It also
has lessons and exercises for the students, that tie in with the
Q-learning agent to reinforce what the students can learn.</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>If students or teachers find a subject difficult, the chance that that
subject is picked (for GCSE or A Levels) goes down substantially
[@Ofqual]. Difficulty can come in many forms, whether it is:</p>
<ul>
<li>
<p>Bad teaching.</p>
</li>
<li>
<p>Under specialisation of teachers.</p>
</li>
</ul>
<p>However, the question will remain. How do we make reinforcement learning
interesting? According to Zahorik [@doi:10.1086/461844] some methods
include:</p>
<ul>
<li>
<p>Novelty.</p>
</li>
<li>
<p>Emotional Appeal.</p>
</li>
<li>
<p>Group Work.</p>
</li>
<li>
<p>Computers.</p>
</li>
<li>
<p>&quot;Lived experience&quot; (practical knowledge).</p>
</li>
</ul>
<h1 id="aims-and-objectives">Aims and Objectives<a hidden class="anchor" aria-hidden="true" href="#aims-and-objectives">#</a></h1>
<p>This project created an engaging set of exercises involving (simulated)
robots and reinforcement learning for key stage 3 students to learn
from.</p>
<p>The objectives are:</p>
<ul>
<li>
<p>Create an ASCII based grid-world reinforcement learning playground.</p>
</li>
<li>
<p>Implement a reinforcement learning framework in an existing Python
based robot simulator.</p>
</li>
<li>
<p>Design an API for reinforcement learning.</p>
</li>
<li>
<p>Devise a set of challenge problems to be used with the simulator.</p>
</li>
<li>
<p>Design the learning exercises.</p>
</li>
<li>
<p>Gather feedback about the learning exercises and improve upon it.</p>
</li>
<li>
<p>Create an autonomously driving robot vehicle agent.</p>
</li>
</ul>
<h1 id="key-literature-and-background-reading">Key Literature and Background Reading<a hidden class="anchor" aria-hidden="true" href="#key-literature-and-background-reading">#</a></h1>
<h2 id="reinforcement-learning-an-introduction">Reinforcement Learning: An Introduction<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-an-introduction">#</a></h2>
<p>This book by Sutton &amp; Barto [@sutton] was one of the most popular
introductions to reinforcement learning. The book starts simple.
Introduction, why reinforcement learning is needed, and early history of
it.</p>
<p>The book then introduced us to reinforcement learning using the K-armed
bandit. This is not a challenge in optimising the algorithm, it&rsquo;s simply
a problem to introduce us to the terminology of reinforcement learning
and how we can conclude that this problem can be solved with
reinforcement learning.</p>
<p>The book then took us through some paradigms we may know that can be
considered reinforcement learning paradigms. Such as:</p>
<ul>
<li>
<p>Dynamic Programming.</p>
</li>
<li>
<p>Finite Markov Decision Processes.</p>
</li>
<li>
<p>Monte Carlo Methods.</p>
</li>
</ul>
<p>Before taking us into tabular (Q-Learning) based reinforcement learning.</p>
<p>The book ends with the psychology of reinforcement learning. We know the
theory, and we know how to implement it, but how does this work with
humans? According to Sutton &amp; Barto [@Sutton]</p>
<blockquote>
<p>&quot;Reinforcement learning is the closest to the kind of learning that
humans&quot;.</p>
</blockquote>
<p>If reinforcement learning was closet to that of humans, Simon &amp; Barto
shows us how humans learn using psychology and neuroscience.</p>
<p>Reinforcement learning is a type of learning that involves repeatedly
(and often, failingly) doing things until the agent learns what moves
return the highest (or lowest) rewards.</p>
<p><img src="agentreward.png" alt="The Agent RewardSystem.[]{label="fig:agentReward&rdquo;}">{#fig:agentReward}</p>
<p>The agent takes an action. This action has some effect on the
environment. If the agent can only see 30cm in front of it and it moves
5cm forward, the environment doesn&rsquo;t change but the agent&rsquo;s perception
of the environment changes.</p>
<p>The agent might destroy, create, or move things in the environment.</p>
<p>This action returns an observation and a reward. The observation is what
has happened to the agent and the environment. For example, if the agent
moved 5cm forward, the agent can now see 5cm more than it previously
could.</p>
<p>The reward is either a positive reward or a negative reward. As an
example, the agent&rsquo;s goal might be to search for rocks. If the agent
sees a rock within 30cm of it now, the agent is rewarded.</p>
<p>The agent would then store this information into a Q-table. The Q-table
answers the question: &quot;When the environment is exactly this, and there
are these moves available, this is the reward you will get&quot;.</p>
<p>However, there are 2 types of rewards. Seen above is the positive
reward. The agent is positively reinforced by something. But a negative
reward is also possible.</p>
<p>As an example of a negative reward, the agent could be in an environment
with one straight line of light. The agent&rsquo;s purpose is to avoid the
light. If the agent moves into the light, the agent gets a negative
reward.</p>
<p>It is unusual for a reward to be both positive and negative. It is
either -1 and 0 or 0 and 1.</p>
<p>The agent uses this Q-learning table to make further decisions. However,
we stumble upon the exploration vs. exploitation problem.</p>
<p>Exploitation here means that the agent exploits the Q-learning table to
maximise its reward.</p>
<p>Let&rsquo;s say our agent learns to avoid the light by staying on the edge
between light and darkness. Our agent realises if it goes into the
light, it gains a negative reward. But if it stays between the two, it
doesn&rsquo;t gain a negative reward - but it also doesn&rsquo;t gain anything
positive.</p>
<p>The agent, since it exploits the Q-learning table, never learns that it
can stay out of the light completely and get a much higher score than
where it is.</p>
<p>Exploration, regardless of how much reward an action has, is important
in this regard as it informs the learning of the agent - it maximises
the agents learning.</p>
<p>Although too much exploration and exploitation will no longer offset the
value of exploration.</p>
<p>This can be solved by having the reward as a Boolean data type.</p>
<h2 id="deep-learning">Deep Learning<a hidden class="anchor" aria-hidden="true" href="#deep-learning">#</a></h2>
<p>Deep Learning by Goodfellow et all [@Goodfellow] is an introductory book
to deep learning. While Reinforcement Learning by Sutton &amp; Barto is an
introduction to reinforcement learning, it assumes prerequisite
knowledge of deep learning. More specifically, the mathematics involved
in deep learning.</p>
<p>Goodfellow et all start the book off with an introduction to applied
mathematics.</p>
<ul>
<li>
<p>Linear Algebra.</p>
</li>
<li>
<p>Probability and Information Theory.</p>
</li>
<li>
<p>Numerical Computation.</p>
</li>
<li>
<p>Machine Learning Basics.</p>
</li>
</ul>
<p>Once the reader is familiar with the mathematics behind deep learning,
Goodfellow et all introduce neural networks. Starting slowly with
feed-forward networks and then eventually ends with convolutional
networks and recurrent neural networks.</p>
<p>The final chapter is on the research, the more academically rigorous
side of this book.</p>
<p>The book proved to be useful in providing the creator knowledge of the
mathematics behind artificial intelligence.</p>
<h2 id="mit-deep-learning">MIT Deep Learning<a hidden class="anchor" aria-hidden="true" href="#mit-deep-learning">#</a></h2>
<p>While this project centred around reinforcement learning, most
fundamentally, it was about creating a framework to <strong>teach</strong>
reinforcement learning.</p>
<p>For the creator to understand the most effective methods to teach
reinforcement learning, they have chosen to study MIT&rsquo;s deep learning
course [@MIT].</p>
<p>The reason why MIT was chosen was that they publish every lecture online
for free. They are also an Ivy Leauge college, which means they pride
themselves on their world-changing research.</p>
<p>Some of the more important things the author learnt whilst watching the
lectures were:</p>
<h3 id="talk-slowly">Talk Slowly<a hidden class="anchor" aria-hidden="true" href="#talk-slowly">#</a></h3>
<p>All of the professors in the lecture videos talked slowly, but not too
slowly. They talked clearly, loudly, so everyone can hear.</p>
<h3 id="different-forms-of-media">Different Forms of Media<a hidden class="anchor" aria-hidden="true" href="#different-forms-of-media">#</a></h3>
<p>The professors used video, whiteboards, quizzes, live-programming and
more. Anything that isn&rsquo;t text on a plain background is appeared to be
fair game for them to use.</p>
<p>One of the things the author wanted to involve in their teachings is
talking over videos. The professor put on a video of a car learning to
drive [@lex], and explains how it is learning. We were engaged in the
video, and we learnt how it works.</p>
<h3 id="using-colour-to-learn">Using Colour to Learn<a hidden class="anchor" aria-hidden="true" href="#using-colour-to-learn">#</a></h3>
<p>All of the mathematical formulae used by MIT were colour coded.</p>
<p><img src="Capture.PNG" alt="An example of the colourful mathematical formulae used byMIT.[]{label="fig:mitformula&rdquo;}">{#fig:mitformula
width=&rdquo;\textwidth&rdquo; height=&rdquo;\textheight&rdquo;}</p>
<p>In this lecture by Professor Amini [@Amini] the colours have meaning.
The purple underline for the second half of the formula corresponds to
the purple final grades matrix on the right.</p>
<p>The quotation under each part of the formula stating what that part does
(&quot;Actual&quot;, &quot;Predicted&quot;). The use of the blue box at the bottom of
the screen showing what the code is for that formula.</p>
<h2 id="comp335">COMP335<a hidden class="anchor" aria-hidden="true" href="#comp335">#</a></h2>
<p>The creator took the module COMP335 - &quot;Communicating Computer
Science&quot;. [@COMP335]. This module aimed to enable the students to
better teach computer science to key stage 3 students.</p>
<p>The creator specifically learnt these key points from the module:</p>
<h3 id="students-are-slow">Students Are Slow<a hidden class="anchor" aria-hidden="true" href="#students-are-slow">#</a></h3>
<p>The idea of learning rates, which may seem simple to a university
student, is not simple for younger students.</p>
<p>Even the idea of a machine that is capable of learning like a human can
be confusing to younger students.</p>
<p>Dr. Thomason has shown that even formulae, as simple as formulae may be,
can still be confusing to a younger audience whose only exploration into
formulae is Pythagoras&rsquo;s Theorem.</p>
<h3 id="do-not-forget-things-we-take-for-granted">Do Not Forget Things We Take for Granted<a hidden class="anchor" aria-hidden="true" href="#do-not-forget-things-we-take-for-granted">#</a></h3>
<p>As a university student, the creator took for granted the idea that all
schools will have Python installed. That they will have the resources to
download packages, or install Python if is it not installed.</p>
<p>This is wasn&rsquo;t the case. Some schools have restrictive downloading, some
teachers may not even know how to install packages.</p>
<h2 id="discrete-mathematics-with-applications">Discrete Mathematics with Applications<a hidden class="anchor" aria-hidden="true" href="#discrete-mathematics-with-applications">#</a></h2>
<p>This book by Epp [@epp] is typically a first-year university students
introduction to discrete mathematics. The book served as a good pointer
to not only explaining mathematics well but explaining how to teach
well.</p>
<p>Like MIT [@MIT], the book made heavy use of any form that isn&rsquo;t plain
text. Images, quotes, history tidbits, miniature tests, tables, and
many, many more.</p>
<p>The book uses colour and syntax-highlighted code to get across how each
formula works.</p>
<h1 id="development-process-and-method">Development Process and Method<a hidden class="anchor" aria-hidden="true" href="#development-process-and-method">#</a></h1>
<p>The Agile Programming Metholdogy was used, more specifically SCRUM
[@schwaber2002agile]. SCRUM allowed the creator to design the end
program with the user in mind, using the user stories feature. Since it
is agile, the creator had a working prototype early on in the
development process which can acted as an ultimate fall back. See the
risks &amp; contingencies chapter for more information on this.</p>
<p>The project has to be easy to use for students. It would be unwise to
build software using waterfall, because at the end the stduents may not
understand how the software works and the project will be a failure.
Agile methodology allows us to work towards good code that is easily
understandable.</p>
<p>This is because if the project had failed entirely, the Waterfall method
would not have provided any working prototypes to use to teach.</p>
<p>The tools the creator used were:</p>
<h2 id="github">GitHub<a hidden class="anchor" aria-hidden="true" href="#github">#</a></h2>
<p>GitHub was used as a version control system, making backups of the code
and making it possible to revert to a previous version.</p>
<p>The GitHub Project board was used as a Kanban board to monitor the
progress of the project. With 4 sections. ToDo, Testing, Implemented,
and finally Issues.</p>
<p>The creator also made heavy use of GitHub actions, the continuous
integration part of GitHub to automatically run tests and verify working
code.</p>
<h2 id="python">Python<a hidden class="anchor" aria-hidden="true" href="#python">#</a></h2>
<p>The code s written entirely in Python 3.8, making use of small libraries
to allow for ease of use for a school.</p>
<h2 id="numpy">NumPy<a hidden class="anchor" aria-hidden="true" href="#numpy">#</a></h2>
<p>The project required NumPy, which is used to perform calculations that
are required in basic reinforcement learning such as the Q learning
algorithm and maintaining the Q learning table.</p>
<h2 id="local-backups">Local backups<a hidden class="anchor" aria-hidden="true" href="#local-backups">#</a></h2>
<p>Local backups of the code were regularly made on the creator&rsquo;s machine
and network-attached storage device.</p>
<h2 id="idle--code-editor">IDLE &amp; Code Editor<a hidden class="anchor" aria-hidden="true" href="#idle--code-editor">#</a></h2>
<p>Some schools were not able to use a code editor and instead used IDLE -
Python&rsquo;s integrated development environment. The code was regularly
tested against both an editor and IDLE.</p>
<p>The creator made a note that the biggest issue was that what worked on
the development PC wouldn&rsquo;t always work on the school PCs. They may not
have the required packages, or they may not even have Python installed
at all.</p>
<p>The software was developed on a separate virtual machine made
specifically for the development of this project. On this machine,
libraries such as NumPy were automatically installed. Each new install
was vetted with the creator.</p>
<p>If the worst came to the worst during the demonstration of this project,
the creator would be able to export the virtual machine and install it
on the computers in the classrooms. Dr. Thomason had said that this was
done before with Kali Linux but is not the preferred way to do things.
However, it was still possible.</p>
<h1 id="data-sources">Data Sources<a hidden class="anchor" aria-hidden="true" href="#data-sources">#</a></h1>
<p>No data sources were used for this project. However, potential data
sources might be used by the students.</p>
<p>The maze program set out for the students can be altered, to such a
degree that students may search online for mazes that will work with the
project. They may download these alternative data sources and attempt to
incorporate those into the program.</p>
<p>However, no data sources were used by the creator of the project.</p>
<h1 id="ethical-considerations">Ethical Considerations<a hidden class="anchor" aria-hidden="true" href="#ethical-considerations">#</a></h1>
<p>Here is a list of the ethical issues discussed between the creator and
their supervisor.</p>
<h2 id="combining-2-modules-into-1">Combining 2 Modules Into 1<a hidden class="anchor" aria-hidden="true" href="#combining-2-modules-into-1">#</a></h2>
<p>The creator combined 2 modules into 1. COMP335 Communicating Computer
Science with COMP39x, this project. There were some ethical issues
raised due to using the same work for 2 modules.</p>
<p>After the creator discussed this with their supervisor, due to COMP335&rsquo;s
deadline of &quot;finish a lesson plan by week 12&quot; this would mean that the
project would have to have a working prototype by week 12.</p>
<p>Because of this limitation by combining 2 modules the creator did more
work than if I was to do each module separately, which made it ethically
&amp; morally okay.</p>
<h2 id="how-to-evaluate-the-effectiveness-of-this-project">How to Evaluate the Effectiveness of This Project<a hidden class="anchor" aria-hidden="true" href="#how-to-evaluate-the-effectiveness-of-this-project">#</a></h2>
<p>The simplest way to evaluate the effectiveness of this project was by
asking the students (who are children) what they think of it. However,
this brings up a plethora of ethical issues with collecting data from
children.</p>
<p>The easiest way to evaluate this project wasn&rsquo;t through the children or
the teachers. During the teaching sessions for COMP335 Dr. Thomason will
evaluate the teaching. By using this dissertation project in the
teaching, Dr. Thomason will partly mark the project (the activity) as
well as the teaching.</p>
<p>Throughout the year, the widening participation team at the department
hosted events. Dr. Dennis, my supervisor, suggested it might be possible
for the creator to bring the project to one of these.</p>
<p>However, due to COVID-19 it wasn&rsquo;t possible to teach, or attend any
events. The evaluation chapter will go into more details.</p>
<h1 id="design">Design<a hidden class="anchor" aria-hidden="true" href="#design">#</a></h1>
<h2 id="the-grid-world">The Grid World<a hidden class="anchor" aria-hidden="true" href="#the-grid-world">#</a></h2>
<p>The Grid World was the environment in which most of the players
experienced the exercises.</p>
<p>The grid world was a $\alpha ;x; \beta$ world. The grid world had
walls, made of the number 0. The idea behind this was that the number 0
is physically wide, much wider than the number 1. Therefore, wideness
equated to a wall and the number 1 equated to a corridor.</p>
<p>python 0000000000000000 0111111111111110 0111111111111110
0000000000000000</p>
<p>The environment object created a new grid world (or use a previously
generated one) for every run. An example of a simple grid world is:</p>
<p>python 0000000000000000 X111111111111110 01111111111111Y0
0000000000000000</p>
<p>Where X is where the agent starts and Y is where the agent ends.</p>
<p>The creator then represented this world in a 2d array.</p>
<p>If the objective was to follow a light, the grid world might have looked
like this:</p>
<p>python 0000000000000000 X 0 &mdash;&mdash;&mdash;&mdash;&mdash;- 0000000000000000</p>
<p>Where the line is the light. The agent will then &quot;explore&quot; the world.
The agent might decide to move to the right (forwards).</p>
<p>python 0000000000000000 X 0 &mdash;&mdash;&mdash;&mdash;&mdash;- 0000000000000000</p>
<p>This did not return a reward, so the agent tries again. This time, it
moves into the light.</p>
<p>python 0000000000000000 0 X&mdash;&mdash;&mdash;&mdash;- 0000000000000000</p>
<p>This returns a reward, so the agent knows it is doing the right thing.</p>
<p>Of course, this is a very simple problem with only 2 lanes to choose
from. A real-life problem might simulate the disparity of light. The
light might not be a perfect line, it might have signs of light all over
the place. Such as this:</p>
<p>python 0000000000000000 &mdash;&mdash;&mdash;&mdash;0 X============= 0000000000000000</p>
<p>Here we used a double line (equals symbols) to represent the strength of
the light. The light in the second lane is weaker, so it only used 1
line.</p>
<p>With a grid world like this, and teaching this to key stage 3 pupils, it
is important to wisely choose the symbolic meaning of each item in the
grid world so the students can effectively learn and understand what is
happening on their screen. Abstract symbols will only make for confused
students.</p>
<p>The reasoning behind 0 is that it is physically wide, like a wall.
Whereas 1 is physically thin, representing a corridor.</p>
<h2 id="components-of-the-system">Components of the System<a hidden class="anchor" aria-hidden="true" href="#components-of-the-system">#</a></h2>
<p>There were 2 separate modules to the system. Agent.py, and Maze.py.</p>
<p><img src="apitalk.png" alt="Organisation of the system[]{label="fig:api&rdquo;}">{#fig:api}</p>
<p>Agent.py controlled the agent and what the agent does, as well as the
Q-table. Maze.py controlled the maze, and decided on what was a legal
move for the agent to take.</p>
<p>Each task focussed on one part of the system, so the students
effecitvely learnt how they integrate togeter. If the task called for a
change to the rewards system, the student would explore Agent.py. If the
task called for a change to the legal moveset or the maze itself, the
students entered into Maze.py.</p>
<p>By seperating 2 distinct parts of the system this way, students saw how
the 2 parts work with eachother.</p>
<h2 id="data-structures-and-algorithms">Data Structures and Algorithms<a hidden class="anchor" aria-hidden="true" href="#data-structures-and-algorithms">#</a></h2>
<h3 id="arrays">Arrays<a hidden class="anchor" aria-hidden="true" href="#arrays">#</a></h3>
<p>The most important datastructure used was multi-dimensional arrays, used
for the Q-table.</p>
<p>To explain this datatructure, look at the below example.</p>
<p>python 0: [0.5, 421, -1, False)], 1: [(1.0, 228, -1, False)], 2:
[(1.0, 348, -1, False)], 3: [(1.0, 328, -1, False)], 4: [(1.0, 328,
-10, False)], 5: [(1.0, 328, -10, False)]</p>
<p>Where the dictionary has the structure:</p>
<p>python action: [(probability, nextstate, reward, done)]</p>
<p>This is an example of a rewards table in a multi-dimensional array.</p>
<h3 id="q-learning">Q-Learning<a hidden class="anchor" aria-hidden="true" href="#q-learning">#</a></h3>
<p>Q-learning is a reinforcement learning algorithm. The goal is to learn
something which tells the agent what action to take under what
circumstances. It does this using the reward table, as seen in the
previous section.</p>
<p>It is the simplest of all reinforcement learning algorithms, as
understood by [@Sutton] in their book. This is useful because a simpler
algorithm will be exponentially simpler to teach to key stage 3
students.</p>
<h2 id="user-interface">User Interface<a hidden class="anchor" aria-hidden="true" href="#user-interface">#</a></h2>
<p>There is no direct user interface for this project. It is a code
library. However, the design of the code library will matter.</p>
<p>All the important variables were represented near the top of the file,
clearly marked for the students to understand. Each function had full
documentation, as well as the thought process behind why that function
exists.</p>
<p>However. the lessons themselves were designed intentionally well.
Therefore this document details the design of the lessons, as they are
what the user interfaced with when they used the project.</p>
<h3 id="exercise-1">Exercise 1<a hidden class="anchor" aria-hidden="true" href="#exercise-1">#</a></h3>
<p>The first exercise was designed to be the easiest. Given this code:</p>
<p>python learningrate = 0.5</p>
<p>The user would change the learning rate to observe what will happen to
how the agent acts on the maze.</p>
<p>More specifically, the learning goal of this lesson was to learn what
the learning rate was, and see for themselves how the learning rate
affected how the agent behaved in the environment.</p>
<h3 id="exercises">Exercises<a hidden class="anchor" aria-hidden="true" href="#exercises">#</a></h3>
<p>Each exercise does not require knowledge of the previous lessons. This
meant that the students could skip ahead without worrying about missing
some vital information.</p>
<p>Each exercise contained a learning goal, to maximise the students
learning. The exercises contained succinct knowledge, with clearly
marked Python code and line number references to enable the students to
easily find the exact variables and functions they needed to change or
observe.</p>
<p>The agent runs with an epoch of 1, meaning it is easy for a student to
observe the changes in the terminal as they come in.</p>
<p>For more information on exercises, look at the implementation chapter
where every exercise created is detailed.</p>
<h2 id="uml">UML<a hidden class="anchor" aria-hidden="true" href="#uml">#</a></h2>
<p>Python had no public or private classes which contradicted the Unified
Modelling Language.</p>
<p><img src="umlDiagram.png" alt="UML Diagram[]{label="fig:UML&rdquo;}">{#fig:UML}</p>
<p>The environment and agent interact with each other and using a simple AI
API (which is the files themselves, as they are objects and can be
interacted with like an API) the tasks can be completed. The preferred
method was to directly edit the classes themselves, so the students
could get a direct feel of how the system works. But the API still
exists.</p>
<h2 id="simplification">Simplification<a hidden class="anchor" aria-hidden="true" href="#simplification">#</a></h2>
<p>The code contains simple variables which can be changed before run-time
to enable users to learn through real-life reinforcement learning. These
variables can be changed:</p>
<ul>
<li>
<p>The maze itself. Which is defined as self.maze. By changing
self.maze, it will change the maze the agent learns on.</p>
</li>
<li>
<p>The agent&rsquo;s starting location.</p>
</li>
<li>
<p>The allowed legal moves of the agent. In the default example, the
agent can move up, down, left or right.</p>
</li>
<li>
<p>The goal of the maze. What does the agent touch to know it has
reached the end of the maze?</p>
</li>
<li>
<p>The rewards table itself. It is possible to manually input into the
rewards table.</p>
</li>
<li>
<p>Gamma. The value of the future reward. How much the agent values
future rewards vs how much the agent values current rewards.</p>
</li>
<li>
<p>Learning rate. The speed at which the agent learns.</p>
</li>
<li>
<p>Maxepochs. How many epochs will the agent perform?</p>
</li>
<li>
<p>Explore. In Reinforcement Learning, it is a balance between
exploratory rewards and exploitionary rewards. Setting this to 50,
for example, would mean half the time the agent explores and the
other half the agent exploits, taking the highest possible reward
move.</p>
</li>
<li>
<p>AgentReward - how much reward the agent starts with.
Traditionally 0.</p>
</li>
<li>
<p>PenaltyMoving - how much of a penalty does the agent receive for
moving? This prevents the agent from going around in circles.</p>
</li>
</ul>
<p>In the code, these variables were very clearly explained and were at the
top of the code. This makes it easier for younger students to find the
variables to play around with. Here is a copy of that code.</p>
<p>python # Where does the agent start&gt; self.start = (0, 0)</p>
<p># Where is the goal of the agent? # self.maze.getSizeOfMaze() sets #
the agent goal to the very bottom right hand corner # Can use
coordinates such as (15, 15) self.goal = self.maze.getSizeOfMaze()</p>
<p># The reward table the agent starts with. # You can manually insert
rewards like: # self.rewardsTable = (1, 1): &quot;State&quot;: (1, 1), #
&quot;Action&quot;: (0, 1), &quot;Reward&quot;: 0.75, # (0, 0): &quot;State&quot;: (0, 0),
&quot;Action&quot;: (0, 1), &quot;Reward&quot;: 0.75 self.rewardsTable =</p>
<p># What actions can the agent take? self.possibleActions = &quot;Up&quot;: (1,
0), &quot;Down&quot;: (-1, 0), &quot;Left&quot;: (0, -1), &quot;Right&quot;: (0, 1)</p>
<p># How much the agent expects to gain from future value. self.gamma =
0.5</p>
<p># How fast can the agent learn? self.learningRate = 0.5</p>
<p># How many times will the agent complete # the maze before it stops?
self.maxepochs = 1000</p>
<p># The probability that the agent will # explore on that round, #
instead of exploiting the rewards table self.explore = 0.15</p>
<p># The agent&rsquo;s current reward when it starts self.Agentreward = 0.00</p>
<p># The agents penalty for moving. Prevents # the agent from running
around # in circles self.penaltyMoving = -0.05</p>
<p>The code was encapsulated in a class, so students could call the class
in Python&rsquo;s IDLE. As an example:</p>
<p>python import Qagent agent = Qagent.agent()</p>
<p>From here, it was possible to change the variables as simple as calling
them in IDLE.</p>
<p>python &gt;&gt;&gt; agent.learningRate = 0.97</p>
<p>And then calling the agent&rsquo;s run() method.</p>
<p>Since there are many epochs, and each epoch can have 10,000+ moves, it
was unreasonable to assume the students can monitor the maze in
real-time. It was also unreasonable for the students&rsquo; computers to be
able to display that many frames a second of a maze changing.</p>
<p>Therefore, it was proposed that students either perform 1 epoch to
monitor how the maze changes over time and thus how the agent will go in
the wrong direction before the rewards table is fully fleshed out, or to
look at the Q table and the successful route of each epoch.</p>
<p>If given as a presentation, it was recommended that the teacher uses the
former method. The students being able to see that the agent is, in
fact, dumb, and repeatedly goes the wrong way, but only finds the
correct route after brute-forcing all possible combinations is an
invaluable lesson.</p>
<p>The latter, watching the successful route of the maze, didn&rsquo;t look nice
on a low-resolution monitor. It was understandable that most schools do
not have a high-resolution monitor, and especially those with dyslexia
may not be have been able to tell the difference between edges of the
maze or the route used. Therefore, it was expected that the latter is
used more often than the former.</p>
<h2 id="design-of-the-documentation">Design of the documentation<a hidden class="anchor" aria-hidden="true" href="#design-of-the-documentation">#</a></h2>
<p>For more advance students, merely playing with the variables may not be
enough to learn. The more advance students may have wished to play
around with the code itself, to understand how to build their own
reinforcement learning agent. For that reason, almost all lines of the
codebase were commented. Extensive documentation was given, not only in
the form of comments but in the form of docstrings for each function.</p>
<p>The thought process behind that function and how it behaves as it does
was also included. So not only do the students learn how a function
behaves as it does, but also how they can develop the thought process to
create their own functions for their own reinforcement learning agents.</p>
<p>As an example, take the documentation for the reward() function at
figure 8.</p>
<p>The end goal of having such extensive documentation was to enable
students of all levels the ability to learn from the code. The students
starting at a lower level would have been able to read the documentation
and understand how the function works and what its rule was in
reinforcement learning. The students starting from a higher level will
be able to see how the creator&rsquo;s thought process solved the problems
solved by the functions, and how they can integrate this own thought
process into their own code to better solve problems.</p>
<h1 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h1>
<p>The implementation of this project was difficult. Not only did the
creator have to wrestle with reinforcement learning, but they had to
create lessons for key stage 3 students to learn from.</p>
<p>And the code written by the creator would have to be simple enough for a
key stage 3 student to manipulate and understand, for them to learn.</p>
<p>Ultimately, this proved many problems for the creator. This section on
the implementation will detail the multiple problems the creator had
with the implementation, as well as how this project was implemented.</p>
<h2 id="python-2-vs-python-3">Python 2 vs Python 3<a hidden class="anchor" aria-hidden="true" href="#python-2-vs-python-3">#</a></h2>
<p>When the creator first started out writing the code for this project,
they had to decide on what version of Python to use. Python 2 would more
likely be universally used in schools, as it was much older. But Python
3 was the newest version of Python, and the creator believed that Python
3 would be better to teach.</p>
<p>However, it is not down to the creator of this project to decide on what
Python versions schools use. The creator cannot simply show up and
demand Python 3 when the school has been teaching with Python 2 for so
long. It was unreasonable to expect the school to install a newer
version of Python too. Since on Windows, this can create compatibility
problems if the Python versions aren&rsquo;t properly sandboxed from each
other.</p>
<p>However, in late 2019 the Python Software Foundation, the overseeing
body of Python, announced that they will officially sunset Python 2 on
the 1st of December 2020 [@pycon].</p>
<p>The creator took this into account and realised that most teachers will
likely choose to upgrade to the latest version of Python 3, as that will
be the only supported version from 2020 onwards.</p>
<p>However, the creator knows that what a teacher should do and what they
are doing are 2 different things. The teacher may not have the resources
or the time to manage the switch over and to upgrade all of their own
teaching material in time.</p>
<p>For this reason, the creator worked hard to ensure Python2 compatibility
as well as Python3 compatibility.</p>
<p>However, it also depended on what version of Python 2 the schools were
using. The latest version Python 2.7 worked fine, but any earlier
versions that are no longer supported may not have worked.</p>
<p>For this reason, there were 3 options the teacher could have taken to
ensure the lesson could continue.</p>
<p>Firstly, the teacher could have installed Python 3 on their own work
laptop and used a projector to project the program, and give a lecture
on each task of the exercises. But this required the teacher to have a
work laptop that they can install software on, and even have a
projector. Not all schools will have this privilege.</p>
<p>The 2nd method was to manually teach the principles of reinforcement
learning without the use of code. Simply, a lecture was given on
learning rates, gamma, and so on.</p>
<p>Finally, the 3rd option was to open an online Python 3 sandbox program.
These programs allow any user to create, run, and test Python 3
applications on the web. What&rsquo;s more is that these online sandboxes can
install third-party software, such as NumPy which was required for this
project.</p>
<p>The final option was a lot harder than it may seem to implement. To use
these online code sandboxes the web browser would have to have been the
latest update, allowing use of WebGL and more.</p>
<h2 id="black">Black<a hidden class="anchor" aria-hidden="true" href="#black">#</a></h2>
<p>Black [@Anaya] was a Python formatter. Run Black on Python code and it
will automatically format it according to PEP8 [@KennethReitz2016], the
Python Style Guide. It was created and owned by the Python Software
Foundation, the organisation behind the language.</p>
<p>By running the code through Black, it was guaranteed to look similar to
other Blackend code, or any code that strictly follows PEP8. Because of
this enhancement, the code is easier to be read to someone that has seen
Blackened code before or even to someone that hasn&rsquo;t heard of PEP8.</p>
<h2 id="external-packages">External Packages<a hidden class="anchor" aria-hidden="true" href="#external-packages">#</a></h2>
<p>There were many ways to install the external package NumPy, all of them
are easy to do and required little resources. The NumPy installation was
20mb, and there are 4 methods mentioned in the documentation.</p>
<ol>
<li>
<p>pip install NumPy</p>
</li>
<li>
<p>python setup.py install</p>
</li>
<li>
<p>pip install -r NumPy</p>
</li>
<li>
<p>run install.py</p>
</li>
</ol>
<p>Pip was in the standard library for Python, so every Python installation
would have Pip installed. And using Pip does not require administrator
rights or a superuser account.</p>
<p>Setup.py will automatically install NumPy. Setup.py is recommended by
the Python Software Foundation to be packaged with all public releases
to allow anyone to run the code.</p>
<p>python from setuptools import setup</p>
<p>setup( name='Reinforcement Learning Agents&rsquo;, version=&rdquo;, packages=[&quot;],
url='https://github.com/Author/diss&rsquo;, license=&rdquo;, author='Author,
authoremail=&rdquo;, description=&rdquo; installrequires=[&lsquo;NumPy&rsquo;] )</p>
<p>Install.py is the final option, a Python file that calls the command
&quot;pip install NumPy&quot;. Below is the source code for install.py:</p>
<p>python import sys sys.os(&quot;pip install NumPy&quot;)</p>
<p>On every version of Python, Pip was automatically installed. It does not
require admin or superuser rights to run Pip. The NumPy module is only
20mb large, which is tiny in comparison to other modules. The hardest
part about installing NumPy will be running the pip install script,
hence 4 options were given to the user.</p>
<h2 id="rewards">Rewards<a hidden class="anchor" aria-hidden="true" href="#rewards">#</a></h2>
<p>One of the most daunting tasks of this project was the implementation of
a rewards system. It was simple to imagine that the Q algorithm would
take care of all the learning, and while it did provide a large relief
for the programmer, it was not completely up to scratch with how the
rewards system should work.</p>
<p>$$\underbrace{\text{New}Q(s,a)}-{\scriptstyle\text{New Q-Value}}=Q(s,a)+\mkern-34mu\underset{\text{New Q-Value}}{\underset{\Bigl|}{\alpha}}\mkern-30mu[\underbrace{R(s,a)}-{\scriptstyle\text{Reward}}+\mkern-30mu\underset{\text{Discount rate}}{\underset{\Biggl|}{\gamma}}\mkern-75mu\overbrace{\max Q&rsquo;(s&rsquo;,a&rsquo;)}^{\scriptstyle\substack{\text{Maximum predicted reward, given} \ \text{new state and all possible actions}}}\mkern-45mu-Q(s,a)]$$</p>
<p>More specifically, the largest problem the creator encountered was
deciding on what the reward should be.</p>
<h3 id="positive-or-negative-rewards">Positive or Negative Rewards<a hidden class="anchor" aria-hidden="true" href="#positive-or-negative-rewards">#</a></h3>
<p>The agent has 2 options for how their reward progresses. Take the maze
example. The agent gains a positive reward for reaching the goal, or the
agent incurs a negative reward every action until it reaches the goal.</p>
<p>In the former, the agent is encouraged to find the goal, but it does not
take into account the amount of time it takes. The agent gets there in
the end. In the latter, the agent is encouraged to move faster to reach
the end goal.</p>
<p>However, we must take into account how students, especially younger
students, understood a rewards system. In the traditional world,
students get rewarded for the &quot;good&quot; things they do, and punished for
the &quot;bad&quot; things they do.</p>
<p>The creator of the program decided that attempting to teach a new
rewards system to students may be too confusing. Especially trying to
explain how incurring a punishment every second was still a rewards
system, as the connotation of reward meant good rewards.</p>
<p>For this purpose, the agent&rsquo;s rewards are positive (more than 0), but
the agent can incur penalties depending on if the agent has done
something the creator does not wish it to do.</p>
<p>The agent reaches a positive reward for reaching the end goal, but the
agent incurs a penalty every time it moves. This penalty encourages 2
things of the agent:</p>
<ul>
<li>
<p>No senseless wandering.</p>
</li>
<li>
<p>To hasten the time it takes to find the goal.</p>
</li>
</ul>
<p>It was possible for the agent to become stuck in an infinite loop if it
never received a punishment for senseless wandering. And the punishment
encourages the agent to move faster to the goal.</p>
<p>However, on some occasions the agent decided to take the punishment as
is and do nothing about it. It was possible for the agent to become
stuck in an infinite loop, continually receiving negative rewards. It
was not enough to merely give the agent a punishment, the agent had to
learn to avoid the punishment.</p>
<p>For this reason, the &quot;game over&quot; variable was invented. This variable
stated that when the agent should declare game over, and stop learning.
The variable is there so the agent cannot get stuck in an infinite loop.
If it does, we start the games afresh and retry until the agent learns.
The game over variable is based on the agents own personal reward, not
the reward from each square in the grid world maze.</p>
<p>The exact numbers chosen for these variables were randomly selected by
the designer as being appropriate. It is expected that students will
fiddle with these numbers in the tasks created.</p>
<p>Finally, the agent incurs a further penalty if the agent re-visits a
square it has already been to. This penalty is the largest penalty of
all, in the hopes that the agent learns that visiting a place it has
already been to is severely bad because it should find the route in one
go, rather than having to double back on itself.</p>
<h2 id="the-maze">The Maze<a hidden class="anchor" aria-hidden="true" href="#the-maze">#</a></h2>
<p>The maze class controlled the operation of the maze. It controlled how
the agent moved around the maze, and what the maze itself was.</p>
<p>The maze itself was defined in a 2-dimensional array. In this array, the
number 0 represented a wall (because 0 is physically wide, and looks
closet to a wall) and the number 1 represents openness (because 1 looks
like a corridor, a thin strip of openness).</p>
<p>The analogies were to help the students understand what each bit of the
maze represented.</p>
<p>python [ 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1,
1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 0, 0, 1, 0, 0, 1, 0, 1, 1,
1], [ 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [ 1, 1, 0, 1, 0, 1, 1, 1, 1,
1], [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 0, 0, 0,
0], [ 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 0, 1,
1]</p>
<p>The start location was defined by agent.startLocation, and the goal was
defined by agent.goal.</p>
<h2 id="movement-of-the-agent">Movement of the Agent<a hidden class="anchor" aria-hidden="true" href="#movement-of-the-agent">#</a></h2>
<p>The agent chose exploratory or exploitatory moves. There was a 15%
chance it would have picked an exploratory (random) move, but this
number was expected to be changed by the students.</p>
<p>If the agent chose an exploratory move, it would pick a random legal
move.</p>
<p>If the agent chose an exploiatory move, it would calculate the legal
moves and work out which one has the highest reward to choose from.</p>
<p>The function getLegalMoves() calculated the legal moves of the agent.
The maze got the current location of the agent, performed a move on
those coordinates (for example, to go up is to add (1, 0) to the vector
of the agent&rsquo;s current coordinates) and then it would calculate whether
that move was legal or not.</p>
<p>A legal move needs to meet 3 conditions:</p>
<ol>
<li>
<p>The new coordinates were not negative, as there is no negative plane
on the 2-dimensional grid world.</p>
</li>
<li>
<p>The new coordinates are not physically outside of the 2-dimensional
grid world.</p>
</li>
<li>
<p>The move was possible to do.</p>
</li>
</ol>
<p>The creator made it impossible for the agent to leave the 2-dimensional
grid world, by visiting coordinates that did not exist in the planes.
For example, starting at (0, 0) and travelling up by (1, 0) would result
in coordinates that do not exist in the grid world.</p>
<p>This function is called before the agent tries to move to determine what
the legal moves are. This function meant that the agent cannot make an
illegal move.</p>
<p>python legalMoves = for move in self.moves: newCoords =
self.vectorAddition(self.agentLocation, moves[move]) # checks to see
if there is an illegal move negativeCoords = True if any(y &lt; 0 for y in
newCoords) else False</p>
<p>outsideMap = True if any(y &gt; len(self.maze[0]) - 1 for y in
newCoords) else False</p>
<p>illegalMove = True if negativeCoords + outsideMap == True else False</p>
<p>if not illegalMove and newCoords != 0: legalMoves[move] =
moves[move]</p>
<h3 id="vector-addition">Vector Addition<a hidden class="anchor" aria-hidden="true" href="#vector-addition">#</a></h3>
<p>One of the problems the designer had with agent movement was vector
addition. The coordinate system is based on vectors (x, y) where x is
the latitude and y is the longitude. To move around on this vector-based
coordinate system the agent had to add or take away other vectors
depending on the appropriate action. For example, to move physically up
by 1 the agent had to add (1, 0) to its current vector represented
coordinates.</p>
<p>But, in Python adding 2 vectors together resulted in a vector
representation that was unsatisfactory for this project.
$$(0, 0) + (1, 1) = (0, 0, 1, 1)$$ Whereas the creator wanted the vector
addition to being element-wise, as seen below:
$$(0, 0) + (1, 1) = (1, 1)$$ To accomplish this, the designer created
the function in figure 14. This function called operator.app on the 2
tuples (vectors), and then mapped this function over both vectors,
eventually turning it back into a tuple (from a list).</p>
<p>python def vectorAddition(self, tup1, tup2): return
tuple(map(operator.add, tup1, tup2))</p>
<h3 id="printing-of-data">Printing of Data<a hidden class="anchor" aria-hidden="true" href="#printing-of-data">#</a></h3>
<p>When the program runs, it prints data to the terminal. For example
running the program for the first time and letting it run for a loops
results in this:</p>
<blockquote>
<p>&quot;The steps taken so far are [(0, 0), (0, 0), (1, 0), (2, 0), (1, 0),
(0, 0)]&quot;</p>
</blockquote>
<p>Here we can see the agent is repeatedly looping back on itself. As the
agent learns, the steps it takes changes.</p>
<p>The agent also prints out the rewards table.</p>
<blockquote>
<p>&quot;(0, 0): &lsquo;Action&rsquo;: 0, &lsquo;Reward&rsquo;: -0.075, &lsquo;State&rsquo;: (0, 0), (1, 0):
&lsquo;Action&rsquo;: 0, &lsquo;Reward&rsquo;: -0.075, &lsquo;State&rsquo;: (1, 0), (2, 0): &lsquo;Action&rsquo;: 0,
&lsquo;Reward&rsquo;: -0.075, &lsquo;State&rsquo;: (2, 0)&quot;</p>
</blockquote>
<p>As the agent runs, the rewards table and taken steps change. Students
have been able to follow the agent as it learns.</p>
<p>In the exercises and by default, the epochs of the agent are set very
low so the students could observe the agent changing in finer detail.
The program also, by default, sleeps for 1 second between every action.
This enabled the students to read the print statements and figure out
where the agent is going.</p>
<h3 id="globe-maze-vs-flat-maze">Globe Maze vs Flat Maze<a hidden class="anchor" aria-hidden="true" href="#globe-maze-vs-flat-maze">#</a></h3>
<p>In some other programs replicating a maze solving algorithm [@5967320],
the maze was built like a globe. That is, if the agent travelled north
they would eventually end up on the southern side of the maze.</p>
<p>Whereas in others, a flat maze was used [@Amini]. The agent can&rsquo;t end up
on the other side of the map by travelling outside of the border.</p>
<p>It was chosen to use a flat maze to represent the maze. This is because
a 3-dimensional global maze represented in 2-dimensions might be too
confusing for a student to worry about. The project was designed to help
students understand reinforcement learning, not to help them understand
how to transpose a 3-dimensional globe into a 2-dimensional flat space.</p>
<p>In some other mazes, the agent receives a harsh negative reward for
trying to visit outside of the square. Logically, the author does not
believe that a student should have to worry about the agent attempting
to travel outside of the maze. To humans, the idea of walking backwards
out of a maze and finding the end by walking around the maze, instead of
through it would be cheating.</p>
<p>While other creators decided to negatively punish the agent, the author
deemed it to too much for the agent to attempt to even travel outside
the maze. After all, the students in the classrooms themselves wouldn&rsquo;t
travel outside of a maze to try and beat it. Therefore, in
getLegalMoves() it is impossible for the agent to even consider
travelling outside of the maze.</p>
<h3 id="the-legal-move-set">The Legal Move Set<a hidden class="anchor" aria-hidden="true" href="#the-legal-move-set">#</a></h3>
<p>Another problem the creator faced was &quot;what moves should the agent be
allowed to perform?&quot;.</p>
<p>One option would be to allow the agent to move Left, Right, Up, and
Down. Another would be to allow diagonals such as North-West,
South-East, South-West, North-East.</p>
<p>Another option would be to disallow any upward movements, as the goal
tended to be downwards. However, at the time the creator decided this
didn&rsquo;t make sense. What if the goal was moved? Similarly, what if we
banned left movements but the goal was on the left-hand side but there
was a wall right below the agent? Therefore, the creator stuck to the
traditional up, down, left, and right movements.</p>
<p>When deciding on diagonal movements or not, the creator had to take into
account the simplicity of the program. The creator wanted to make the
code as simple as possible to allow the students to focus on the
reinforcement learning aspect of the project.</p>
<p>If the creator added in diagonal movements, the vector addition would
have to change. One cannot simply add one vector to another to get a
diagonal movement, the program would have to add at least 3 vectors to
get to the right spot. Overcomplicating the code meaning it is harder
for students to learn from.</p>
<p>In the end, the creator chose to use the normal move set of up, down,
left, and right.</p>
<h2 id="creation-of-the-learning-materials">Creation of the learning materials<a hidden class="anchor" aria-hidden="true" href="#creation-of-the-learning-materials">#</a></h2>
<p>The project was not intended to be 100% code-based. More so it was
intended to create learning material to teach reinforcement learning to
students that use code as a means to teach.</p>
<p>For this reason, the majority of the project sits in designing an easy
to understand code interface for students, as well as lessons that teach
students reinforcement learning.</p>
<h3 id="the-learning-materials">The Learning Materials<a hidden class="anchor" aria-hidden="true" href="#the-learning-materials">#</a></h3>
<p>Here is each part of the lesson that was created. It features a small
explanation, along with a task. It was expected that the teacher would
give a short presentation on the task, as well as setting everyone up to
use the code. It was estimated to take 15 - 25 minutes to get everyone
to use the code, and 10 - 20 minutes to explain what&rsquo;s happening and why
it&rsquo;s happening. Hence why the tasks are short. Also, the creator of the
project was tasked with creating a program with some example tasks to
teach reinforcement learning, not lesson plans themselves.</p>
<h2 id="the-lesson">The Lesson<a hidden class="anchor" aria-hidden="true" href="#the-lesson">#</a></h2>
<h3 id="what-is-reinforcement-learning">What is Reinforcement Learning?<a hidden class="anchor" aria-hidden="true" href="#what-is-reinforcement-learning">#</a></h3>
<p>Reinforcement learning is similar to how we humans learn. When we want
to learn how to open a jar, we repeatedly try to open it in many
different ways. First clockwise, then anti-clockwise, maybe we&rsquo;ll use a
tea towel or our shirts to get a better grip. Eventually, after trial
and error, we know how to open a jar.</p>
<p>Reinforcement learning is the same. The machine repeatedly tries one
thing after another until it gets it right.</p>
<p>For example, if we have a maze where the machine has to go from one side
to the other, the machine will bump and get lost millions of times
before it eventually learns how to complete the maze and even the
fastest way through the maze.</p>
<h3 id="what-is-an-agent">What is an Agent?<a hidden class="anchor" aria-hidden="true" href="#what-is-an-agent">#</a></h3>
<p>The agent is the little person that explores the maze! We traditionally
call them agents, because we tell them to accomplish something and they
do it. And also, universities prefer agents over &quot;tiny computerised
humans&quot;.</p>
<h3 id="what-is-a-rewards-table">What is a Rewards Table?<a hidden class="anchor" aria-hidden="true" href="#what-is-a-rewards-table">#</a></h3>
<p>The way agents learn is that they store all the information they gather
in a table. Specifically, the information gained from an agent is:</p>
<ul>
<li>
<p>Where did the agent start?</p>
</li>
<li>
<p>What is the new location of the agent?</p>
</li>
<li>
<p>What is the reward of moving this way?</p>
</li>
</ul>
<p>Take, for instance, the agent is at the start at (0, 0). If the agent
moves right, that might give a reward of 0.2. If the agent moves down,
it might give a reward of 0.5. The agent now knows the best move is to
move down.</p>
<p>The agent has to write these things down. As humans, we also use memory
to remember things. &quot;This door opens inwards&quot;, &quot;to unlock the front
door you have to push the lockdown and the door handle at the same
time&quot; and so on. The agent has a memory, just like us.</p>
<h3 id="task-1">Task 1<a hidden class="anchor" aria-hidden="true" href="#task-1">#</a></h3>
<p>We&rsquo;re going to change how fast the agent learns, and we&rsquo;re going to
explore what this will result in.</p>
<p>The learning rate is the speed at which the agent learns. Set it to 1.0,
and the agent learns extremely fast. Set it to 0, and the agent doesn&rsquo;t
learn at all.</p>
<p>The learning rate can only be between 0 and 1, think of increments like
0.25, 0.7831 and so on.</p>
<p>However, simply increasing it to 1 does not mean the agent finds the
best route through the maze. Because the agent learns so fast, it might
be rushing through the maze missing all the import steps to finding the
fastest way through it.</p>
<p>But setting it too low such as 0.01 means the agent is very slow, and it
will take a longer time for the program to complete.</p>
<p>Open the file &lsquo;Qagent.py&rsquo;, and find this part in the file (it&rsquo;s on line
13):</p>
<blockquote>
<p>&quot;# How fast can the agent learn? self.learningRate = 0.25&quot;</p>
</blockquote>
<p>1. Set the learning rate to 0.5.</p>
<p>Now, run the file &lsquo;maze.py&rsquo; and observe the output. **Note: every time
we want to run the agent, we have to run maze.py**</p>
<p>Look at the path the agent took, and the rewards table. How big is the
path? How small is the rewards table?</p>
<p>Play around with the learning rate. Set it to any number between 0 and
1, and see for yourself how changing the pace of learning the agent&rsquo;s
outputs are different.</p>
<h3 id="task-2">Task 2<a hidden class="anchor" aria-hidden="true" href="#task-2">#</a></h3>
<p>We&rsquo;re going to play around with the &lsquo;self.explore&rsquo; variable now.</p>
<p>The explore variable is a probability between 0 and 1 (where 0.15 is 15</p>
<p>In reinforcement learning, we have a trade-off of exploiting what we
know or trying something different.</p>
<p>Take, for instance, buttering bread.</p>
<p>If we butter bread like a circle, going clockwise around the bread it&rsquo;s
not very efficient. But if we never explore, we would never find the
most efficient way to butter the bread.</p>
<p>If the agent only does what it knows, it will never find the most
efficient route through the maze.</p>
<p>The exploratory variable means that every move the agent takes, it has a
15% chance to randomly select a move, regardless of whether or not it
understands how rewarding the move is.</p>
<p>Find the exploratory variable at line 37 of Qagent.</p>
<blockquote>
<p>&quot;The probability that the agent will explore on that round, instead
of exploiting the rewards table self.explore = 0.15&quot;</p>
</blockquote>
<p>1. Set the exploratory percentage to 100 (&lsquo;self.explore = 1&rsquo;) See how
the agent has lost its memory? It has a memory, but it doesn&rsquo;t use its
memories! Every move it makes is random.</p>
<p>What if we set explore to 0? Will the agent find the best route through
the maze or not?</p>
<h3 id="task-3">Task 3<a hidden class="anchor" aria-hidden="true" href="#task-3">#</a></h3>
<p>Now we&rsquo;re going to understand the randomness aspect of exploratory.
Because the agent explores, that means every time the agent attempts the
maze will be different from the last time. Run your programs and have a
look at the person next to you. See how they&rsquo;re different routes?
Different rewards? One of your agents is smarter than the other!</p>
<p>But, you might see a problem. There is only 1 route which is the fastest
way through the maze. But if there are 1 route and every agent is
slightly different because of the random explorations, how does it find
the best route?</p>
<p>The answer is using epochs.</p>
<p>Epoch is a fancy word for &quot;time&quot;. Specifically, we might tell the
agent: &gt; &quot;Do this maze 10 billion times&quot; If we make 2 agents do the
maze 2 or 3 times, their answers will be completely different. But if we
make the agents do the maze 10 billion times, they will converge on the
correct answer - the fastest route through the maze!</p>
<p>Set the epochs variable to a high number, such as &lsquo;10000&rsquo;. However! Your
screen might go crazy, so try not to read every single line that comes
out of the program.</p>
<p>You can find epochs on line 34 of Qagent.</p>
<blockquote>
<p>&quot;How many times will the agent complete the maze before it stops?
self.maxepochs = 1&quot;</p>
</blockquote>
<p>Note: You will need to set maxepochs back to 1 for the rest of this
tutorial.</p>
<h3 id="task-4">Task 4<a hidden class="anchor" aria-hidden="true" href="#task-4">#</a></h3>
<p>Let&rsquo;s talk about gamma!</p>
<p>Gamma is the value of the future reward. If Gamma is set to 1, the agent
sees reaching the goal in 10 moves and reaching the goal in 1 move as
the same value.</p>
<p>If gamma is set to 0, the agent values direct moves (what it&rsquo;s doing on
the next turn) more than it does in the future.</p>
<p>When we humans try to solve a maze, we know where the exit is and we try
to tend towards the exit. If the agent&rsquo;s gamma was 0, the agent wouldn&rsquo;t
tend towards the exit. The agent would simply try to collect as much
reward as possible until it accidentally stumbles upon the exit.</p>
<p>Find gamma on line 31 of Qagent:</p>
<blockquote>
<p>&quot;How much the agent expects to gain from future value. self.gamma =
0.5&quot;</p>
</blockquote>
<p>1. Set the gamma rate to 1. 2. Set the gamma rate to 0.</p>
<p>See how they differ?</p>
<h3 id="task-5">Task 5<a hidden class="anchor" aria-hidden="true" href="#task-5">#</a></h3>
<p>Now, we&rsquo;re going to work on punishing the agent. To prevent the agent
from wandering around in circles, the agent takes a punishment everytime
it moves.</p>
<p>If the punishment is too much, the agent gives up and we get a new agent
to try (but with the same rewards table as the last agent).</p>
<p>Increasing the punishment means the agent gives up faster, but also
means the agent is more encouraged to find the goal faster.</p>
<p>Too low of a punishment and the agent will think &quot;why bother?&quot; and
will reach the goal in a much slower method.</p>
<p>Find this on line 43 of Qagent:</p>
<blockquote>
<p>&quot;The agents penalty for moving. Prevents the agent from running
around in circles self.penaltyMoving = -0.05&quot;</p>
</blockquote>
<p>1. Set the penalty to -1. 2. Set the penalty to 0.</p>
<p>Observe the outputs.</p>
<h3 id="task-6">Task 6<a hidden class="anchor" aria-hidden="true" href="#task-6">#</a></h3>
<p>This is a more advanced task, so be prepared!</p>
<p>The maze the agent uses is located in &lsquo;maze.py&rsquo; (line 12), specifically
this is it:</p>
<blockquote>
<p>&quot;self.maze = np.array([ [ 1, 0, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1,
1, 1, 1, 0, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 0, 1, 1, 1, 1], [ 0, 0,
1, 0, 0, 1, 0, 1, 1, 1], [ 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [ 1, 1,
0, 1, 0, 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [ 1, 1,
1, 1, 1, 1, 0, 0, 0, 0], [ 1, 0, 0, 0, 0, 0, 1, 1, 1, 1], [ 1, 1,
1, 1, 1, 1, 1, 0, 1, 1] ])&quot;</p>
</blockquote>
<p>The 1 represents openness, a straight corridor for the agent. The 0&rsquo;s
represent a wall, something the agent can not pass.</p>
<p>What if we make the goal impossible to reach? The goal is in the bottom
right-hand corner.</p>
<p>1. Change some of the 1&rsquo;s to 0&rsquo;s (walls) to prevent the agent from
reaching the goal. What will happen then?</p>
<p>Play around with changing the layout of the maze and see how the agent
reacts.</p>
<h3 id="task-7">Task 7<a hidden class="anchor" aria-hidden="true" href="#task-7">#</a></h3>
<p>Now we&rsquo;re going to change what moves the agent knows.</p>
<p>On line 28 of &lsquo;Qagent.py&rsquo; the agent defines the moveeset as:</p>
<blockquote>
<p>&quot;self.possibleActions = &quot;Up&quot;: (1, 0), &quot;Down&quot;: (-1, 0), &quot;Left&quot;:
(0, -1), &quot;Right&quot;: (0, 1)&quot;</p>
</blockquote>
<p>The agent can move up, down, left, or right.</p>
<p>It does this using vector addition. On a 2 dimensional map, adding (1,
0) to our current coordinates (15, 15) means we would go up 1 square to
(16, 15).</p>
<p>1. Try to remove some moves. You can do this by deleting the entire
dictionary entry. For example, to reduce Up do this:</p>
<blockquote>
<p>&quot;self.possibleActions = &quot;Down&quot;: (-1, 0), &quot;Left&quot;: (0, -1),
&quot;Right&quot;: (0, 1)&quot;</p>
</blockquote>
<p>Don&rsquo;t forget to remove the comma!</p>
<h3 id="task-8">Task 8<a hidden class="anchor" aria-hidden="true" href="#task-8">#</a></h3>
<p>Now we&rsquo;re going to change the agents start location. Currently, it
starts at the top left (0, 0). But it can start anywhere!</p>
<p>The way the coordinates system works is (row, column). &quot;Along the
corridor, up the stairs&quot;.</p>
<p>Find the code on line 26 of &lsquo;maze.py&rsquo;:</p>
<blockquote>
<p>&quot;self.agentLocation = (0, 0)&quot;</p>
</blockquote>
<p>1. Try setting the agent location to (5, 5) and seeing what the agent
does! Does it walk to the top left again? Or go straight to the goal.</p>
<h3 id="task-9">Task 9<a hidden class="anchor" aria-hidden="true" href="#task-9">#</a></h3>
<p>Change the goal.</p>
<p>The goal variable is located at line 21 of &lsquo;Qagent.py&rsquo;.</p>
<blockquote>
<p>&quot;self.goal = self.maze.getSizeOfMaze()&quot;</p>
</blockquote>
<p>1. Change the goal to a location somewhere on the maze. For example,
(15, 15) like so:</p>
<blockquote>
<p>&quot;self.goal = (15, 15)&quot;</p>
</blockquote>
<p>2. If you set the goal and start location to the same place, what
happens?</p>
<h3 id="task-10">Task 10<a hidden class="anchor" aria-hidden="true" href="#task-10">#</a></h3>
<p>This is an advanced task. 1. Change how the agent learns</p>
<p>Some things you may want to do: * Remove the penalty completely (by
setting it to 0.0) * Remove the agent giving up when it is punished too
much * Play with the learning rate * Play with the gamma rate</p>
<p>Try to find the most optimal settings which result in the most optimal
agent. Often, we would program a computer to automatically find the most
optimal settings. But it is possible to deduct and use logic to work out
roughly what the most optimal settings are.</p>
<p>Google things you don&rsquo;t understand such as &quot;learning rate&quot; or &quot;gamma
q learning&quot; to get the answers you&rsquo;re looking for.</p>
<h1 id="testing-and-evaluation">Testing and Evaluation<a hidden class="anchor" aria-hidden="true" href="#testing-and-evaluation">#</a></h1>
<p>Throughout the project, the creator used many means of testing to ensure
the program was of high quality. Furthermore, the creator regularly
asked for feedback to enhance the project.</p>
<h2 id="continuous-integration">Continuous Integration<a hidden class="anchor" aria-hidden="true" href="#continuous-integration">#</a></h2>
<p>Every time the code is committed to the master branch on GitHub, a
GitHub action automatically tests the code. Firstly, for syntax
compliance. Secondly, for unit testing. The syntax testing used was
Black, and the testing module used was Pytest.</p>
<p>python name: Python application</p>
<p>on: push: branches: [ master ] pullrequest: branches: [ master ]</p>
<p>jobs: build:</p>
<p>runs-on: ubuntu-latest</p>
<p>steps: - uses: actions/checkout@v2 - name: Set up Python 3.8 uses:
actions/setup-python@v1 with: python-version: 3.8 - name: Install
dependencies run: | python -m pip install &ndash;upgrade pip pip install
black pytest if [ -f requirements.txt ]; then pip install -r
requirements.txt; fi - name: Lint with black run: | # stop the build
if there are Python # syntax errors or undefined names black . &ndash;count
&ndash;select=E9,F63,F7,F82 &ndash;show-source &ndash;statistics # exit-zero treats
all errors as warnings. The GitHub editor is 127 chars wide black .
&ndash;count &ndash;exit-zero &ndash;max-complexity=10 &ndash;max-line-length=127
&ndash;statistics - name: Test with pytest run: | pytest</p>
<p>More specifically, the GitHub action in figure 15 was executed.</p>
<h2 id="unit-tests">Unit Tests<a hidden class="anchor" aria-hidden="true" href="#unit-tests">#</a></h2>
<p>Because the project was based on reinforcement learning, the exact maze,
outputs, rewards, and other variables will differ for every time the
program was run. Thus, there are little unittests involved other than to
make such the Q learning algorithm operates correctly and the program
doesn&rsquo;t break under pressure.</p>
<p>The testing software used was PyTest, rather than the standard library
UnitTest. This was because:</p>
<ul>
<li>
<p>PyTest required less code to create tests.</p>
</li>
<li>
<p>PyTest provided more information on the errors.</p>
</li>
</ul>
<p>Although, the biggest benefit to PyTest was that it was the default
testing suite in GitHub Actions.</p>
<p>The tests were written in the style of boundary testing. Where a test is
written for one extreme, and then the other. For instance, the Q
Learning Algorithm tests were written using negative numbers, extremely
large numbers, and the number 0. See figure 16 for some of the tests
used.</p>
<p>python def test-q-algorithm-normal(): &quot;&quot;&quot; Testing to see if the q
algorithm operates correctly &quot;&quot;&quot; q = Qagent.agent(m) result =
q.qAlgorithm(0.54, 1, 0.67) assert result == 0.835</p>
<p>def test-q-algorithm-negative(): &quot;&quot;&quot; Testing to see if it breaks on
negative &quot;&quot;&quot; q = Qagent.agent(m) result = q.qAlgorithm(-50000, 60,
0.9999999) assert result &gt; 1</p>
<p>def test-q-algorithm-big(): &quot;&quot;&quot; Testing to see if it breaks on large
numbers &quot;&quot;&quot; q = Qagent.agent(m) result =
q.qAlgorithm(5000000000000000000 00000000000000000000000000000000,
5000000000000000 00000000000000000000000000000000000, 50000000000
0000000000000000000000000000000000000000) assert result &gt; 1</p>
<p>def test-q-algorithm-zero(): &quot;&quot;&quot; Testing to see if it breaks on 0
input &quot;&quot;&quot; q = Qagent.agent(m) result = q.qAlgorithm(0, 0, 0) assert
result == 0.0</p>
<p>At the time this test was run, PyTest outputted this:</p>
<p>python .&hellip; 4 passed in 0.09s</p>
<p>Where the four full stops represent each test as seen above passing.</p>
<p>More tests were written for the maze program. The maze class was
responsible for creating the maze and deciding on what moves are legal.
See figure 18 for the maze program&rsquo;s tests.</p>
<p>python def test-maze-movement(): &quot;&quot;&quot; Testing the movement of the
agent in the maze&quot;&quot;&quot; m = maze.maze() # move to the right starting at
0,0 result = m.vectorAddition((0, 1), m.getAgentLocation())</p>
<p>assert result == (0, 1)</p>
<p>def test-maze-movement-up(): &quot;&quot;&quot; Testing the movement of the agent in
the maze&quot;&quot;&quot; m = maze.maze() # move to the right starting at 0,0
result = m.vectorAddition((1, 0), m.getAgentLocation())</p>
<p>assert result == (1, 0)</p>
<p>def test-maze-movement-legal-moves(): &quot;&quot;&quot; Does legal moves
work?&quot;&quot;&quot; m = maze.maze() # move to the right starting at 0,0 result
= m.getLegalMoves()</p>
<p>assert True if &quot;Right&quot; in result else False</p>
<p>python test-maze.py ... test-program.py .&hellip; 7 passed in 0.10s</p>
<p>All the tests ran successfully.</p>
<h3 id="continuous-manual-testing">Continuous Manual Testing<a hidden class="anchor" aria-hidden="true" href="#continuous-manual-testing">#</a></h3>
<p>After a new function was written, that function would be copied to
Python&rsquo;s IDLE, where it was evaluated. Take the function of element-wise
vector addition. After the function was written, it was copied and
pasted into a terminal and manually tested.</p>
<p>The function did not rely on any external libraries or code, it merely
added 2 vectors element-wise and returned it. As such, the chance of
this function containing future errors after testing was none, unless
Python&rsquo;s default operators changed drastically, which was unlikely in
the near future.</p>
<p>Other functions were also the same, where it was unfeasible to imagine
that in the future they would break. Thus, no unit tests were written
for these functions.</p>
<p>After each function was added and integrated into the core code, the
code was run. The programmer evaluated the code, making sure that the
integration of the new function did not break any important parts of the
code.</p>
<p>If the code did break, no major harm came to the already working code.
The programmers utilised branches on GitHub, creating a new branch for
each change to the code. This meant that the master branch was always
fully tested and worked, whereas the other branches could be broken.</p>
<p>Once the programmer committed code to the master GitHub branch, the code
was evaluated using the GitHub action. If GitHub failed that testing, it
would email the creator to let them know.</p>
<h2 id="evaluation">Evaluation<a hidden class="anchor" aria-hidden="true" href="#evaluation">#</a></h2>
<p>Earlier in this projects life cycle, it had been planned that the
project would be demonstrated to teachers in classrooms or even used in
classes, and the creator would use this feedback to better the product.</p>
<p>However, due to COVID-19, the creator couldn&rsquo;t enter a classroom.</p>
<p>Therefore, the creator opted to ask other people who work with children
(who are around the same age as year 7&rsquo;s to year 11&rsquo;s).</p>
<p>The creator sent the program and the learning materials to these people.
However, most of them weren&rsquo;t in ICT and didn&rsquo;t understand Python, or
artificial intelligence.</p>
<p>One clear comment was that if the teacher had decided to google &quot;gamma
reinforcement learning&quot;, the explanation given is that which would
require a university degree to decipher. Because of this feedback, the
creator tried to create a simpler explanation of some of the core
concepts. So not only students can understand, but teaching staff may
understand as well.</p>
<p>Throughout this project, the creator spoke regularly with Dr. Dennis and
Dr. Thomason, who were in the outreach program at the University of
Liverpool for Computer Science.</p>
<p>Dr. Thomason&rsquo;s most critical comment was that the project was too
ambitious for a classroom. They stated that in a class, the creator can
expect up to 20 minutes of solely installing the program, and without
clear guidance, the students were likely not to learn. Thus it was
needed for learning material to be created alongside, which specifies
exactly what the students should do.</p>
<p>Another comment made by Dr. Dennis was that the students are all at
different levels. It may be a GCSE Computer Science class, but some of
the students will speed through the resources while others might spend
the entire lesson on the first task. It proposed an age-old question,
&quot;what do we do about the difference in students&rsquo; abilities?&quot;.</p>
<p>The answer was to create many tasks, progressively getting harder with
each task. The idea is that the first few tasks are easy, and can be
used to encourage the slower students to get through them. While the
last tasks are very difficult, which given the limited amount of time in
a classroom should prove worthwhile to the faster students.</p>
<p>Another comment made that impacted the work of the creator was that
reinforcement learning was simply too hard. The original idea was to
teach students how to design an autonomous driving vehicle. Dr. Thomason
told the creator that what may seem easy to a university student isn&rsquo;t
easy to a secondary school student. For this reason, the simplest task
of them all is changing a variable and seeing how it affects the agent&rsquo;s
run.</p>
<p>Overall, the creator would have liked to get the software in front of a
classroom, to ask teachers directly and see how the students learn from
it. But due to COVID-19, it simply wasn&rsquo;t possible.</p>
<h1 id="bcs-project-criteria">BCS Project Criteria<a hidden class="anchor" aria-hidden="true" href="#bcs-project-criteria">#</a></h1>
<h2 id="practical-and-analytical-skills">Practical and Analytical Skills<a hidden class="anchor" aria-hidden="true" href="#practical-and-analytical-skills">#</a></h2>
<p>This relates to BCS criteria:</p>
<blockquote>
<p>&quot;An ability to apply practical and analytical skills gained during
the degree programme.&quot;</p>
</blockquote>
<p>This project is an AI system designed to be taught in schools. The
creators&rsquo; modules over their degree which relate to this project are:</p>
<ul>
<li>
<p>Artificial Intelligence.</p>
</li>
<li>
<p>Advanced Artificial Intelligence.</p>
</li>
<li>
<p>Communicating Computer Science.</p>
</li>
<li>
<p>Software Engineering.</p>
</li>
<li>
<p>Software Engineering 2.</p>
</li>
</ul>
<p>The skills gained from artificial intelligence were used to create a
reinforcement learning agent. Without those modules, the creators
understanding of artificial intelligence would have acted as an
inhibitor for finishing this project.</p>
<p>The skills gained from communicating computer science were used to
better design effective exercises for the students, as mentioned in the
key literature and background reading.</p>
<p>The skills gained from software engineering helped the creator write
better code, better in the sense that it runs faster but also better in
the sense that users can read it, that it is tested, and it is generally
good code.</p>
<h2 id="innovation-and-creativity">Innovation and creativity<a hidden class="anchor" aria-hidden="true" href="#innovation-and-creativity">#</a></h2>
<p>As Isaacson [@Isaacson] says in his book, The Innovators:</p>
<blockquote>
<p>&quot;innovation comes from being able to stand at the intersection of art
and science.&quot;</p>
</blockquote>
<p>The project is scientific in the sense of creating a reinforcement
learning agent. However, the intersection with art comes from creating a
project to use it to teach key stage 3 students.</p>
<p>On the technical side, the innovation came from creating a Python
implementation of a reinforcement learning agent using basic code,
without relying too heavily on external libraries. In most examples, the
code was written to only be understood by other programmers. Whereas the
code for this project was written to be understood by anyone.</p>
<h2 id="synthesis-of-information">Synthesis of Information<a hidden class="anchor" aria-hidden="true" href="#synthesis-of-information">#</a></h2>
<p>This relates to BCS criteria:</p>
<blockquote>
<p>&quot;Synthesis of information, ideas and practices to provide a quality
solution together with an evaluation of that solution.&quot;</p>
</blockquote>
<p>This project was created by Dr. Dennis to solve a real-world problem.
When the creator took up this project, they read books such as [@Sutton]
as discussed in Key Literature and Background Reading.</p>
<p>The creator took in this information and decided on what were the best
parts to take from each resource. The maze problem from Sutton, the
teaching skills from MIT and COMP335. The creator took all of this
information and combined the best of it into one quality solution.</p>
<p>Then, the creator critically evaluated this project in the Evaluation
section. As well as continually evaluating it throughout the projects
life cycle. The creator regularly sought guidance, asked for help, and
watched as other people attempted to use the project to learn. Using
this information, the creator continually improved the project until the
end of its life cycle.</p>
<h2 id="real-world-need">Real World Need<a hidden class="anchor" aria-hidden="true" href="#real-world-need">#</a></h2>
<p>This relates to BCS criteria:</p>
<blockquote>
<p>&quot;That your project meets a real need in a wider context&quot;</p>
</blockquote>
<p>Dr. Dennis, the creator&rsquo;s supervisor, created this project because of a
gap in the market. More specifically, computer science taught in
classrooms wasn&rsquo;t fun, or approximately close to what computer
scientists were working on at the time. Sorting algorithms such as
bubble sort were taught for multiple lessons, as an example.</p>
<p>Dr. Dennis decided that reinforcement learning would be fun, as it is a
current subject in artificial intelligence still being worked on, and
can be explained easily to a younger student using analogies such as how
that student learns.</p>
<h2 id="self-manage-a-significant-piece-of-work">Self-Manage a Significant Piece of Work<a hidden class="anchor" aria-hidden="true" href="#self-manage-a-significant-piece-of-work">#</a></h2>
<p>This relates to the BCS criteria:</p>
<blockquote>
<p>&quot;An ability to self-manage a significant piece of work.&quot;</p>
</blockquote>
<p>Throughout this project, the creator regularly stuck to the Gantt chart
to monitor how well they were doing and the time schedule of code. As
well as the Gantt chart the creator planned for emergencies, any issues
that might arise and took care of them accordingly.</p>
<p>This dissertation is one example of many that the creator can
self-manage a significant piece of work.</p>
<h2 id="critical-self-evaluation-of-the-process">Critical Self-Evaluation of the Process<a hidden class="anchor" aria-hidden="true" href="#critical-self-evaluation-of-the-process">#</a></h2>
<p>At the start of the project, the creator went to work quickly. Having
finished a prototype before the first month finished. The creator asked
Dr. Thomason for support in creating the educational texts and even took
up online classes to better learn how to teach this subject to the
students.</p>
<p>The creator followed the Gantt chart originally made, and all was set to
be well.</p>
<p>Unfortunately, the creator suffered an illness in December that went on
until late February or early March. During this time, the creator
carried on coding but could not communicate effectively with their
supervisor or the university.</p>
<p>Shortly after the creator became well again, COVID-19 stormed the world.
Causing panic in the creator, and worry about what&rsquo;s going to happen.</p>
<p>Unfortunately, the creator did not succeed in all the original tasks of
the dissertation. One of the original tasks was to implement this
reinforcement learning agent into a small robot. This task failed,
mainly due to the illness but partly due to COVID-19 not allowing the
creator to physically pick up the small robot to try and implement it.</p>
<p>In the future, the creator will take extra care and will try to assume
the worst - including a potential global pandemic. If the creator had
been in a more communicative state during their December - February
illness, they might have just avoided COVID-19 and may have been able to
implement the agent into the small robot.</p>
<p>The creator, unfortunately, felt like they didn&rsquo;t need to tell anyone
what was wrong with them, which led to their supervisor and university
becoming worried and asking if they were okay. The creator did not want
to admit their own defeats, so they refused to communicate with the
university from December to February.</p>
<p>The dissertation portion of the final year project was also delayed. The
creator felt like they could create more tasks, more agents that could
help aid the students. The creator felt bad that they couldn&rsquo;t complete
the small autonomous robot task, as it was impossible to pick up during
COVID-19. Therefore, the creator, instead of spending time working on
the dissertation, spent most of their time trying to create more tasks
and improving the agents.</p>
<p>What would have been a better choice would be to discuss with their
supervisor what the best use of their time could be.</p>
<p>Overall, the creator has learnt that to self-manage such a large project
means that the creator should regularly communicate with those
supporting them. Without that support in December - February, the
creator was lost and this led to the failure of the mini robot car task.</p>
<p>The creator should have put their own health first, and then the
university studies second. This would have no doubt caused the illness
the creator to suffer from to have been reduced, and it would have let
the creator continue to come back to work much sooner. The creator was
scared that their supervisor wouldn&rsquo;t allow them to look after
themselves and they would have been forced to work full speed on their
project. When in actual fact, looking at what has happened it seems more
likely that the creator would have done more work and gotten back on
track much quicker if they had been honest and open with those actively
supporting the creator.</p>
<p>It would have been nice for the creator to show the agent moving through
the maze on the map, clearly. However, this caused issues at higher
epochs and it was hard to see the agent moving through the maze amongst
the 0&rsquo;s and 1&rsquo;s, again the creator focused too much on the design and
creation of the project rather than spending it on working what matters
most to the project.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>Although this project failed in one of the more major tasks, it was a
success in that the creator still managed to fulfil the original goal of
creating a reinforcement learning system that can teach key stage 3
students about artificial intelligence and encourage them.</p>
<p>There would always have been something extra to have done or something
new and exciting to add to the project. Merely finishing the project on
time was a blessing for the creator, and it is exciting to know that the
project will potentially be used to excite the next generation of
computer scientists.</p>
<p>Looking back at the introduction, these were the tasks assigned to the
creator:</p>
<ul>
<li>
<p>Create an ASCII based grid-world reinforcement learning playground.</p>
</li>
<li>
<p>Implement a reinforcement learning framework in an existing
Python-based robot simulator.</p>
</li>
<li>
<p>Design an API for reinforcement learning.</p>
</li>
<li>
<p>Devise a set of challenge problems to be used with the simulator.</p>
</li>
<li>
<p>Design the learning exercises.</p>
</li>
<li>
<p>Gather feedback about the learning exercises and improve upon it.</p>
</li>
<li>
<p>Create an autonomously driving robot vehicle agent.</p>
</li>
</ul>
<p>The creator did make a grid-world reinforcement learning playground, in
maze.py. The creator did design an API to be used in IDLE, they did
devise a set of problems to be used with the simulator as seen in the
exercises section. The creator gathered valuable feedback from the
exercises and used that feedback to improve upon the exercises.</p>
<p>Overall, the creator succeeds in 6 out of the 7 tasks, which is positive
as most software projects tend to fail or over-run.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://polymath.cloud/tags/artificial-intelligence/">Artificial Intelligence</a></li>
    </ul>






<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on twitter"
        href="https://twitter.com/intent/tweet/?text=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots&amp;url=https%3a%2f%2fpolymath.cloud%2fdissertation%2f&amp;hashtags=ArtificialIntelligence">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpolymath.cloud%2fdissertation%2f&amp;title=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots&amp;summary=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots&amp;source=https%3a%2f%2fpolymath.cloud%2fdissertation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpolymath.cloud%2fdissertation%2f&title=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpolymath.cloud%2fdissertation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on whatsapp"
        href="https://api.whatsapp.com/send?text=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots%20-%20https%3a%2f%2fpolymath.cloud%2fdissertation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Creating a Framework to Teach Key Stage 3 Students Reinforcement Learning with Autonomous (Simulated) Robots on telegram"
        href="https://telegram.me/share/url?text=Creating%20a%20Framework%20to%20Teach%20Key%20Stage%203%20Students%20Reinforcement%20Learning%20with%20Autonomous%20%28Simulated%29%20Robots&amp;url=https%3a%2f%2fpolymath.cloud%2fdissertation%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>

  
  <br>
  <script src="https://f.convertkit.com/ckjs/ck.5.js"></script>
  <form action="https://app.convertkit.com/forms/1896338/subscriptions" class="seva-form formkit-form" method="post" data-sv-form="1896338" data-uid="9d5f55b5a0" data-format="inline" data-version="5" data-options="{&quot;settings&quot;:{&quot;after_subscribe&quot;:{&quot;action&quot;:&quot;message&quot;,&quot;success_message&quot;:&quot;Success! Now check your email to confirm your subscription.&quot;,&quot;redirect_url&quot;:&quot;&quot;},&quot;analytics&quot;:{&quot;google&quot;:null,&quot;facebook&quot;:null,&quot;segment&quot;:null,&quot;pinterest&quot;:null,&quot;sparkloop&quot;:null,&quot;googletagmanager&quot;:null},&quot;modal&quot;:{&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;powered_by&quot;:{&quot;show&quot;:true,&quot;url&quot;:&quot;https://convertkit.com?utm_source=dynamic&amp;utm_medium=referral&amp;utm_campaign=poweredby&amp;utm_content=form&quot;},&quot;recaptcha&quot;:{&quot;enabled&quot;:false},&quot;return_visitor&quot;:{&quot;action&quot;:&quot;show&quot;,&quot;custom_content&quot;:&quot;&quot;},&quot;slide_in&quot;:{&quot;display_in&quot;:&quot;bottom_right&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;sticky_bar&quot;:{&quot;display_in&quot;:&quot;top&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15}},&quot;version&quot;:&quot;5&quot;}" min-width="400 500 600 700 800" style="background-color: rgb(249, 250, 251); border-radius: 4px;"><div class="formkit-background" style="opacity: 0.2;"></div><div data-style="minimal"><div class="formkit-header" data-element="header" style="color: rgb(77, 77, 77); font-size: 27px; font-weight: 700;"><h1>Join the Newsletter</h1></div><div class="formkit-subheader" data-element="subheader" style="color: rgb(104, 104, 104); font-size: 18px;">Subscribe to get our latest content by email.</div><ul class="formkit-alert formkit-alert-error" data-element="errors" data-group="alert"></ul><div data-element="fields" data-stacked="false" class="seva-fields formkit-fields"><div class="formkit-field"><input class="formkit-input" name="email_address" aria-label="Email Address" placeholder="Email Address" required="" type="email" style="color: rgb(0, 0, 0); border-color: rgb(227, 227, 227); border-radius: 4px; font-weight: 400;"></div><button data-element="submit" class="formkit-submit formkit-submit" style="color: rgb(255, 255, 255); background-color: rgb(22, 119, 190); border-radius: 4px; font-weight: 400;"><div class="formkit-spinner"><div></div><div></div><div></div></div><span class="">Subscribe</span></button></div><div class="formkit-guarantee" data-element="guarantee" style="color: rgb(77, 77, 77); font-size: 13px; font-weight: 400;">We won't send you spam. Unsubscribe at any time.</div><div class="formkit-powered-by-convertkit-container"><a href="https://convertkit.com?utm_source=dynamic&amp;utm_medium=referral&amp;utm_campaign=poweredby&amp;utm_content=form" data-element="powered-by" class="formkit-powered-by-convertkit" data-variant="dark" target="_blank" rel="noopener noreferrer">Built with ConvertKit</a></div></div><style>.formkit-form[data-uid="9d5f55b5a0"] *{box-sizing:border-box;}.formkit-form[data-uid="9d5f55b5a0"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid="9d5f55b5a0"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid="9d5f55b5a0"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid="9d5f55b5a0"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid="9d5f55b5a0"] h1,.formkit-form[data-uid="9d5f55b5a0"] h2,.formkit-form[data-uid="9d5f55b5a0"] h3,.formkit-form[data-uid="9d5f55b5a0"] h4,.formkit-form[data-uid="9d5f55b5a0"] h5,.formkit-form[data-uid="9d5f55b5a0"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="9d5f55b5a0"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="9d5f55b5a0"] ol:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] ul:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid="9d5f55b5a0"] p:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] hr:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] blockquote:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] ol:not([template-default]),.formkit-form[data-uid="9d5f55b5a0"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid="9d5f55b5a0"] .ordered-list,.formkit-form[data-uid="9d5f55b5a0"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid="9d5f55b5a0"] .list-item{padding-left:0;}.formkit-form[data-uid="9d5f55b5a0"][data-format="modal"]{display:none;}.formkit-form[data-uid="9d5f55b5a0"][data-format="slide in"]{display:none;}.formkit-form[data-uid="9d5f55b5a0"][data-format="sticky bar"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid="9d5f55b5a0"][data-format="sticky bar"]{display:block;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input,.formkit-form[data-uid="9d5f55b5a0"] .formkit-select,.formkit-form[data-uid="9d5f55b5a0"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-button,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-button:hover,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit:hover,.formkit-form[data-uid="9d5f55b5a0"] .formkit-button:focus,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-button:hover > span,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit:hover > span,.formkit-form[data-uid="9d5f55b5a0"] .formkit-button:focus > span,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid="9d5f55b5a0"] .formkit-button > span,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="dropdown"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="dropdown"]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="dropdown"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="dropdown"] select:focus{outline:none;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"]{text-align:left;margin:0;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"]{margin-bottom:10px;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] *{cursor:pointer;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]{display:none;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"] + label::after{content:none;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::after{border-color:#ffffff;content:"";}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] label::before,.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] label::after{position:absolute;content:"";display:inline-block;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid="9d5f55b5a0"] [data-group="checkboxes"] [data-group="checkbox"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-alert:empty{display:none;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0px;width:0px;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0px;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-spinner > div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:0.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-9d5f55b5a0- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-9d5f55b5a0- 1.4s infinite ease-in-out both;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-spinner > div:nth-child(1){-webkit-animation-delay:-0.32s;animation-delay:-0.32s;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-spinner > div:nth-child(2){-webkit-animation-delay:-0.16s;animation-delay:-0.16s;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit[data-active] .formkit-spinner ~ span{opacity:0;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;z-index:5;margin:10px 0;position:relative;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit-container[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#ffffff;border:1px solid #dce1e5;border-radius:4px;color:#373f45;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:0.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:190px;background-repeat:no-repeat;background-position:center;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='%23373F45'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='%23373F45'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='%23373F45'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='%23373F45'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='%23373F45'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='%23373F45'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='%23373F45'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='%23373F45'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='%23373F45'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='%23373F45'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='%23373F45'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='%23373F45'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='%23373F45'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='%23373F45'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='%23373F45'/%3E%3C/svg%3E");}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit:focus{background-color:#ffffff;-webkit-transform:scale(1.025) perspective(1px);-ms-transform:scale(1.025) perspective(1px);transform:scale(1.025) perspective(1px);opacity:1;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit[data-variant="dark"],.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit[data-variant="light"]{background-color:transparent;border-color:transparent;width:166px;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit[data-variant="light"]{color:#ffffff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='white'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='white'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='white'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='white'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='white'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='white'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='white'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='white'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='white'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='white'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='white'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='white'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='white'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='white'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='white'/%3E%3C/svg%3E");}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-9d5f55b5a0-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}@keyframes formkit-bouncedelay-formkit-form-data-uid-9d5f55b5a0-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}.formkit-form[data-uid="9d5f55b5a0"] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1;} .formkit-form[data-uid="9d5f55b5a0"]{border:1px solid #e3e3e3;max-width:700px;position:relative;overflow:hidden;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-background{width:100%;height:100%;position:absolute;top:0;left:0;background-size:cover;background-position:center;opacity:0.3;}.formkit-form[data-uid="9d5f55b5a0"] [data-style="minimal"]{padding:20px;width:100%;position:relative;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-header{margin:0 0 27px 0;text-align:center;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-subheader{margin:18px 0;text-align:center;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-guarantee{font-size:13px;margin:10px 0 15px 0;text-align:center;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-guarantee > p{margin:0;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-powered-by-convertkit-container{margin-bottom:0;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:25px auto 0 auto;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-field{min-width:220px;}.formkit-form[data-uid="9d5f55b5a0"] .formkit-field,.formkit-form[data-uid="9d5f55b5a0"] .formkit-submit{margin:0 0 15px 0;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%;}.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] [data-style="minimal"]{padding:40px;}.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] .formkit-fields[data-stacked="false"]{margin-left:-5px;margin-right:-5px;}.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field,.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{margin:0 5px 15px 5px;}.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto;}.formkit-form[data-uid="9d5f55b5a0"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;} </style></form>
  <br>
</article>
    </main><footer class="footer">
    <span>&copy; 2020 <a href="https://polymath.cloud">Polymath.cloud</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>

<script src="https://f.convertkit.com/ckjs/ck.5.js"></script>
      <form action="https://app.convertkit.com/forms/1896317/subscriptions" class="seva-form formkit-form" method="post" data-sv-form="1896317" data-uid="813d4e331a" data-format="slide in" data-version="5" data-options="{&quot;settings&quot;:{&quot;after_subscribe&quot;:{&quot;action&quot;:&quot;message&quot;,&quot;success_message&quot;:&quot;Success! Now check your email to confirm your subscription.&quot;,&quot;redirect_url&quot;:&quot;&quot;},&quot;analytics&quot;:{&quot;google&quot;:null,&quot;facebook&quot;:null,&quot;segment&quot;:null,&quot;pinterest&quot;:null,&quot;sparkloop&quot;:null,&quot;googletagmanager&quot;:null},&quot;modal&quot;:{&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15},&quot;powered_by&quot;:{&quot;show&quot;:true,&quot;url&quot;:&quot;https://convertkit.com?utm_source=dynamic&amp;utm_medium=referral&amp;utm_campaign=poweredby&amp;utm_content=form&quot;},&quot;recaptcha&quot;:{&quot;enabled&quot;:false},&quot;return_visitor&quot;:{&quot;action&quot;:&quot;show&quot;,&quot;custom_content&quot;:&quot;&quot;},&quot;slide_in&quot;:{&quot;display_in&quot;:&quot;bottom_right&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:&quot;0&quot;,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:&quot;0&quot;},&quot;sticky_bar&quot;:{&quot;display_in&quot;:&quot;top&quot;,&quot;trigger&quot;:&quot;timer&quot;,&quot;scroll_percentage&quot;:null,&quot;timer&quot;:5,&quot;devices&quot;:&quot;all&quot;,&quot;show_once_every&quot;:15}},&quot;version&quot;:&quot;5&quot;}" min-width="400 500 600 700 800" style="background-color: rgb(249, 250, 251); border-radius: 4px;"><div class="formkit-background" style="opacity: 0.2;"></div><div data-style="minimal"><div class="formkit-header" data-element="header" style="color: rgb(77, 77, 77); font-size: 27px; font-weight: 700;"><h1>At least this isn't a full screen popup</h1></div><div class="formkit-subheader" data-element="subheader" style="color: rgb(104, 104, 104); font-size: 18px;"><p>Subscribe to get:</p><p><strong>3 free books</strong> on computer science</p><p>Access to my <strong>weekly newsletter</strong> "Computer Science for Devs"</p></div><ul class="formkit-alert formkit-alert-error" data-element="errors" data-group="alert"></ul><div data-element="fields" data-stacked="false" class="seva-fields formkit-fields"><div class="formkit-field"><input class="formkit-input" name="email_address" aria-label="Email Address" placeholder="Email Address" required="" type="email" style="color: rgb(0, 0, 0); border-color: rgb(227, 227, 227); border-radius: 4px; font-weight: 400;"></div><button data-element="submit" class="formkit-submit formkit-submit" style="color: rgb(255, 255, 255); background-color: rgb(22, 119, 190); border-radius: 4px; font-weight: 400;"><div class="formkit-spinner"><div></div><div></div><div></div></div><span class="">Subscribe</span></button></div><div class="formkit-guarantee" data-element="guarantee" style="color: rgb(77, 77, 77); font-size: 13px; font-weight: 400;">We won't send you spam. Unsubscribe at any time.</div><div class="formkit-powered-by-convertkit-container"><a href="https://convertkit.com?utm_source=dynamic&amp;utm_medium=referral&amp;utm_campaign=poweredby&amp;utm_content=form" data-element="powered-by" class="formkit-powered-by-convertkit" data-variant="dark" target="_blank" rel="noopener noreferrer">Built with ConvertKit</a></div></div><style>.formkit-form[data-uid="813d4e331a"] *{box-sizing:border-box;}.formkit-form[data-uid="813d4e331a"]{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}.formkit-form[data-uid="813d4e331a"] legend{border:none;font-size:inherit;margin-bottom:10px;padding:0;position:relative;display:table;}.formkit-form[data-uid="813d4e331a"] fieldset{border:0;padding:0.01em 0 0 0;margin:0;min-width:0;}.formkit-form[data-uid="813d4e331a"] body:not(:-moz-handler-blocked) fieldset{display:table-cell;}.formkit-form[data-uid="813d4e331a"] h1,.formkit-form[data-uid="813d4e331a"] h2,.formkit-form[data-uid="813d4e331a"] h3,.formkit-form[data-uid="813d4e331a"] h4,.formkit-form[data-uid="813d4e331a"] h5,.formkit-form[data-uid="813d4e331a"] h6{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="813d4e331a"] p{color:inherit;font-size:inherit;font-weight:inherit;}.formkit-form[data-uid="813d4e331a"] ol:not([template-default]),.formkit-form[data-uid="813d4e331a"] ul:not([template-default]),.formkit-form[data-uid="813d4e331a"] blockquote:not([template-default]){text-align:left;}.formkit-form[data-uid="813d4e331a"] p:not([template-default]),.formkit-form[data-uid="813d4e331a"] hr:not([template-default]),.formkit-form[data-uid="813d4e331a"] blockquote:not([template-default]),.formkit-form[data-uid="813d4e331a"] ol:not([template-default]),.formkit-form[data-uid="813d4e331a"] ul:not([template-default]){color:inherit;font-style:initial;}.formkit-form[data-uid="813d4e331a"] .ordered-list,.formkit-form[data-uid="813d4e331a"] .unordered-list{list-style-position:outside !important;padding-left:1em;}.formkit-form[data-uid="813d4e331a"] .list-item{padding-left:0;}.formkit-form[data-uid="813d4e331a"][data-format="modal"]{display:none;}.formkit-form[data-uid="813d4e331a"][data-format="slide in"]{display:none;}.formkit-form[data-uid="813d4e331a"][data-format="sticky bar"]{display:none;}.formkit-sticky-bar .formkit-form[data-uid="813d4e331a"][data-format="sticky bar"]{display:block;}.formkit-form[data-uid="813d4e331a"] .formkit-input,.formkit-form[data-uid="813d4e331a"] .formkit-select,.formkit-form[data-uid="813d4e331a"] .formkit-checkboxes{width:100%;}.formkit-form[data-uid="813d4e331a"] .formkit-button,.formkit-form[data-uid="813d4e331a"] .formkit-submit{border:0;border-radius:5px;color:#ffffff;cursor:pointer;display:inline-block;text-align:center;font-size:15px;font-weight:500;cursor:pointer;margin-bottom:15px;overflow:hidden;padding:0;position:relative;vertical-align:middle;}.formkit-form[data-uid="813d4e331a"] .formkit-button:hover,.formkit-form[data-uid="813d4e331a"] .formkit-submit:hover,.formkit-form[data-uid="813d4e331a"] .formkit-button:focus,.formkit-form[data-uid="813d4e331a"] .formkit-submit:focus{outline:none;}.formkit-form[data-uid="813d4e331a"] .formkit-button:hover > span,.formkit-form[data-uid="813d4e331a"] .formkit-submit:hover > span,.formkit-form[data-uid="813d4e331a"] .formkit-button:focus > span,.formkit-form[data-uid="813d4e331a"] .formkit-submit:focus > span{background-color:rgba(0,0,0,0.1);}.formkit-form[data-uid="813d4e331a"] .formkit-button > span,.formkit-form[data-uid="813d4e331a"] .formkit-submit > span{display:block;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;padding:12px 24px;}.formkit-form[data-uid="813d4e331a"] .formkit-input{background:#ffffff;font-size:15px;padding:12px;border:1px solid #e3e3e3;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;line-height:1.4;margin:0;-webkit-transition:border-color ease-out 300ms;transition:border-color ease-out 300ms;}.formkit-form[data-uid="813d4e331a"] .formkit-input:focus{outline:none;border-color:#1677be;-webkit-transition:border-color ease 300ms;transition:border-color ease 300ms;}.formkit-form[data-uid="813d4e331a"] .formkit-input::-webkit-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="813d4e331a"] .formkit-input::-moz-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="813d4e331a"] .formkit-input:-ms-input-placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="813d4e331a"] .formkit-input::placeholder{color:inherit;opacity:0.8;}.formkit-form[data-uid="813d4e331a"] [data-group="dropdown"]{position:relative;display:inline-block;width:100%;}.formkit-form[data-uid="813d4e331a"] [data-group="dropdown"]::before{content:"";top:calc(50% - 2.5px);right:10px;position:absolute;pointer-events:none;border-color:#4f4f4f transparent transparent transparent;border-style:solid;border-width:6px 6px 0 6px;height:0;width:0;z-index:999;}.formkit-form[data-uid="813d4e331a"] [data-group="dropdown"] select{height:auto;width:100%;cursor:pointer;color:#333333;line-height:1.4;margin-bottom:0;padding:0 6px;-webkit-appearance:none;-moz-appearance:none;appearance:none;font-size:15px;padding:12px;padding-right:25px;border:1px solid #e3e3e3;background:#ffffff;}.formkit-form[data-uid="813d4e331a"] [data-group="dropdown"] select:focus{outline:none;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"]{text-align:left;margin:0;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"]{margin-bottom:10px;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] *{cursor:pointer;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"]:last-of-type{margin-bottom:0;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]{display:none;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"] + label::after{content:none;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::after{border-color:#ffffff;content:"";}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] input[type="checkbox"]:checked + label::before{background:#10bf7a;border-color:#10bf7a;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] label{position:relative;display:inline-block;padding-left:28px;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] label::before,.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] label::after{position:absolute;content:"";display:inline-block;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] label::before{height:16px;width:16px;border:1px solid #e3e3e3;background:#ffffff;left:0px;top:3px;}.formkit-form[data-uid="813d4e331a"] [data-group="checkboxes"] [data-group="checkbox"] label::after{height:4px;width:8px;border-left:2px solid #4d4d4d;border-bottom:2px solid #4d4d4d;-webkit-transform:rotate(-45deg);-ms-transform:rotate(-45deg);transform:rotate(-45deg);left:4px;top:8px;}.formkit-form[data-uid="813d4e331a"] .formkit-alert{background:#f9fafb;border:1px solid #e3e3e3;border-radius:5px;-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;list-style:none;margin:25px auto;padding:12px;text-align:center;width:100%;}.formkit-form[data-uid="813d4e331a"] .formkit-alert:empty{display:none;}.formkit-form[data-uid="813d4e331a"] .formkit-alert-success{background:#d3fbeb;border-color:#10bf7a;color:#0c905c;}.formkit-form[data-uid="813d4e331a"] .formkit-alert-error{background:#fde8e2;border-color:#f2643b;color:#ea4110;}.formkit-form[data-uid="813d4e331a"] .formkit-spinner{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:0px;width:0px;margin:0 auto;position:absolute;top:0;left:0;right:0;width:0px;overflow:hidden;text-align:center;-webkit-transition:all 300ms ease-in-out;transition:all 300ms ease-in-out;}.formkit-form[data-uid="813d4e331a"] .formkit-spinner > div{margin:auto;width:12px;height:12px;background-color:#fff;opacity:0.3;border-radius:100%;display:inline-block;-webkit-animation:formkit-bouncedelay-formkit-form-data-uid-813d4e331a- 1.4s infinite ease-in-out both;animation:formkit-bouncedelay-formkit-form-data-uid-813d4e331a- 1.4s infinite ease-in-out both;}.formkit-form[data-uid="813d4e331a"] .formkit-spinner > div:nth-child(1){-webkit-animation-delay:-0.32s;animation-delay:-0.32s;}.formkit-form[data-uid="813d4e331a"] .formkit-spinner > div:nth-child(2){-webkit-animation-delay:-0.16s;animation-delay:-0.16s;}.formkit-form[data-uid="813d4e331a"] .formkit-submit[data-active] .formkit-spinner{opacity:1;height:100%;width:50px;}.formkit-form[data-uid="813d4e331a"] .formkit-submit[data-active] .formkit-spinner ~ span{opacity:0;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit-container{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;width:100%;z-index:5;margin:10px 0;position:relative;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit-container[data-active="false"]{opacity:0.35;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#ffffff;border:1px solid #dce1e5;border-radius:4px;color:#373f45;cursor:pointer;display:block;height:36px;margin:0 auto;opacity:0.95;padding:0;-webkit-text-decoration:none;text-decoration:none;text-indent:100%;-webkit-transition:ease-in-out all 200ms;transition:ease-in-out all 200ms;white-space:nowrap;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:190px;background-repeat:no-repeat;background-position:center;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='%23373F45'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='%23373F45'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='%23373F45'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='%23373F45'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='%23373F45'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='%23373F45'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='%23373F45'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='%23373F45'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='%23373F45'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='%23373F45'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='%23373F45'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='%23373F45'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='%23373F45'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='%23373F45'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='%23373F45'/%3E%3C/svg%3E");}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit:hover,.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit:focus{background-color:#ffffff;-webkit-transform:scale(1.025) perspective(1px);-ms-transform:scale(1.025) perspective(1px);transform:scale(1.025) perspective(1px);opacity:1;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit[data-variant="dark"],.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit[data-variant="light"]{background-color:transparent;border-color:transparent;width:166px;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit[data-variant="light"]{color:#ffffff;background-image:url("data:image/svg+xml;charset=utf8,%3Csvg width='162' height='20' viewBox='0 0 162 20' fill='none' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M83.0561 15.2457C86.675 15.2457 89.4722 12.5154 89.4722 9.14749C89.4722 5.99211 86.8443 4.06563 85.1038 4.06563C82.6801 4.06563 80.7373 5.76407 80.4605 8.28551C80.4092 8.75244 80.0387 9.14403 79.5686 9.14069C78.7871 9.13509 77.6507 9.12841 76.9314 9.13092C76.6217 9.13199 76.3658 8.88106 76.381 8.57196C76.4895 6.38513 77.2218 4.3404 78.618 2.76974C80.1695 1.02445 82.4289 0 85.1038 0C89.5979 0 93.8406 4.07791 93.8406 9.14749C93.8406 14.7608 89.1832 19.3113 83.1517 19.3113C78.8502 19.3113 74.5179 16.5041 73.0053 12.5795C72.9999 12.565 72.9986 12.5492 73.0015 12.534C73.0218 12.4179 73.0617 12.3118 73.1011 12.2074C73.1583 12.0555 73.2143 11.907 73.2062 11.7359L73.18 11.1892C73.174 11.0569 73.2075 10.9258 73.2764 10.8127C73.3452 10.6995 73.4463 10.6094 73.5666 10.554L73.7852 10.4523C73.9077 10.3957 74.0148 10.3105 74.0976 10.204C74.1803 10.0974 74.2363 9.97252 74.2608 9.83983C74.3341 9.43894 74.6865 9.14749 75.0979 9.14749C75.7404 9.14749 76.299 9.57412 76.5088 10.1806C77.5188 13.1 79.1245 15.2457 83.0561 15.2457Z' fill='white'/%3E%3Cpath d='M155.758 6.91365C155.028 6.91365 154.804 6.47916 154.804 5.98857C154.804 5.46997 154.986 5.06348 155.758 5.06348C156.53 5.06348 156.712 5.46997 156.712 5.98857C156.712 6.47905 156.516 6.91365 155.758 6.91365ZM142.441 12.9304V9.32833L141.415 9.32323V8.90392C141.415 8.44719 141.786 8.07758 142.244 8.07986L142.441 8.08095V6.55306L144.082 6.09057V8.08073H145.569V8.50416C145.569 8.61242 145.548 8.71961 145.506 8.81961C145.465 8.91961 145.404 9.01047 145.328 9.08699C145.251 9.16351 145.16 9.2242 145.06 9.26559C144.96 9.30698 144.853 9.32826 144.745 9.32822H144.082V12.7201C144.082 13.2423 144.378 13.4256 144.76 13.4887C145.209 13.5629 145.583 13.888 145.583 14.343V14.9626C144.029 14.9626 142.441 14.8942 142.441 12.9304Z' fill='white'/%3E%3Cpath d='M110.058 7.92554C108.417 7.88344 106.396 8.92062 106.396 11.5137C106.396 14.0646 108.417 15.0738 110.058 15.0318C111.742 15.0738 113.748 14.0646 113.748 11.5137C113.748 8.92062 111.742 7.88344 110.058 7.92554ZM110.07 13.7586C108.878 13.7586 108.032 12.8905 108.032 11.461C108.032 10.1013 108.878 9.20569 110.071 9.20569C111.263 9.20569 112.101 10.0995 112.101 11.459C112.101 12.8887 111.263 13.7586 110.07 13.7586Z' fill='white'/%3E%3Cpath d='M118.06 7.94098C119.491 7.94098 120.978 8.33337 120.978 11.1366V14.893H120.063C119.608 14.893 119.238 14.524 119.238 14.0689V10.9965C119.238 9.66506 118.747 9.16047 117.891 9.16047C117.414 9.16047 116.797 9.52486 116.502 9.81915V14.069C116.502 14.1773 116.481 14.2845 116.44 14.3845C116.398 14.4845 116.337 14.5753 116.261 14.6519C116.184 14.7284 116.093 14.7891 115.993 14.8305C115.893 14.8719 115.786 14.8931 115.678 14.8931H114.847V8.10918H115.773C115.932 8.10914 116.087 8.16315 116.212 8.26242C116.337 8.36168 116.424 8.50033 116.46 8.65577C116.881 8.19328 117.428 7.94098 118.06 7.94098ZM122.854 8.09713C123.024 8.09708 123.19 8.1496 123.329 8.2475C123.468 8.34541 123.574 8.48391 123.631 8.64405L125.133 12.8486L126.635 8.64415C126.692 8.48402 126.798 8.34551 126.937 8.2476C127.076 8.1497 127.242 8.09718 127.412 8.09724H128.598L126.152 14.3567C126.091 14.5112 125.986 14.6439 125.849 14.7374C125.711 14.831 125.549 14.881 125.383 14.8809H124.333L121.668 8.09713H122.854Z' fill='white'/%3E%3Cpath d='M135.085 14.5514C134.566 14.7616 133.513 15.0416 132.418 15.0416C130.496 15.0416 129.024 13.9345 129.024 11.4396C129.024 9.19701 130.451 7.99792 132.191 7.99792C134.338 7.99792 135.254 9.4378 135.158 11.3979C135.139 11.8029 134.786 12.0983 134.38 12.0983H130.679C130.763 13.1916 131.562 13.7662 132.615 13.7662C133.028 13.7662 133.462 13.7452 133.983 13.6481C134.535 13.545 135.085 13.9375 135.085 14.4985V14.5514ZM133.673 10.949C133.785 9.87621 133.061 9.28752 132.191 9.28752C131.321 9.28752 130.734 9.93979 130.679 10.9489L133.673 10.949Z' fill='white'/%3E%3Cpath d='M137.345 8.11122C137.497 8.11118 137.645 8.16229 137.765 8.25635C137.884 8.35041 137.969 8.48197 138.005 8.62993C138.566 8.20932 139.268 7.94303 139.759 7.94303C139.801 7.94303 140.068 7.94303 140.489 7.99913V8.7265C140.489 9.11748 140.15 9.4147 139.759 9.4147C139.31 9.4147 138.651 9.5829 138.131 9.8773V14.8951H136.462V8.11112L137.345 8.11122ZM156.6 14.0508V8.09104H155.769C155.314 8.09104 154.944 8.45999 154.944 8.9151V14.8748H155.775C156.23 14.8748 156.6 14.5058 156.6 14.0508ZM158.857 12.9447V9.34254H157.749V8.91912C157.749 8.46401 158.118 8.09506 158.574 8.09506H158.857V6.56739L160.499 6.10479V8.09506H161.986V8.51848C161.986 8.97359 161.617 9.34254 161.161 9.34254H160.499V12.7345C160.499 13.2566 160.795 13.44 161.177 13.503C161.626 13.5774 162 13.9024 162 14.3574V14.977C160.446 14.977 158.857 14.9086 158.857 12.9447ZM98.1929 10.1124C98.2033 6.94046 100.598 5.16809 102.895 5.16809C104.171 5.16809 105.342 5.44285 106.304 6.12953L105.914 6.6631C105.654 7.02011 105.16 7.16194 104.749 6.99949C104.169 6.7702 103.622 6.7218 103.215 6.7218C101.335 6.7218 99.9169 7.92849 99.9068 10.1123C99.9169 12.2959 101.335 13.5201 103.215 13.5201C103.622 13.5201 104.169 13.4717 104.749 13.2424C105.16 13.0799 105.654 13.2046 105.914 13.5615L106.304 14.0952C105.342 14.7819 104.171 15.0566 102.895 15.0566C100.598 15.0566 98.2033 13.2842 98.1929 10.1124ZM147.619 5.21768C148.074 5.21768 148.444 5.58663 148.444 6.04174V9.81968L151.82 5.58131C151.897 5.47733 151.997 5.39282 152.112 5.3346C152.227 5.27638 152.355 5.24607 152.484 5.24611H153.984L150.166 10.0615L153.984 14.8749H152.484C152.355 14.8749 152.227 14.8446 152.112 14.7864C151.997 14.7281 151.897 14.6436 151.82 14.5397L148.444 10.3025V14.0508C148.444 14.5059 148.074 14.8749 147.619 14.8749H146.746V5.21768H147.619Z' fill='white'/%3E%3Cpath d='M0.773438 6.5752H2.68066C3.56543 6.5752 4.2041 6.7041 4.59668 6.96191C4.99219 7.21973 5.18994 7.62695 5.18994 8.18359C5.18994 8.55859 5.09326 8.87061 4.8999 9.11963C4.70654 9.36865 4.42822 9.52539 4.06494 9.58984V9.63379C4.51611 9.71875 4.84717 9.88721 5.05811 10.1392C5.27197 10.3882 5.37891 10.7266 5.37891 11.1543C5.37891 11.7314 5.17676 12.1841 4.77246 12.5122C4.37109 12.8374 3.81152 13 3.09375 13H0.773438V6.5752ZM1.82373 9.22949H2.83447C3.27393 9.22949 3.59473 9.16064 3.79688 9.02295C3.99902 8.88232 4.1001 8.64502 4.1001 8.31104C4.1001 8.00928 3.99023 7.79102 3.77051 7.65625C3.55371 7.52148 3.20801 7.4541 2.7334 7.4541H1.82373V9.22949ZM1.82373 10.082V12.1167H2.93994C3.37939 12.1167 3.71045 12.0332 3.93311 11.8662C4.15869 11.6963 4.27148 11.4297 4.27148 11.0664C4.27148 10.7324 4.15723 10.4849 3.92871 10.3237C3.7002 10.1626 3.35303 10.082 2.88721 10.082H1.82373Z' fill='white'/%3E%3Cpath d='M13.011 6.5752V10.7324C13.011 11.207 12.9084 11.623 12.7034 11.9805C12.5012 12.335 12.2068 12.6089 11.8201 12.8022C11.4363 12.9927 10.9763 13.0879 10.4402 13.0879C9.6433 13.0879 9.02368 12.877 8.5813 12.4551C8.13892 12.0332 7.91772 11.4531 7.91772 10.7148V6.5752H8.9724V10.6401C8.9724 11.1704 9.09546 11.5615 9.34155 11.8135C9.58765 12.0654 9.96557 12.1914 10.4753 12.1914C11.4656 12.1914 11.9607 11.6714 11.9607 10.6313V6.5752H13.011Z' fill='white'/%3E%3Cpath d='M15.9146 13V6.5752H16.9649V13H15.9146Z' fill='white'/%3E%3Cpath d='M19.9255 13V6.5752H20.9758V12.0991H23.696V13H19.9255Z' fill='white'/%3E%3Cpath d='M28.2828 13H27.2325V7.47607H25.3428V6.5752H30.1724V7.47607H28.2828V13Z' fill='white'/%3E%3Cpath d='M41.9472 13H40.8046L39.7148 9.16796C39.6679 9.00097 39.6093 8.76074 39.539 8.44727C39.4687 8.13086 39.4262 7.91113 39.4116 7.78809C39.3823 7.97559 39.3339 8.21875 39.2665 8.51758C39.2021 8.81641 39.1479 9.03905 39.1039 9.18554L38.0405 13H36.8979L36.0673 9.7832L35.2236 6.5752H36.2958L37.2143 10.3193C37.3578 10.9199 37.4604 11.4502 37.5219 11.9102C37.5541 11.6611 37.6025 11.3828 37.6669 11.0752C37.7314 10.7676 37.79 10.5186 37.8427 10.3281L38.8886 6.5752H39.9301L41.0024 10.3457C41.1049 10.6943 41.2133 11.2158 41.3276 11.9102C41.3715 11.4912 41.477 10.958 41.644 10.3105L42.558 6.5752H43.6215L41.9472 13Z' fill='white'/%3E%3Cpath d='M45.7957 13V6.5752H46.846V13H45.7957Z' fill='white'/%3E%3Cpath d='M52.0258 13H50.9755V7.47607H49.0859V6.5752H53.9155V7.47607H52.0258V13Z' fill='white'/%3E%3Cpath d='M61.2312 13H60.1765V10.104H57.2146V13H56.1643V6.5752H57.2146V9.20312H60.1765V6.5752H61.2312V13Z' fill='white'/%3E%3C/svg%3E");}@-webkit-keyframes formkit-bouncedelay-formkit-form-data-uid-813d4e331a-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}@keyframes formkit-bouncedelay-formkit-form-data-uid-813d4e331a-{0%,80%,100%{-webkit-transform:scale(0);-ms-transform:scale(0);transform:scale(0);}40%{-webkit-transform:scale(1);-ms-transform:scale(1);transform:scale(1);}}.formkit-form[data-uid="813d4e331a"] blockquote{padding:10px 20px;margin:0 0 20px;border-left:5px solid #e1e1e1;} .formkit-form[data-uid="813d4e331a"]{border:1px solid #e3e3e3;max-width:700px;position:relative;overflow:hidden;}.formkit-form[data-uid="813d4e331a"] .formkit-background{width:100%;height:100%;position:absolute;top:0;left:0;background-size:cover;background-position:center;opacity:0.3;}.formkit-form[data-uid="813d4e331a"] [data-style="minimal"]{padding:20px;width:100%;position:relative;}.formkit-form[data-uid="813d4e331a"] .formkit-header{margin:0 0 27px 0;text-align:center;}.formkit-form[data-uid="813d4e331a"] .formkit-subheader{margin:18px 0;text-align:center;}.formkit-form[data-uid="813d4e331a"] .formkit-guarantee{font-size:13px;margin:10px 0 15px 0;text-align:center;}.formkit-form[data-uid="813d4e331a"] .formkit-guarantee > p{margin:0;}.formkit-form[data-uid="813d4e331a"] .formkit-powered-by-convertkit-container{margin-bottom:0;}.formkit-form[data-uid="813d4e331a"] .formkit-fields{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin:25px auto 0 auto;}.formkit-form[data-uid="813d4e331a"] .formkit-field{min-width:220px;}.formkit-form[data-uid="813d4e331a"] .formkit-field,.formkit-form[data-uid="813d4e331a"] .formkit-submit{margin:0 0 15px 0;-webkit-flex:1 0 100%;-ms-flex:1 0 100%;flex:1 0 100%;}.formkit-form[data-uid="813d4e331a"][min-width~="600"] [data-style="minimal"]{padding:40px;}.formkit-form[data-uid="813d4e331a"][min-width~="600"] .formkit-fields[data-stacked="false"]{margin-left:-5px;margin-right:-5px;}.formkit-form[data-uid="813d4e331a"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field,.formkit-form[data-uid="813d4e331a"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{margin:0 5px 15px 5px;}.formkit-form[data-uid="813d4e331a"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-field{-webkit-flex:100 1 auto;-ms-flex:100 1 auto;flex:100 1 auto;}.formkit-form[data-uid="813d4e331a"][min-width~="600"] .formkit-fields[data-stacked="false"] .formkit-submit{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;} </style></form>

<script defer src="https://polymath.cloud/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
