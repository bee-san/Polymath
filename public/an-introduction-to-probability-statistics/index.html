<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>An Introduction to Probability &amp; Statistics | Polymath.cloud</title>

<meta name="keywords" content="University, Computer Science" />
<meta name="description" content="“Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.
 We start off by studying Probability Theory and then delve into statistics.
Probability and Statistics are used all the time in Computer Science. Machine learning? It’s probability. Data science? It’s statistics.
High Level Probability Probability provides a way of summarising the uncertainty that comes from our laziness and ignorance. In other words, probability finds out the likeliness that something will happen.">
<meta name="author" content="Bee">
<link rel="canonical" href="https://polymath.cloud/an-introduction-to-probability-statistics/" />
<link href="https://polymath.cloud/assets/css/stylesheet.min.94a69f3d0b70cac76c6d6f7dfecc9f91f2319ec73d54be960b0d3624fa5a25e2.css" integrity="sha256-lKafPQtwysdsbW99/syfkfIxnsc9VL6WCw02JPpaJeI=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://polymath.cloud/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://polymath.cloud/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://polymath.cloud/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://polymath.cloud/apple-touch-icon.png">
<link rel="mask-icon" href="https://polymath.cloud/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.68.3" />




<style>
    td, th {
    border: thin solid #999 !important;
    padding: 12px 15px;
}

thead tr {
    background-color: #009879;
    color: #ffffff;
    text-align: left;
}

table {
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    font-family: sans-serif;
    min-width: 400px;
    overflow: auto;
    display: table;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
}

tbody tr {
    border-bottom: thin solid #dddddd;
}


tbody tr:last-of-type {
    border-bottom: 2px solid #009879;
}

tbody td.active-item {
    font-weight: bold;
    color: #009879;
}

tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
    }


body.dark tbody tr:nth-of-type(even) {
    background-color: #383838;
}


img {
    display: block;
    margin: auto;
    text-align: center;
}

</style>
<meta property="og:title" content="An Introduction to Probability &amp; Statistics" />
<meta property="og:description" content="“Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.
 We start off by studying Probability Theory and then delve into statistics.
Probability and Statistics are used all the time in Computer Science. Machine learning? It’s probability. Data science? It’s statistics.
High Level Probability Probability provides a way of summarising the uncertainty that comes from our laziness and ignorance. In other words, probability finds out the likeliness that something will happen." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://polymath.cloud/an-introduction-to-probability-statistics/" />
<meta property="article:published_time" content="2019-01-26T15:23:06+00:00" />
<meta property="article:modified_time" content="2019-01-26T15:23:06+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An Introduction to Probability &amp; Statistics"/>
<meta name="twitter:description" content="“Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.
 We start off by studying Probability Theory and then delve into statistics.
Probability and Statistics are used all the time in Computer Science. Machine learning? It’s probability. Data science? It’s statistics.
High Level Probability Probability provides a way of summarising the uncertainty that comes from our laziness and ignorance. In other words, probability finds out the likeliness that something will happen."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "An Introduction to Probability \u0026 Statistics",
  "name": "An Introduction to Probability \x26 Statistics",
  "description": "“Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.\n We start off by studying Probability Theory and then delve into statistics. …",
  "keywords": [
    "University", "Computer Science"
  ],
  "articleBody": " “Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.\n We start off by studying Probability Theory and then delve into statistics.\nProbability and Statistics are used all the time in Computer Science. Machine learning? It’s probability. Data science? It’s statistics.\nHigh Level Probability Probability provides a way of summarising the uncertainty that comes from our laziness and ignorance. In other words, probability finds out the likeliness that something will happen.\nDiscrete Probability Discrete probability is a formalisation of probability theory that describes probability for usage in computers from discrete mathematics.\nWhen solving problems with discrete probability, we start with using probability spaces. A probability space is the pairing (S, P) where:\n S is the sample space of all elementary events X ∈ S. Members of S are called outcomes of the experiment. P is the probability distribution, that is, assigning a real number P(x) to every elementary event X ∈ S such that it’s probability is between 0 and 1 and ∑P(x) = 1  For point 2, P(x) is read as “the probability of X”. The probability must always be between 0 and 1, or often represented as 0% and 100%.\nExample Imagine flipping a coin. The probability space is (S, P).The outcome S is S = {H, T} where S can either be Heads or Tails.Therefore the probability isP(H) = P(T) = 1/2The probability for heads is the same as the probability for tails which is the same as a half. In other words, if you flip a coin there is an even chance of it becoming head-side up or tail-side up.\nA probability distribution is considered uniform if every outcome is equally as likely.\nAn Introduction to Solving Probability Problems Many many people including university professors and PhD students cannot solve probability problems. As discussed later in this article the Monty Hall problem is a famous problem and good example of this.\n Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say №1, and the host, who knows what’s behind the doors, opens another door, say №3, which has a goat. He then says to you, “Do you want to pick door №2?” Is it to your advantage to switch your choice?\n This question was sent into Voe Savant who at the time had the highest IQ in the world. Voe Savant replied that there is a 2/3rd chance of winning the car if you switch and 1/3rd if you don’t switch.\nThousands of people argued over the Monty Hall problem and many university professors of maths said that math illiteracy was rampant in America because the Monty Hall problem solution suggested was wrong.\nThis problem appeared in every maths class the following week and thousands of readers, many who have PhDs in maths wrote in to explain that Savant was wrong. Even Paul Erdős, one of the worlds most famous mathematicians, said that Savant was wrong.\nUnfortunately for them, Savant was right. This is a simple probability problem that if defined formally can be explained. Many of the mathematicians used their intuition to solve this problem and not follow the steps in solving a probability problem that will be outlined below.\nThere are a few steps you need to take before you solve a probability problem to prove you fully understand the problem.\nSample Space The sample space is the set which contains all possible outcomes.\nSo given a coin, the sample space is {heads, tails} as the coin can only land on heads or tails.\nOutcome An outcome consists of all the information of an experiment after the experiment has been performed. When you flip a coin and it lands on heads, the outcome is {heads}.\nProbability Space The probability space is the sample space but every possible outcome has a probability applied to it. With the coin flip the probability space is {(Heads, 0.5), (Tails, 0.5)}.\nThe total probability of all probabilities in the probability space must be equal to 1. No single probability can be less than 0 or more than 1. Many high-achieving students tell me that they try to visualise what they are dealing with as much as possible.\nExample Let’s suppose we roll a 6-sided dice and we want to work out the probability we get a 4.\n Count the number of possible events. There are 6 sides to the dice. So there are 6 possible events Decide which event you are examining for probability. The problem let’s us know we are trying to roll a four. Count the number of chances that heads can occur out of the possible events. There is only one side of the die that has 4 dots, so there is only 1 chance to roll a four out of 6 total chances. Write the number of chances heads could occur over the number of possible events in a fraction. (1/6)  Although this is a simple problem to solve,it illustrates the important steps to take when solving harder probability problems.\nEvents Events are often overlooked in probability theory and are not talked about much, so I took it upon myself to expand on what an event is and why they’re important in this section.\nAn event is a set of outcomes of an experiment in probability. In Bayesian Probability an event is defined as describing the next possible state space using knowledge from the current state.\nAn event is often denoted with the character ‘e’. Such as the probability being P(e) of an event. Events are a lot more important in probability than most people make them out to be.\nAn event can be the result of rolling a dice such as a “5” or getting a Tail when flipping a coin.\nEvents can be:\n Independent — Each event is not effected by previous or future events. Dependent — An event is affected by other events Mutually Exclusive — Events can’t happen at the same time  Why are Events Important? Well, events allow us to do some pretty amazing things with probability. Take for instance, the Monty Hall Problem. Attempt the question below: One of the doors above contains a fancy sports car, the other 2 doors contain goats. Pick any door you like, go on!\nOkay, let’s say you picked #1, the game show host will open a door that contains a goat, so let’s say we open door number 3 and it contains a goat. So you know that door 1 is your pick, door 3 is a goat and door 2 is untouched. Note: It doesn’t matter what door you originally picked, what matters is that you pick a door and the gameshow host opens a door with a goat in it. The game show then asks: “Are you sure door number 1 is right? Do you want to switch?”\nWhat do you do?\nWell, probability states that we should pick door number 2, as in you would switch. Why? well, door number 2 has a 2/3 chance or 77% chance of containing the car and door number 1 (your original pick) has a 33% chance of containing a car.\nWhaaaaattt??\nThis is a famous probability problem called the Monty Hall Problem and it displays how events can affect probabilities. For an explanation of this, watch this Numberphile video below:\nThe Probability of the Complement of an Event The complement of an event is all the other outcomes of an event.\nFor example, if the event is Tails, the complement is Heads. If the event is {Monday, Tuesday}, the complement is {Wednesday, Thursday, Friday, Saturday, Sunday}.\nIf you know the probability of p(x) you can find the compliment by doing 1 — P(x). Since all probabillities equal 100%, we can express this as 1.\nWhy is the Complement Useful? Sometimes it is easier to work out the complement first before the actual probability. For example:\nWork out the probability that when 2 die are thrown that the two scores are different  A different score is like getting a 2 and 3, or 1 and 6. The set of all possible different scores is quite large, but the complement of all possible different scores (scores are the same) is quite low. In fact, it is:\n{ (1, 1), (2, 2), (3,3), (4,4), (5,5), (6,6) }\nThe total number of different combinations is 6*6 which is 36, so the probability of getting a score that is the same is 6/36 or 1/6. Now we can take 1/6 away from 1 (think of 1 as the universal set) which equates to 5/6.\nThe Union of Two Events (Inlcusion-Exclusion Principle) This requries you to know a little about set theory, so click here to learn more.\nIf two events are mutually exlusive (they cannot occur at the same time) then the probability of them happening at the same time is 0.\nIf two events are not mutually exclusive, then the probability of the union of the two events is the probability of both events added together. The reason we take away the intersection of A and B is because P(A) + P(B) contains all that it is in A or B but because of the way union works, there will be an intersection which will make 2 A’s and 2 B’s so we need to take away the intersection to get the probability of each events.\nIn other words, A contains elements that are in B and B contains elements that are in A. By adding: Union of Three Disjoint Events Suppose that I were to roll a fair dice 3 times.S is the set of sequences of events over length three such that { 1..6)³}P(x) = 1/666 = 1/216 for all x ∈ SWhat is the probability that we roll at least one 6?So because we throw the dice 3 times, let E1 be the probability that the dice roll is a 6, E2 = P(6), E3 = P(6)We would like to work outP(E1∪E2∪E3)\nRemember, the union of probabilities is P(A) + P(B) — Intersection of A and B. We want the union of A, b and C which also includes the intersection in the middle. We take away the intersections of A B, A C, B C and add the intersection of all 3 to get the middle part. So this is just: You might have noticed that the intersection is 6/216. This may seem confusing because we didn’t hand-define a set for this. Worry not: The formula for intersection is: Example Question Given 4 coins, what is the probability that at least 3 of them come up tails?  The event that at least 3 coins come up tails is the union of five disjoint events, that all coins come up tails (1 disjoint event) and that 4 specified coins (4 disjoint events) come up heads. This may sound confusing, so I’ll explain it visually. Feel free to skip the next paragraph if you’re not confused.\nA disjoint event means that events cannot happen at the same time. The first disjoint event is “what if all coins come up tails?” That is that the 5 coins {T, T, T, T, T}. The other 4 events are what if one specified coin comes up heads? So the first disjoint event is {H, T, T, T}, the second is {T, H, T, T} etc. Since we need at least 3 coins to be tails, {H, H, T, T} isn’t valid.\nThe union of 5 disjoint events is the probability of each event happening added together.\nFirst, lets find out the probability that any probability within this space is possible. The problem space is {H, T} over 4 different coins. Each coin has a 1/2 chance of being heads or tails and there are 4 coins so 1/2 * 1/2 * 1/2 * 1/2 is 1/16 chance of any possible outcome in the state space.\nTherefore the probability of an event is P(1/16)\nKnow that we know how likely it is to get any combination of {H, T} over the 4 coins we can use this to work out how likely it is to get the 5 disjoint events. Since each event is disjoint, one event does not effect another so it’s just a case of 1/16 * 5 (for the 5 disjoint events) which results in 5/16.\nThus the probability of at least 3 coins coming up tails is 5/16.\nConditional Probability Conditional probability is where an event can only happen if another event has happened. Let’s start with an easy problem:\nJohn’s favourite programming languages are Haskell and x86 Assembley. Let A represent the event that he forces a class to learn Haskell and B represent the event that he forces a class to learn x86 Assembley.On a randomly selected day, John is taken over by Satan himself, so the probability of P(A) is 0.6 and the probability of P(B) is 0.4 and the conditional probability that he teaches Haskell, given that he has taught x86 Assembley that day is P(A|B) = 0.7.Based on the information, what is P(B|A), the conditional that John teaches x86 Assembley given that he taught Haskell, rounded to the nearest hundredth?\nThe probability of P(A and B) = P(A|B) * P(B) read “|” as given, as in, “A|B” is read as “A given B”. It can also be written as P(B|A) * P(A).\nThe reason it is P(A|B) * P(B) is because given the probability of “Given the probability that B happens, A happens” and the probability of B is P(B). (A|B) is a different probability to P(B) and P(A and B) can only happen if P(B) happens which then allows P(B|A) to happen.\nSo we can transform this into a mathematical formula:\nP(A and B) = P(A|B) * P(B) = 0.7 * 0.5 = 0.35Solving itP(B|A) * P(A)P(A) = 0.5So0.6 * P(B|A)Now we don’t know what P(B|A) is, but we want to find out. We know that P(B|A) must be a part of P(A and B) because P(A and B) is the probability that both of these events happen so…P(A and B) = 0.350.35 = P(B|A) * 0.5With simple algebraic manipulation0.35/0.5 = P(B|A)P(B|A) = 0.7\nFor a visual explanation of conditional probability, watch this video by Khan Academy\nBayes Therom Bayes Therom allows us to work out the probability of events given prior knowledge about the events. It is more of an observation than a therom, as it correctly works all the time. Bayes therom is created by Thomas Bayes, who noted this observation in a notebook. He never published it, so he wasn’t recgonised for his famous therom during his life time. Bayes Therom from https://betterexplained.com/articles/colorized-math-equations/ The probability of A given B is the probability of B given A (note: it’s reversed here) times by the probability of A divided by the probability of B.\nOf course this sounds confusing, so it may help to see an example.\nSuppose a new strand of mexican black tar heroin is found on the streets and the police want to identify whether someone is a user or not.The drug is 99% sensitive, that is that the proportion of people who are correctly identified as taking the drug.The drug is 99% specific, that is that the proportion of people who are correctly identified as not taking the drug.Note: there is a 1% false positive rate for both users and non users.Suppose that 0.5% of people at John Moores takes the drug. What is the probability that a randomly selected John Moores student with a positive test is a user? Once you have all the information, it’s simply a case of substituting in the values and working it out.\nBelow is a video explaining Bayes Therom intuitively with real-world examples along with the history behind it as well as the philosophy of Bayes Therom:\nIf you want to see how Bayes Therom is used in Machine Learning — check this out!\nAbsolute Fundamentals of Machine Learning *Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to…*hackernoon.com\nRandom Variables A random variable is a function, it is not random or a variable.\nA random variable does not need to specify the sample space S directly but assign a probability that a variable (X) takes a certain value. Unlike previous probability where we needed to define a sample space, we only care about the probability itself.\nRandom variables are often written as P(f=r) where f is the event name and r is the probability.\nThe probably has to be between 0 and 1, like all probability values.\nWe write NOT (using any notation you may wish) (F=r) for the event that F is every variable apart from R.\nAn example of this\nP(Die=1) = 1/6The probability that this die takes the value 1 is 1/6NOT P(Die=1) is the event that the die is(Die=2) OR (Die=3) OR (Die=4) OR (Die=5) Or (Die=6)  The complement of P(f=r) ; the notation used to represent random variables, is 1 — P(f=r), where 1 is 100% or just 1.\nWe sometimes use symbols (words) instead of numbers to represent random variables. This is really useful. Let’s say the weather can be 1 of 4 states, sunny, rain, cloudy, snow. Thus, instead of assigning Weather = 1 we could write Weather = sunny.\nSometimes it lengthy to write up all the probabilities such as P(Weather = sunny) = 0.7 or P(Weather = rain) = 0.3. If the values are fixed in order then we could write P(Weather) = (0.7, 0.3)\nWe use bold-face P to indicate that the result is a vector of numbers representing the individual values of Weather. An example of this is: P(Weather) = (0.7, 0.3).\nJoint Probability Distributions A joint probability distribution allows you to have multiple random variables, typically 50 or 100 but our examples will include fewer.\nA possible joint probability distribution P(Weather, Cavity) for the random variables Weather and Cavity is given by the following table: This is a joint probability distribution for tooth cavities and the weather. Cavity is a boolean value, it is either 0 or 1 and there are 4 options for the weather. If we want to create a joint probability distribution of P(Weather, Cavity) we would make the table above.\nThe probability for weather = sunny, and cavity = 1 is 0.144. The probability of the joint distribution sums to 1.\nFull Joint Probability Distribution We call it a full joint probability distribution if everything that is relevant in the domain is included. Unlike the above example, cavities and weather aren’t in the same domain.\nAssume the random variables Toothache, Cavity, Catch fully describe a visit to a dentist\nThen a full joint probability distribution is given by the following table: From here\nMarginalisation One can computer the marginal probabilities of random variables by summing the variables. For example in the above example, if one wanted to sum the probability of P(Cavity=1) then you will sum all the probabilities where the cavity is equal to 1. Conditonal / Posterior Probability We can calculate the conditional / posteior probability of a full joint distribution the same way we would do it normally. Note that (F, G) signifies F (and, intersection) G.\nExpected Value The expected value is exactly what it sounds like, what do you expect the value to be? You can use this to work out the average score of a dice roll over 6 rolls, or anything really relating to probability where it has a value property.\nGiven the outcomes=(1, 2) and the probabilities=(1/8, 1/4) the expected value, E[x] is E[x] = 1(1/8) + 2(1/4) = 0.625. Suppose we’re counting types of bikes, and we have 4 bikes. We assign a code to each bike like so: For every bike, we give it a number. For every coding, we can see we use 2 bits. Either 0 or 1. For the expected value, not only do we need the value for the variable but the probability. Each bike has equal probability. So each bike has a 25% chance of appearing.\nCalculating the expected value we multiply the probability by 2 bits, which gets us: What if the probability wasn’t equal? What we need to do is to multiply the number of bits by the probability Entropy Entropy is a measure of uncertainty assiocated with a random variable. It’s defined as the expected number of bits required to communicate the value of the variable. Entropy is trying to give a number to how uncertain something is.\nStatistics Statistics is not probability theory. Statistics is the real world application of ideas which come from probabiltiy theory. These can invole:\n Psepholohy — Analysing voting patterns Data analysis — Data science Quality control  Sample Space A sample space is a collection of data as a single finite set that looks something like: Where S is the sample space.\nProbability Distribution Let’s say we want to pick a random person from a set of all people who read the Sun newspaper. The probability of a single person being chosen is: A probability distribution is a sample space where every item has a probability value between 0 and 1 assigned to them that represents how likely they are to be picked.\nIn total, if s is an element of S, that is, if an element s is a part of the set (group) of sample space, S, then: If you add the probability of every element in the sample space, it has to sum to 1.\nWhen we want to sample this data set, we could just go through every single person in the dataset to get a good feel of the generalness of this sample. However, if there were 7 billion people in this dataset that may take a very very long time.\nThere are 2 ways we can now sample the data.\nWe can either randomly choose people from the dataset and use that as our sample or we can hand-pick a specific subset of the data to use.\nA uniform dataset is one where everyone is equally likely to be picked. A biased sample is not uniform, the people were hand picked.\nUnbiased data sets seem “fair” whereas unbiased seem “unfair”. With an unbiased sample we cannot fix the outcome. We cannot change the data to our favor.\nSometimes we don’t care about “fairness” and sometimes unbiased samples can lead to unexpected results.\nRandom Variables Remember earlier when we said that random variables are functions? Well, if you apply a random variable to a sample space, a population like so: You get a biased data set from that sample space. It’s biased because we’re not randomly picking people in the set; we are applying a filter — a rule to the set to get a subset of the population.\nProfessor Paul Dunne had this to say about random variables:\n The notion of a probability distribution. This is the description of the probability that a member of a population (ie set) is selected. For example if we consider a single die the population has 6 members: {1,2,3,4,5,6} We might have a probability distribution corresponding to a fair die so that each has a probability of 1/6 of being chosen. If it’s a biased die then, for example, the probability distribution could be P[6]=5/6 P[1]=0 and P[2]=P[3]=P[4]=P[5]=1/24 With this the sum of individual outcomes is 1.\n  A random variable is best thought of firstly by forgetting about probabilities and thinking of an arbitrary function from the population to for instance the real numbers. In the die example we could choose f(x)=x² now unlike the probability distribution function the function chosen has no constraints: members of the population do not have to have values between 0 and 1, the sum of the function values doesn’t have to add up to 1. Where the idea of “random variable” enters is when a function is combined with a probability distribution. Now the distribution is treated not as simply choosing a MEMBER of the population but as choosing the VALUE of the function in a random style, that is instead of returning the selected member (eg result of throwing a die) the function value for that member is reported (eg the square of the number thrown).\n Mean Average Value with Random Variables Given a population, S, whose members are sampled according to a distribution, D. The mean (expected) value of the random variable r(s) under D is denoted as\nThis is simply states that the expected value is a “weighted” sum (taken over all the members, s, of the total population, S) of:  *the chance that D selects s multipled by the value of the function returned by r for s, ie r(s).*In Unbiased Distributions\n Unbiased Distributions In unbiased distributions the expected value is just the total sum of all the random variables divided by the population size: This is just your typical mean value, the one you learn in school. My teacher taught me a cool song to remember the differences between mean, range, median etc.\n Hey diddle diddle the median is the middle we add and divide for the mean. The mode is the one you see the most, and the range is the difference in between!\n Suppose S is a collection of outcomes that may appear by rolling a die 6,000 times.\nThen for a “fair” die you would expect to see each outcome 1,000 times.\nSuppose we have a game where players stake £1 and if the die lands on one of {1, 2, 3} the player gets £2 afterwards otherwise they lose their stake. In a fair game the player can expect to win 3/6 = 1/2 = half of the time.\nConfidence Testing Let’s say the hypothesis of an experiment’s outcome is X, and the actual outcome is Y.\nThe outcome Y is so far away from the prediction that the hypothesis is false. This is called significance.\nA null hypothesis states that the outcome will be X.\nSignificance represents that the likelihood of the observed outcome being “consistent” with the predicted outcome.\nAn hypothesis can be “rejected” with observed outcomes with three increasing levels of confience:\n The probability that X holds given Y is at most 0.05 (significant) The probability that X holds given that Y has resulted is at most 0.01 (highly significant) The probability that X holds given that Y has resulted is 0.001 (very highly siginificant)  There are two types of errors that can occur here:\nType 1 error — A true hypothesis is rejected Type 2 error — A false hypothesis is accecpted\nMeasuring Signifcance The outcome of the event will get “closer and closer” towards the expected value can be expressed as a formula called the deviance. Recall that the event of a random variable in a sample space is: Variance is just:\n “How far away a chosen member is from the expected variable”\n Doesn’t that look horrible? Well, if we were to put the first formula in it would look like: Doesn’t that look like the most horrible formula ever?\nthe r(s) part is the random variable, the subset of the population. The part is the expected value of a random member.\nVariance always produces a non negative value.\nThe standard deviation is just this formula, square rooted. It’s actually more commonly written as:\nI just wanted to see how convuluted the formula could become.\nThe standard deviation is just:\n “How far away the largest (or smallest) data point is from the mean average”.\n Q-test Given a predicted outcome, X, of an experiment and the actual outcome, Y. If we know the standard deviation for the environment in which the experiment is set, then we can compute the value: If q  0.01 then X holds with probability at best 0.05 If q  2.33 then X holds with probability at best 0.01 If q  3.09 then X holds with probability at best 0.001\nIf you liked this article, connect with me! LinkedIn | Twitter | Newsletter\nBuy Brandon Skerritt a Coffee. ko-fi.com/hacker Support the content you love. Buy a Coffee for Brandon Skerritt with Ko-fi.comko-fi.comPay Brandon Skerritt using PayPal.Me *Go to paypal.me/BrandonSkerritt and type in the amount. Since it’s PayPal, it’s easy and secure. Don’t have a PayPal…*www.paypal.me\n",
  "wordCount" : "4720",
  "inLanguage": "en",
  "datePublished": "2019-01-26T15:23:06Z",
  "dateModified": "2019-01-26T15:23:06Z",
  "author":{
    "@type": "Person",
    "name": "Bee"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://polymath.cloud/an-introduction-to-probability-statistics/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Polymath.cloud",
    "logo": {
      "@type": "ImageObject",
      "url": "https://polymath.cloud/favicon.ico"
    }
  }
}
</script>



</head>

<body class="">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://polymath.cloud" accesskey="h">Polymath.cloud</a>
            <span class="logo-switches">
                <span class="theme-toggle">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://polymath.cloud/archives" title="Archive">
                    <span>
                        Archive
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/search/" title="Search">
                    <span>
                        Search
                    </span>
                </a>
            </li>
            <li>
                <a href="https://polymath.cloud/tags/" title="Tags">
                    <span>
                        Tags
                    </span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      An Introduction to Probability &amp; Statistics
    </h1>
    <div class="post-meta">

January 26, 2019&nbsp;·&nbsp;23 min&nbsp;·&nbsp;Bee

    </div>
  </header> 

  <div class="post-content">
<blockquote>
<p><em>“Probability Theory Should Be Thrown Under A Bus” — Artifical Intelligence Expert, Carlos E. Perez.</em></p>
</blockquote>
<p>We start off by studying Probability Theory and then delve into statistics.</p>
<p>Probability and Statistics are used all the time in Computer Science. Machine learning? It’s probability. Data science? It’s statistics.</p>
<h3 id="high-level-probability">High Level Probability<a hidden class="anchor" aria-hidden="true" href="#high-level-probability">#</a></h3>
<p>Probability provides a way of summarising the uncertainty that comes from our laziness and ignorance. In other words, probability finds out the likeliness that something will happen.</p>
<h3 id="discrete-probability">Discrete Probability<a hidden class="anchor" aria-hidden="true" href="#discrete-probability">#</a></h3>
<p>Discrete probability is a formalisation of probability theory that describes probability for usage in computers from discrete mathematics.</p>
<p>When solving problems with discrete probability, we start with using probability spaces. A probability space is the pairing (S, P) where:</p>
<ol>
<li>S is the sample space of all elementary events X ∈ S. Members of S are called outcomes of the experiment.</li>
<li>P is the probability distribution, that is, assigning a real number P(x) to every elementary event X ∈ S such that it’s probability is between 0 and 1 and ∑P(x) = 1</li>
</ol>
<p>For point 2, P(x) is read as “the probability of X”. The probability must always be between 0 and 1, or often represented as 0% and 100%.</p>
<h3 id="example">Example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h3>
<p>Imagine flipping a coin. The probability space is (S, P).The outcome S is <strong>S = {H, T}</strong> where S can either be Heads or Tails.Therefore the probability isP(H) = P(T) = 1/2The probability for heads is the same as the probability for tails which is the same as a half. In other words, if you flip a coin there is an even chance of it becoming head-side up or tail-side up.</p>
<p>A probability distribution is considered <em>uniform</em> if every outcome is equally as likely.</p>
<h3 id="an-introduction-to-solving-probability-problems">An Introduction to Solving Probability Problems<a hidden class="anchor" aria-hidden="true" href="#an-introduction-to-solving-probability-problems">#</a></h3>
<p>Many many people including university professors and PhD students cannot solve probability problems. As discussed later in this article the Monty Hall problem is a famous problem and good example of this.</p>
<blockquote>
<p><em>Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say №1, and the host, who knows what’s behind the doors, opens another door, say №3, which has a goat. He then says to you, “Do you want to pick door №2?” Is it to your advantage to switch your choice?</em></p>
</blockquote>
<p>This question was sent into Voe Savant who at the time had the highest IQ in the world. Voe Savant replied that there is a 2/3rd chance of winning the car if you switch and 1/3rd if you don’t switch.</p>
<p>Thousands of people argued over the Monty Hall problem and many university professors of maths said that math illiteracy was rampant in America because the Monty Hall problem solution suggested was wrong.</p>
<p>This problem appeared in every maths class the following week and thousands of readers, many who have PhDs in maths wrote in to explain that Savant was wrong. Even Paul Erdős, one of the worlds most famous mathematicians, said that Savant was wrong.</p>
<p>Unfortunately for them, Savant was right. This is a simple probability problem that if defined formally can be explained. Many of the mathematicians used their intuition to solve this problem and not follow the steps in solving a probability problem that will be outlined below.</p>
<p>There are a few steps you need to take before you solve a probability problem to prove you fully understand the problem.</p>
<h3 id="sample-space">Sample Space<a hidden class="anchor" aria-hidden="true" href="#sample-space">#</a></h3>
<p>The sample space is the set which contains all possible outcomes.</p>
<p>So given a coin, the sample space is {heads, tails} as the coin can only land on heads or tails.</p>
<h3 id="outcome">Outcome<a hidden class="anchor" aria-hidden="true" href="#outcome">#</a></h3>
<p>An outcome consists of all the information of an experiment after the experiment has been performed. When you flip a coin and it lands on heads, the outcome is {heads}.</p>
<h3 id="probability-space">Probability Space<a hidden class="anchor" aria-hidden="true" href="#probability-space">#</a></h3>
<p>The probability space is the sample space but every possible outcome has a probability applied to it. With the coin flip the probability space is {(Heads, 0.5), (Tails, 0.5)}.</p>
<p>The total probability of all probabilities in the probability space must be equal to 1. No single probability can be less than 0 or more than 1.
<img src="https://cdn-images-1.medium.com/max/800/1*w98z_3H7Vm9Ruu-3rR_FrA.png" alt="img">
Many high-achieving students tell me that they try to visualise what they are dealing with as much as possible.</p>
<h3 id="example-1">Example<a hidden class="anchor" aria-hidden="true" href="#example-1">#</a></h3>
<p>Let’s suppose we roll a 6-sided dice and we want to work out the probability we get a 4.</p>
<ol>
<li>Count the number of possible events. There are 6 sides to the dice. So there are 6 possible events</li>
<li>Decide which event you are examining for probability. The problem let’s us know we are trying to roll a four.</li>
<li>Count the number of chances that heads can occur out of the possible events. There is only one side of the die that has 4 dots, so there is only 1 chance to roll a four out of 6 total chances.</li>
<li>Write the number of chances heads could occur over the number of possible events in a fraction. (1/6)</li>
</ol>
<p>Although this is a simple problem to solve,it illustrates the important steps to take when solving harder probability problems.</p>
<h3 id="events">Events<a hidden class="anchor" aria-hidden="true" href="#events">#</a></h3>
<p>Events are often overlooked in probability theory and are not talked about much, so I took it upon myself to expand on what an event is and why they’re important in this section.</p>
<p>An event is a set of outcomes of an experiment in probability. In Bayesian Probability an event is defined as describing the next possible state space using knowledge from the current state.</p>
<p>An event is often denoted with the character ‘e’. Such as the probability being P(e) of an event. Events are a lot more important in probability than most people make them out to be.</p>
<p>An event can be the result of rolling a dice such as a “5” or getting a Tail when flipping a coin.</p>
<p>Events can be:</p>
<ol>
<li>Independent — Each event is not effected by previous or future events.</li>
<li>Dependent — An event is affected by other events</li>
<li>Mutually Exclusive — Events can’t happen at the same time</li>
</ol>
<h3 id="why-are-events-important">Why are Events Important?<a hidden class="anchor" aria-hidden="true" href="#why-are-events-important">#</a></h3>
<p>Well, events allow us to do some pretty amazing things with probability. Take for instance, the Monty Hall Problem. Attempt the question below:
<img src="https://cdn-images-1.medium.com/max/800/1*qHDZ_HkSDRAullLhe05VSQ.png" alt="img">
One of the doors above contains a fancy sports car, the other 2 doors contain goats. Pick any door you like, go on!</p>
<p>Okay, let’s say you picked #1, the game show host will open a door that contains a goat, so let’s say we open door number 3 and it contains a goat. So you know that door 1 is your pick, door 3 is a goat and door 2 is untouched. Note: It doesn’t matter what door you originally picked, what matters is that you pick a door and the gameshow host opens a door with a goat in it.
<img src="https://cdn-images-1.medium.com/max/800/1*Va0XV0-W6rspB4xihWBmQA.png" alt="img">
The game show then asks: “Are you sure door number 1 is right? Do you want to switch?”</p>
<p>What do you do?</p>
<p>Well, probability states that we should pick door number 2, as in you would switch. Why? well, door number 2 has a 2/3 chance or 77% chance of containing the car and door number 1 (your original pick) has a 33% chance of containing a car.</p>
<p>Whaaaaattt??</p>
<p>This is a famous probability problem called the Monty Hall Problem and it displays how events can affect probabilities. For an explanation of this, watch this Numberphile video below:</p>
<h3 id="the-probability-of-the-complement-of-an-event">The Probability of the Complement of an Event<a hidden class="anchor" aria-hidden="true" href="#the-probability-of-the-complement-of-an-event">#</a></h3>
<p>The complement of an event is all the other outcomes of an event.</p>
<p>For example, if the event is Tails, the complement is Heads. If the event is {Monday, Tuesday}, the complement is {Wednesday, Thursday, Friday, Saturday, Sunday}.</p>
<p>If you know the probability of p(x) you can find the compliment by doing 1 — P(x). Since all probabillities equal 100%, we can express this as 1.</p>
<h3 id="why-is-the-complement-useful">Why is the Complement Useful?<a hidden class="anchor" aria-hidden="true" href="#why-is-the-complement-useful">#</a></h3>
<p>Sometimes it is easier to work out the complement first before the actual probability. For example:</p>
<pre><code>Work out the probability that when 2 die are thrown that the two scores are different
</code></pre>
<p>A different score is like getting a 2 and 3, or 1 and 6. The set of all possible different scores is quite large, but the complement of all possible different scores (scores are the same) is quite low. In fact, it is:</p>
<p>{ (1, 1), (2, 2), (3,3), (4,4), (5,5), (6,6) }</p>
<p>The total number of different combinations is 6*6 which is 36, so the probability of getting a score that is the same is 6/36 or 1/6. Now we can take 1/6 away from 1 (think of 1 as the universal set) which equates to 5/6.</p>
<h3 id="the-union-of-two-events-inlcusion-exclusion-principle">The Union of Two Events (Inlcusion-Exclusion Principle)<a hidden class="anchor" aria-hidden="true" href="#the-union-of-two-events-inlcusion-exclusion-principle">#</a></h3>
<p>This requries you to know a little about set theory, so click <a href="https://medium.com/brandons-computer-science-notes/a-primer-on-set-theory-746cd0b13d13">here</a> to learn more.</p>
<p>If two events are mutually exlusive (they cannot occur at the same time) then the probability of them happening at the same time is 0.</p>
<p>If two events are not mutually exclusive, then the probability of the union of the two events is the probability of both events added together.
<img src="https://cdn-images-1.medium.com/max/800/1*6oJV0e4u70nsViAjjVkEOA.png" alt="">
The reason we take away the intersection of A and B is because P(A) + P(B) contains all that it is in A or B but because of the way union works, there will be an intersection which will make 2 A’s and 2 B’s so we need to take away the intersection to get the probability of each events.</p>
<p>In other words, A contains elements that are in B and B contains elements that are in A. By adding:
<img src="https://cdn-images-1.medium.com/max/800/1*kjnsvpWVqtuMe9zSRLMyAA.png" alt=""><img src="https://cdn-images-1.medium.com/max/800/1*nL3wFe5taxhNJZhZwaFljw.png" alt=""></p>
<h3 id="union-of-three-disjoint-events">Union of Three Disjoint Events<a hidden class="anchor" aria-hidden="true" href="#union-of-three-disjoint-events">#</a></h3>
<p>Suppose that I were to roll a fair dice 3 times.S is the set of sequences of events over length three such that { 1..6)³}P(x) = 1/6<em>6</em>6 = 1/216 for all x ∈ SWhat is the probability that we roll at least one 6?So because we throw the dice 3 times, let E1 be the probability that the dice roll is a 6, E2 = P(6), E3 = P(6)We would like to work outP(E1∪E2∪E3)</p>
<p>Remember, the union of probabilities is P(A) + P(B) — Intersection of A and B. We want the union of A, b and C which also includes the intersection in the middle. We take away the intersections of A B, A C, B C and add the intersection of all 3 to get the middle part.
<img src="https://cdn-images-1.medium.com/max/800/0*Ziv4odRalsjjVFsn.png" alt="">
So this is just:
<img src="https://cdn-images-1.medium.com/max/800/1*JNvJ5ezGe_mtLE0wKXx4Sg.png" alt="">
You might have noticed that the intersection is 6/216. This may seem confusing because we didn’t hand-define a set for this. Worry not: The formula for intersection is:
<img src="https://cdn-images-1.medium.com/max/800/1*yqWpNDejt5Khtiuw-zN1Iw.png" alt=""></p>
<h3 id="example-question">Example Question<a hidden class="anchor" aria-hidden="true" href="#example-question">#</a></h3>
<pre><code>Given 4 coins, what is the probability that at least 3 of them come up tails?
</code></pre>
<p>The event that at least 3 coins come up tails is the union of five disjoint events, that all coins come up tails (1 disjoint event) and that 4 specified coins (4 disjoint events) come up heads. This may sound confusing, so I’ll explain it visually. Feel free to skip the next paragraph if you’re not confused.</p>
<p>A disjoint event means that events cannot happen at the same time. The first disjoint event is “what if all coins come up tails?” That is that the 5 coins {T, T, T, T, T}. The other 4 events are what if one specified coin comes up heads? So the first disjoint event is {H, T, T, T}, the second is {T, H, T, T} etc. Since we need at least 3 coins to be tails, {H, H, T, T} isn’t valid.</p>
<p>The union of 5 disjoint events is the probability of each event happening added together.</p>
<p>First, lets find out the probability that any probability within this space is possible. The problem space is {H, T} over 4 different coins. Each coin has a 1/2 chance of being heads or tails and there are 4 coins so 1/2 * 1/2 * 1/2 * 1/2 is 1/16 chance of any possible outcome in the state space.</p>
<p>Therefore the probability of an event is P(1/16)</p>
<p>Know that we know how likely it is to get any combination of {H, T} over the 4 coins we can use this to work out how likely it is to get the 5 disjoint events. Since each event is disjoint, one event does not effect another so it’s just a case of 1/16 * 5 (for the 5 disjoint events) which results in 5/16.</p>
<p>Thus the probability of at least 3 coins coming up tails is 5/16.</p>
<h3 id="conditional-probability">Conditional Probability<a hidden class="anchor" aria-hidden="true" href="#conditional-probability">#</a></h3>
<p>Conditional probability is where an event can only happen if another event has happened. Let’s start with an easy problem:</p>
<p>John&rsquo;s favourite programming languages are Haskell and x86 Assembley. Let A represent the event that he forces a class to learn Haskell and B represent the event that he forces a class to learn x86 Assembley.On a randomly selected day, John is taken over by Satan himself, so the probability of P(A) is 0.6 and the probability of P(B) is 0.4 and the conditional probability that he teaches Haskell, given that he has taught x86 Assembley that day is P(A|B) = 0.7.Based on the information, what is P(B|A), the conditional that John teaches x86 Assembley given that he taught Haskell, rounded to the nearest hundredth?</p>
<p>The probability of P(A and B) = P(A|B) * P(B) read “|” as given, as in, “A|B” is read as “A given B”. It can also be written as P(B|A) * P(A).</p>
<p>The reason it is P(A|B) * P(B) is because given the probability of “Given the probability that B happens, A happens” and the probability of B is P(B). (A|B) is a different probability to P(B) and P(A and B) can only happen if P(B) happens which then allows P(B|A) to happen.</p>
<p>So we can transform this into a mathematical formula:</p>
<p>P(A and B) = P(A|B) * P(B) = 0.7 * 0.5 = 0.35Solving itP(B|A) * P(A)P(A) = 0.5So0.6 * P(B|A)Now we don&rsquo;t know what P(B|A) is, but we want to find out. We know that P(B|A) must be a part of P(A and B) because P(A and B) is the probability that both of these events happen so&hellip;P(A and B) = 0.350.35 = P(B|A) * 0.5With simple algebraic manipulation0.35/0.5 = P(B|A)P(B|A) = 0.7</p>
<p>For a visual explanation of conditional probability, watch this video by Khan Academy</p>
<h3 id="bayes-therom">Bayes Therom<a hidden class="anchor" aria-hidden="true" href="#bayes-therom">#</a></h3>
<p>Bayes Therom allows us to work out the probability of events given prior knowledge about the events. It is more of an observation than a therom, as it correctly works all the time. Bayes therom is created by Thomas Bayes, who noted this observation in a notebook. He never published it, so he wasn’t recgonised for his famous therom during his life time.
<img src="https://cdn-images-1.medium.com/max/800/1*4NP-Lj4PxOP98zmv6IfilA.png" alt="img">Bayes Therom from <a href="https://betterexplained.com/articles/colorized-math-equations/">https://betterexplained.com/articles/colorized-math-equations/</a>
The probability of A given B is the probability of B given A (note: it’s reversed here) times by the probability of A divided by the probability of B.</p>
<p>Of course this sounds confusing, so it may help to see an example.</p>
<p>Suppose a new strand of mexican black tar heroin is found on the streets and the police want to identify whether someone is a user or not.The drug is 99% sensitive, that is that the proportion of people who are correctly identified as taking the drug.The drug is 99% specific, that is that the proportion of people who are correctly identified as not taking the drug.Note: there is a 1% false positive rate for both users and non users.Suppose that 0.5% of people at John Moores takes the drug. What is the probability that a randomly selected John Moores student with a positive test is a user?
<img src="https://cdn-images-1.medium.com/max/800/1*HmzHFZOkqEjKx3ulkPFuBQ.png" alt="img">
Once you have all the information, it’s simply a case of substituting in the values and working it out.</p>
<p>Below is a video explaining Bayes Therom intuitively with real-world examples along with the history behind it as well as the philosophy of Bayes Therom:</p>
<p>If you want to see how Bayes Therom is used in Machine Learning — check this out!</p>
<p><a href="https://hackernoon.com/absolute-fundamentals-of-machine-learning-dca5deee78df"><strong>Absolute Fundamentals of Machine Learning</strong></a>
<a href="https://hackernoon.com/absolute-fundamentals-of-machine-learning-dca5deee78df">*Machine learning, what a buzzword. I’m sure you all want to understand machine learning, and that’s what I’m going to…*hackernoon.com</a></p>
<h3 id="random-variables">Random Variables<a hidden class="anchor" aria-hidden="true" href="#random-variables">#</a></h3>
<p>A random variable is a function, it is not random or a variable.</p>
<p>A random variable does not need to specify the sample space S directly but assign a probability that a variable (X) takes a certain value. Unlike previous probability where we needed to define a sample space, we only care about the probability itself.</p>
<p>Random variables are often written as P(f=r) where f is the event name and r is the probability.</p>
<p>The probably has to be between 0 and 1, like all probability values.</p>
<p>We write NOT (using any notation you may wish) (F=r) for the event that F is every variable apart from R.</p>
<p>An example of this</p>
<pre><code>P(Die=1) = 1/6The probability that this die takes the value 1 is 1/6NOT P(Die=1) is the event that the die is(Die=2) OR (Die=3) OR (Die=4) OR (Die=5) Or (Die=6)
</code></pre>
<p>The complement of P(f=r) ; the notation used to represent random variables, is 1 — P(f=r), where 1 is 100% or just 1.</p>
<p>We sometimes use symbols (words) instead of numbers to represent random variables. This is really useful. Let’s say the weather can be 1 of 4 states, sunny, rain, cloudy, snow. Thus, instead of assigning Weather = 1 we could write Weather = sunny.</p>
<p>Sometimes it lengthy to write up all the probabilities such as P(Weather = sunny) = 0.7 or P(Weather = rain) = 0.3. If the values are fixed in order then we could write P(Weather) = (0.7, 0.3)</p>
<p>We use bold-face <strong>P</strong> to indicate that the result is a vector of numbers representing the individual values of Weather. An example of this is: <strong>P</strong>(Weather) = (0.7, 0.3).</p>
<h3 id="joint-probability-distributions">Joint Probability Distributions<a hidden class="anchor" aria-hidden="true" href="#joint-probability-distributions">#</a></h3>
<p>A joint probability distribution allows you to have multiple random variables, typically 50 or 100 but our examples will include fewer.</p>
<p>A possible joint probability distribution <strong>P</strong>(Weather, Cavity) for the random variables Weather and Cavity is given by the following table:
<img src="https://cdn-images-1.medium.com/max/800/1*0hoJlRjzSXrTqCyPDre1Lg.png" alt="">
This is a joint probability distribution for tooth cavities and the weather. Cavity is a boolean value, it is either 0 or 1 and there are 4 options for the weather. If we want to create a joint probability distribution of P(Weather, Cavity) we would make the table above.</p>
<p>The probability for weather = sunny, and cavity = 1 is 0.144. The probability of the joint distribution sums to 1.</p>
<h3 id="full-joint-probability-distribution">Full Joint Probability Distribution<a hidden class="anchor" aria-hidden="true" href="#full-joint-probability-distribution">#</a></h3>
<p>We call it a full joint probability distribution if everything that is relevant in the domain is included. Unlike the above example, cavities and weather aren’t in the same domain.</p>
<p>Assume the random variables Toothache, Cavity, Catch fully describe a visit to a dentist</p>
<p>Then a full joint probability distribution is given by the following table:
<img src="https://cdn-images-1.medium.com/max/800/0*gea23EgYphQi3foF.png" alt="">
From <a href="https://math.stackexchange.com/questions/1976663/need-help-calculating-full-joint-probability-distribution">here</a></p>
<h3 id="marginalisation">Marginalisation<a hidden class="anchor" aria-hidden="true" href="#marginalisation">#</a></h3>
<p>One can computer the marginal probabilities of random variables by summing the variables. For example in the above example, if one wanted to sum the probability of P(Cavity=1) then you will sum all the probabilities where the cavity is equal to 1.
<img src="https://cdn-images-1.medium.com/max/800/1*ObUU8ZDp9CWfTyUulP5nnw.png" alt=""></p>
<h3 id="conditonal--posterior-probability">Conditonal / Posterior Probability<a hidden class="anchor" aria-hidden="true" href="#conditonal--posterior-probability">#</a></h3>
<p>We can calculate the conditional / posteior probability of a full joint distribution the same way we would do it normally.
<img src="https://cdn-images-1.medium.com/max/800/1*vIWe-Tmj75QHvqlbAR9ZTw.png" alt="">
Note that (F, G) signifies F (and, intersection) G.</p>
<h3 id="expected-value">Expected Value<a hidden class="anchor" aria-hidden="true" href="#expected-value">#</a></h3>
<p>The expected value is exactly what it sounds like, what do you expect the value to be? You can use this to work out the average score of a dice roll over 6 rolls, or anything really relating to probability where it has a value property.</p>
<p>Given the outcomes=(1, 2) and the probabilities=(1/8, 1/4) the expected value, E[x] is E[x] = 1(1/8) + 2(1/4) = 0.625.
<img src="https://cdn-images-1.medium.com/max/800/1*AeV39_mjhpyjZtaSRLC7LA.png" alt="img">
Suppose we’re counting types of bikes, and we have 4 bikes. We assign a code to each bike like so:
<img src="https://cdn-images-1.medium.com/max/800/1*abbFixm6lr_ENDq5OuVFsw.png" alt="">
For every bike, we give it a number. For every coding, we can see we use 2 bits. Either 0 or 1. For the expected value, not only do we need the value for the variable but the probability. Each bike has equal probability. So each bike has a 25% chance of appearing.</p>
<p>Calculating the expected value we multiply the probability by 2 bits, which gets us:
<img src="https://cdn-images-1.medium.com/max/800/1*COX5CcPoUAawmFXqd_27WQ.png" alt="">
What if the probability wasn’t equal?
<img src="https://cdn-images-1.medium.com/max/800/1*2BN_KfFV2guDUfs89gBSLA.png" alt="">
What we need to do is to multiply the number of bits by the probability
<img src="https://cdn-images-1.medium.com/max/800/1*wHvOPbsGOQYbRuMh-A_Ybg.png" alt=""></p>
<h3 id="entropy">Entropy<a hidden class="anchor" aria-hidden="true" href="#entropy">#</a></h3>
<p>Entropy is a measure of uncertainty assiocated with a random variable. It’s defined as the expected number of bits required to communicate the value of the variable.
<img src="https://cdn-images-1.medium.com/max/800/1*c_3RiTHigg36ry1XOTiQwg.png" alt="">
Entropy is trying to give a number to how uncertain something is.</p>
<h3 id="statistics">Statistics<a hidden class="anchor" aria-hidden="true" href="#statistics">#</a></h3>
<p>Statistics is not probability theory. Statistics is the real world application of ideas which come from probabiltiy theory. These can invole:</p>
<ol>
<li>Psepholohy — Analysing voting patterns</li>
<li>Data analysis — Data science</li>
<li>Quality control</li>
</ol>
<h3 id="sample-space-1">Sample Space<a hidden class="anchor" aria-hidden="true" href="#sample-space-1">#</a></h3>
<p>A sample space is a collection of data as a single finite set that looks something like:
<img src="https://cdn-images-1.medium.com/max/800/1*pGfv8JlWce49qMdVyVb6-w.png" alt="">
Where S is the sample space.</p>
<h3 id="probability-distribution">Probability Distribution<a hidden class="anchor" aria-hidden="true" href="#probability-distribution">#</a></h3>
<p>Let’s say we want to pick a random person from a set of all people who read the Sun newspaper. The probability of a single person being chosen is:
<img src="https://cdn-images-1.medium.com/max/800/1*IIuQJTJK_AzbgK6OrK5gqw.png" alt="">
A probability distribution is a sample space where every item has a probability value between 0 and 1 assigned to them that represents how likely they are to be picked.</p>
<p>In total, if s is an element of S, that is, if an element s is a part of the set (group) of sample space, S, then:
<img src="https://cdn-images-1.medium.com/max/800/1*q8frCWvgjDzsYy0znPPxoQ.png" alt="">
If you add the probability of every element in the sample space, it has to sum to 1.</p>
<p>When we want to sample this data set, we could just go through every single person in the dataset to get a good feel of the generalness of this sample. However, if there were 7 billion people in this dataset that may take a very very long time.</p>
<p>There are 2 ways we can now sample the data.</p>
<p>We can either randomly choose people from the dataset and use that as our sample or we can hand-pick a specific subset of the data to use.</p>
<p>A <strong>uniform</strong> dataset is one where everyone is equally likely to be picked. A <strong>biased</strong> sample is not uniform, the people were hand picked.</p>
<p><strong>Unbiased</strong> data sets seem “fair” whereas unbiased seem “unfair”. With an unbiased sample we cannot fix the outcome. We cannot change the data to our favor.</p>
<p>Sometimes we don’t care about “fairness” and sometimes unbiased samples can lead to unexpected results.</p>
<h3 id="random-variables-1">Random Variables<a hidden class="anchor" aria-hidden="true" href="#random-variables-1">#</a></h3>
<p>Remember earlier when we said that random variables are functions? Well, if you apply a random variable to a sample space, a population like so:
<img src="https://cdn-images-1.medium.com/max/800/1*1pd0CT0KmFOdbOZXTQoPYQ.png" alt="">
You get a <strong>biased</strong> data set from that sample space. It’s biased because we’re not randomly picking people in the set; we are applying a filter — a rule to the set to get a subset of the population.</p>
<p>Professor Paul Dunne had this to say about random variables:</p>
<blockquote>
<p>The notion of a probability distribution. This is the description of the probability that a member of a population (ie set) is selected. For example if we consider a single die the population has 6 members: {1,2,3,4,5,6} We might have a probability distribution corresponding to a fair die so that each has a probability of 1/6 of being chosen. If it’s a biased die then, for example, the probability distribution could be P[6]=5/6 P[1]=0 and P[2]=P[3]=P[4]=P[5]=1/24
With this the sum of individual outcomes is 1.</p>
</blockquote>
<blockquote>
<p>A random variable is best thought of firstly by forgetting about probabilities and thinking of an arbitrary function from the population to for instance the real numbers. In the die example we could choose f(x)=x² now unlike the probability distribution function the function chosen has no constraints: members of the population do not have to have values between 0 and 1, the sum of the function values doesn’t have to add up to 1. Where the idea of “random variable” enters is when a function is combined with a probability distribution. Now the distribution is treated not as simply choosing a MEMBER of the population but as choosing the VALUE of the function in a random style, that is instead of returning the selected member (eg result of throwing a die) the function value for that member is reported (eg the square of the number thrown).</p>
</blockquote>
<h3 id="mean-average-value-with-random-variables">Mean Average Value with Random Variables<a hidden class="anchor" aria-hidden="true" href="#mean-average-value-with-random-variables">#</a></h3>
<p>Given a population, S, whose members are sampled according to a distribution, D. The mean (expected) value of the random variable r(s) under D is denoted as</p>
<p>This is simply states that the expected value is a “weighted” sum (taken over all the members, s, of the total population, S) of:
<img src="https://cdn-images-1.medium.com/max/800/1*k9W5oonAUUfXsZ9AyIE8gg.png" alt=""></p>
<blockquote>
<p>*the chance that D selects s multipled by the value of the function returned by r for s, ie r(s).*In Unbiased Distributions</p>
</blockquote>
<h4 id="unbiased-distributions">Unbiased Distributions<a hidden class="anchor" aria-hidden="true" href="#unbiased-distributions">#</a></h4>
<p>In unbiased distributions the expected value is just the total sum of all the random variables divided by the population size:
<img src="https://cdn-images-1.medium.com/max/800/1*DRqshziDzXJMMIoc7NFjfw.png" alt="">
This is just your typical mean value, the one you learn in school. My teacher taught me a cool song to remember the differences between mean, range, median etc.</p>
<blockquote>
<p><em>Hey diddle diddle the median is the middle we add and divide for the mean. The mode is the one you see the most, and the range is the difference in between!</em></p>
</blockquote>
<p>Suppose S is a collection of outcomes that may appear by rolling a die 6,000 times.</p>
<p>Then for a “fair” die you would expect to see each outcome 1,000 times.</p>
<p>Suppose we have a game where players stake £1 and if the die lands on one of {1, 2, 3} the player gets £2 afterwards otherwise they lose their stake. In a fair game the player can expect to win 3/6 = 1/2 = half of the time.</p>
<h3 id="confidence-testing">Confidence Testing<a hidden class="anchor" aria-hidden="true" href="#confidence-testing">#</a></h3>
<p>Let’s say the hypothesis of an experiment’s outcome is X, and the actual outcome is Y.</p>
<p>The outcome Y is so far away from the prediction that the hypothesis is false. This is called <strong>significance</strong>.</p>
<p>A <strong>null hypothesis</strong> states that the outcome <strong>will</strong> be X.</p>
<p><strong>Significance</strong> represents that the likelihood of the <strong>observed outcome</strong> being “consistent” with the <strong>predicted outcome</strong>.</p>
<p>An hypothesis can be “rejected” with observed outcomes with three increasing levels of confience:</p>
<ol>
<li>The probability that X holds given Y is at most 0.05 (significant)</li>
<li>The probability that X holds given that Y has resulted is at most 0.01 (highly significant)</li>
<li>The probability that X holds given that Y has resulted is 0.001 (very highly siginificant)</li>
</ol>
<p>There are two types of errors that can occur here:</p>
<p><strong>Type 1 error</strong> — A true hypothesis is rejected <strong>Type 2 error</strong> — A false hypothesis is accecpted</p>
<h3 id="measuring-signifcance">Measuring Signifcance<a hidden class="anchor" aria-hidden="true" href="#measuring-signifcance">#</a></h3>
<p>The outcome of the event will get “closer and closer” towards the expected value can be expressed as a formula called the deviance. Recall that the event of a random variable in a sample space is:
<img src="https://cdn-images-1.medium.com/max/800/1*YI56gC-6p7BMUib9OoXTwg.png" alt="">
Variance is just:</p>
<blockquote>
<p><em>“How far away a chosen member is from the expected variable”</em></p>
</blockquote>
<p><img src="https://cdn-images-1.medium.com/max/800/1*I2SfKcgHI2gpgfs0rE6Dug.png" alt="">
Doesn’t that look horrible? Well, if we were to put the first formula in it would look like:
<img src="https://cdn-images-1.medium.com/max/800/1*YKibcjT2qOEtHYpR1trzmg.png" alt="">
Doesn’t that look like the most horrible formula ever?</p>
<p>the r(s) part is the random variable, the subset of the population. The part is the expected value of a random member.</p>
<p>Variance always produces a non negative value.</p>
<p>The standard deviation is just this formula, square rooted.
<img src="https://cdn-images-1.medium.com/max/800/1*u74zNcbCgBj2bF6mfj8mow.png" alt="">
It’s actually more commonly written as:</p>
<p>I just wanted to see how convuluted the formula could become.</p>
<p>The standard deviation is just:</p>
<blockquote>
<p><em>“How far away the largest (or smallest) data point is from the mean average”.</em></p>
</blockquote>
<p><img src="https://cdn-images-1.medium.com/max/800/1*vegPq019hD09KdtSAmCylw.png" alt=""></p>
<h3 id="q-test">Q-test<a hidden class="anchor" aria-hidden="true" href="#q-test">#</a></h3>
<p>Given a predicted outcome, X, of an experiment and the actual outcome, Y. If we know the <strong>standard deviation</strong> for the environment in which the experiment is set, then we can compute the value:
<img src="https://cdn-images-1.medium.com/max/800/1*8PZoP7uQHpBIfXVXj70hOg.png" alt="">
If q &gt; 0.01 then X holds with probability at best 0.05 If q &gt; 2.33 then X holds with probability at best 0.01 If q &gt; 3.09 then X holds with probability at best 0.001</p>
<h3 id="if-you-liked-this-article-connect-with-me">If you liked this article, connect with me!<a hidden class="anchor" aria-hidden="true" href="#if-you-liked-this-article-connect-with-me">#</a></h3>
<p><a href="https://www.linkedin.com/in/brandonls/">LinkedIn</a> | <a href="https://twitter.com/brandon_skerrit">Twitter</a> | <a href="https://upscri.be/885736-2/">Newsletter</a></p>
<p><a href="https://ko-fi.com/hacker"><strong>Buy Brandon Skerritt a Coffee. ko-fi.com/hacker</strong></a>
<a href="https://ko-fi.com/hacker"><em>Support the content you love. Buy a Coffee for Brandon Skerritt with Ko-fi.com</em>ko-fi.com</a><a href="https://www.paypal.me/BrandonSkerritt"><strong>Pay Brandon Skerritt using PayPal.Me</strong></a>
<a href="https://www.paypal.me/BrandonSkerritt">*Go to paypal.me/BrandonSkerritt and type in the amount. Since it&rsquo;s PayPal, it&rsquo;s easy and secure. Don&rsquo;t have a PayPal…*www.paypal.me</a></p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://polymath.cloud/tags/computer-science/">Computer Science</a></li>
      <li><a href="https://polymath.cloud/tags/university/">University</a></li>
    </ul>






<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on twitter"
        href="https://twitter.com/intent/tweet/?text=An%20Introduction%20to%20Probability%20%26%20Statistics&amp;url=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f&amp;hashtags=University%2cComputerScience">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f&amp;title=An%20Introduction%20to%20Probability%20%26%20Statistics&amp;summary=An%20Introduction%20to%20Probability%20%26%20Statistics&amp;source=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f&title=An%20Introduction%20to%20Probability%20%26%20Statistics">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on whatsapp"
        href="https://api.whatsapp.com/send?text=An%20Introduction%20to%20Probability%20%26%20Statistics%20-%20https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Probability &amp; Statistics on telegram"
        href="https://telegram.me/share/url?text=An%20Introduction%20to%20Probability%20%26%20Statistics&amp;url=https%3a%2f%2fpolymath.cloud%2fan-introduction-to-probability-statistics%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2020 <a href="https://polymath.cloud">Polymath.cloud</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<button class="top-link" id="top-link" type="button" aria-label="go to top" title="Go to Top" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
        <path d="M12 6H0l6-6z" /></svg>
</button>



<script defer src="https://polymath.cloud/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
    mybutton.onclick = function () {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
        window.location.hash = ''
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
